{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb7fc6c6",
   "metadata": {},
   "source": [
    "### TC로 만든 Text2SQL 결과를 평가한다. \n",
    "#### product_id list를 받아, 정답셋과 비교하여 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afb50f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL 연결 정보:\n",
      "   Host: dev-rubicon-postgresql.postgres.database.azure.com\n",
      "   Port: 5432\n",
      "\n",
      "✅ PostgreSQL 설정 로드 완료\n"
     ]
    }
   ],
   "source": [
    "# CREATE TABLE IF NOT EXISTS tc_check_table_20251015 (\n",
    "# \tid SERIAL PRIMARY KEY,\n",
    "#   query_number INT,\n",
    "# \tcategory VARCHAR(15),\n",
    "# \tinstruction TEXT,\n",
    "# \tquery TEXT,\n",
    "# \tproduct_id_list TEXT[],\n",
    "# \tcreated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "# );\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 데이터베이스 관련\n",
    "from sqlalchemy import create_engine, text\n",
    "import psycopg2\n",
    "\n",
    "load_dotenv('.env')\n",
    "\n",
    "# PostgreSQL 설정 로드\n",
    "PG_HOST = os.getenv('PG_HOST')\n",
    "PG_PORT = os.getenv('PG_PORT')\n",
    "PG_DATABASE = os.getenv('PG_DATABASE')\n",
    "PG_USER = os.getenv('PG_USER')\n",
    "PG_PASSWORD = os.getenv('PG_PASSWORD')\n",
    "\n",
    "print(f\"PostgreSQL 연결 정보:\")\n",
    "print(f\"   Host: {PG_HOST}\")\n",
    "print(f\"   Port: {PG_PORT}\")\n",
    "\n",
    "# SQLAlchemy 연결 문자열 생성\n",
    "POSTGRES_URL = f\"postgresql://{PG_USER}:{PG_PASSWORD}@{PG_HOST}:{PG_PORT}/{PG_DATABASE}\"\n",
    "\n",
    "print(f\"\\n✅ PostgreSQL 설정 로드 완료\")\n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"PostgreSQL 데이터베이스 연결 객체를 반환합니다.\"\"\"\n",
    "    return psycopg2.connect(\n",
    "            host=PG_HOST,\n",
    "            port=PG_PORT,\n",
    "            database=PG_DATABASE,\n",
    "            user=PG_USER,\n",
    "            password=PG_PASSWORD\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5154e9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "class Text2SQLEvaluator:\n",
    "    \"\"\"Text2SQL 평가를 위한 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.connection = None\n",
    "        self.evaluation_results = []\n",
    "        \n",
    "    def connect_db(self):\n",
    "        \"\"\"데이터베이스 연결\"\"\"\n",
    "        try:\n",
    "            self.connection = get_db_connection()\n",
    "            print(\"✅ 데이터베이스 연결 성공\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 데이터베이스 연결 실패: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def close_connection(self):\n",
    "        \"\"\"데이터베이스 연결 종료\"\"\"\n",
    "        if self.connection:\n",
    "            self.connection.close()\n",
    "            print(\"데이터베이스 연결 종료\")\n",
    "    \n",
    "    def get_ground_truth(self, query_number: int) -> Tuple[Optional[List[str]], Optional[str], Optional[str]]:\n",
    "        \"\"\"\n",
    "        정답 product_id list 조회\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (product_id_list, category, instruction)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            cursor = self.connection.cursor()\n",
    "            query = \"\"\"\n",
    "                SELECT product_id_list, category, instruction\n",
    "                FROM tc_check_table_20251015\n",
    "                WHERE query_number = %s\n",
    "            \"\"\"\n",
    "            cursor.execute(query, (query_number,))\n",
    "            result = cursor.fetchone()\n",
    "            cursor.close()\n",
    "            \n",
    "            if result:\n",
    "                return result[0], result[1], result[2]\n",
    "            else:\n",
    "                print(f\"⚠️ 쿼리 번호 {query_number}에 대한 정답을 찾을 수 없습니다.\")\n",
    "                return None, None, None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 정답 조회 중 오류 발생: {e}\")\n",
    "            return None, None, None\n",
    "    \n",
    "    def calculate_metrics(self, predicted: List[str], ground_truth: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        예측 결과와 정답을 비교하여 평가 지표 계산\n",
    "        \n",
    "        Args:\n",
    "            predicted: 예측된 product_id 리스트\n",
    "            ground_truth: 정답 product_id 리스트\n",
    "            \n",
    "        Returns:\n",
    "            평가 지표 딕셔너리\n",
    "        \"\"\"\n",
    "        # Set으로 변환하여 비교\n",
    "        pred_set = set(predicted) if predicted else set()\n",
    "        truth_set = set(ground_truth) if ground_truth else set()\n",
    "        \n",
    "        # True Positives: 예측과 정답 모두에 있는 항목\n",
    "        tp = len(pred_set.intersection(truth_set))\n",
    "        \n",
    "        # False Positives: 예측에만 있고 정답에는 없는 항목\n",
    "        fp = len(pred_set - truth_set)\n",
    "        \n",
    "        # False Negatives: 정답에만 있고 예측에는 없는 항목\n",
    "        fn = len(truth_set - pred_set)\n",
    "        \n",
    "        # True Negatives는 product_id 전체 집합을 알아야 계산 가능\n",
    "        # 여기서는 관련 없는 product_id의 수를 모르므로 생략\n",
    "        \n",
    "        # Precision, Recall, F1 Score 계산\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        # Exact Match (완전 일치 여부)\n",
    "        exact_match = 1 if pred_set == truth_set else 0\n",
    "        \n",
    "        # Jaccard Similarity (IoU)\n",
    "        jaccard = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'TP': tp,\n",
    "            'FP': fp,\n",
    "            'FN': fn,\n",
    "            'Precision': round(precision, 4),\n",
    "            'Recall': round(recall, 4),\n",
    "            'F1_Score': round(f1_score, 4),\n",
    "            'Exact_Match': exact_match,\n",
    "            'Jaccard_Similarity': round(jaccard, 4),\n",
    "            'Predicted_Count': len(pred_set),\n",
    "            'Ground_Truth_Count': len(truth_set)\n",
    "        }\n",
    "    \n",
    "    def evaluate_single_query(self, query_number: int, predicted_ids: List[str], \n",
    "                            verbose: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        단일 쿼리에 대한 평가 수행\n",
    "        \n",
    "        Args:\n",
    "            query_number: 쿼리 번호\n",
    "            predicted_ids: 예측된 product_id 리스트\n",
    "            verbose: 상세 출력 여부\n",
    "            \n",
    "        Returns:\n",
    "            평가 결과 딕셔너리\n",
    "        \"\"\"\n",
    "        # 정답 조회\n",
    "        ground_truth_ids, category, instruction = self.get_ground_truth(query_number)\n",
    "        \n",
    "        if ground_truth_ids is None:\n",
    "            return None\n",
    "        \n",
    "        # 평가 지표 계산\n",
    "        metrics = self.calculate_metrics(predicted_ids, ground_truth_ids)\n",
    "        \n",
    "        # 결과에 메타 정보 추가\n",
    "        result = {\n",
    "            'query_number': query_number,\n",
    "            'category': category,\n",
    "            'instruction': instruction[:50] + '...' if instruction and len(instruction) > 50 else instruction,\n",
    "            **metrics\n",
    "        }\n",
    "        \n",
    "        # 결과 저장\n",
    "        self.evaluation_results.append(result)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"쿼리 번호: {query_number}\")\n",
    "            print(f\"카테고리: {category}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"예측된 product_id 수: {metrics['Predicted_Count']}\")\n",
    "            print(f\"정답 product_id 수: {metrics['Ground_Truth_Count']}\")\n",
    "            print(f\"\\n[평가 지표]\")\n",
    "            print(f\"  - TP (True Positives): {metrics['TP']}\")\n",
    "            print(f\"  - FP (False Positives): {metrics['FP']}\")\n",
    "            print(f\"  - FN (False Negatives): {metrics['FN']}\")\n",
    "            print(f\"  - Precision: {metrics['Precision']:.2%}\")\n",
    "            print(f\"  - Recall: {metrics['Recall']:.2%}\")\n",
    "            print(f\"  - F1 Score: {metrics['F1_Score']:.2%}\")\n",
    "            print(f\"  - Exact Match: {'✅' if metrics['Exact_Match'] else '❌'}\")\n",
    "            print(f\"  - Jaccard Similarity: {metrics['Jaccard_Similarity']:.2%}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def evaluate_batch(self, query_predictions: List[Tuple[int, List[str]]], \n",
    "                      verbose: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        여러 쿼리에 대한 일괄 평가\n",
    "        \n",
    "        Args:\n",
    "            query_predictions: [(query_number, predicted_ids), ...] 형태의 리스트\n",
    "            verbose: 각 쿼리별 상세 출력 여부\n",
    "            \n",
    "        Returns:\n",
    "            평가 결과 DataFrame\n",
    "        \"\"\"\n",
    "        # 평가 시작 전에 결과 리스트 초기화하지 않음 (누적 평가를 위해)\n",
    "        batch_results = []\n",
    "        \n",
    "        for query_number, predicted_ids in query_predictions:\n",
    "            result = self.evaluate_single_query(query_number, predicted_ids, verbose)\n",
    "            if result:\n",
    "                batch_results.append(result)\n",
    "        \n",
    "        # 배치 결과만 DataFrame으로 반환\n",
    "        return pd.DataFrame(batch_results)\n",
    "    \n",
    "    def get_overall_metrics(self) -> Dict:\n",
    "        \"\"\"\n",
    "        전체 평가 결과 요약\n",
    "        \n",
    "        Returns:\n",
    "            전체 평가 지표 딕셔너리\n",
    "        \"\"\"\n",
    "        if not self.evaluation_results:\n",
    "            print(\"평가된 결과가 없습니다.\")\n",
    "            return None\n",
    "        \n",
    "        df = pd.DataFrame(self.evaluation_results)\n",
    "        \n",
    "        # 중복 제거 (같은 query_number가 여러 번 평가된 경우 최신 결과만 사용)\n",
    "        df = df.drop_duplicates(subset=['query_number'], keep='last')\n",
    "        \n",
    "        # 전체 통계\n",
    "        total_tp = df['TP'].sum()\n",
    "        total_fp = df['FP'].sum()\n",
    "        total_fn = df['FN'].sum()\n",
    "        \n",
    "        # Micro-averaged metrics (전체 TP, FP, FN 기준)\n",
    "        micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "        micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "        micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0\n",
    "        \n",
    "        # Macro-averaged metrics (각 쿼리별 평균)\n",
    "        macro_precision = df['Precision'].mean()\n",
    "        macro_recall = df['Recall'].mean()\n",
    "        macro_f1 = df['F1_Score'].mean()\n",
    "        \n",
    "        # 카테고리별 성능\n",
    "        category_metrics = None\n",
    "        if 'category' in df.columns and df['category'].notna().any():\n",
    "            category_metrics = df.groupby('category').agg({\n",
    "                'Precision': 'mean',\n",
    "                'Recall': 'mean',\n",
    "                'F1_Score': 'mean',\n",
    "                'Exact_Match': 'mean',\n",
    "                'query_number': 'count'\n",
    "            }).round(4)\n",
    "            category_metrics.rename(columns={'query_number': 'Count'}, inplace=True)\n",
    "        \n",
    "        overall_metrics = {\n",
    "            'total_queries': len(df),\n",
    "            'total_tp': int(total_tp),\n",
    "            'total_fp': int(total_fp),\n",
    "            'total_fn': int(total_fn),\n",
    "            'micro_precision': round(micro_precision, 4),\n",
    "            'micro_recall': round(micro_recall, 4),\n",
    "            'micro_f1': round(micro_f1, 4),\n",
    "            'macro_precision': round(macro_precision, 4),\n",
    "            'macro_recall': round(macro_recall, 4),\n",
    "            'macro_f1': round(macro_f1, 4),\n",
    "            'exact_match_rate': round(df['Exact_Match'].mean(), 4),\n",
    "            'avg_jaccard': round(df['Jaccard_Similarity'].mean(), 4),\n",
    "            'category_metrics': category_metrics\n",
    "        }\n",
    "        \n",
    "        return overall_metrics\n",
    "    \n",
    "    def print_overall_report(self):\n",
    "        \"\"\"\n",
    "        전체 평가 리포트 출력\n",
    "        \"\"\"\n",
    "        metrics = self.get_overall_metrics()\n",
    "        \n",
    "        if not metrics:\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\" \" * 25 + \"📊 전체 평가 결과\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"\\n[평가 대상]\")\n",
    "        print(f\"  총 쿼리 수: {metrics['total_queries']}개\")\n",
    "        \n",
    "        print(f\"\\n[전체 통계]\")\n",
    "        print(f\"  - Total TP: {metrics['total_tp']}\")\n",
    "        print(f\"  - Total FP: {metrics['total_fp']}\")\n",
    "        print(f\"  - Total FN: {metrics['total_fn']}\")\n",
    "        \n",
    "        print(f\"\\n[Micro-averaged Metrics] (전체 TP, FP, FN 기준)\")\n",
    "        print(f\"  - Precision: {metrics['micro_precision']:.2%}\")\n",
    "        print(f\"  - Recall: {metrics['micro_recall']:.2%}\")\n",
    "        print(f\"  - F1 Score: {metrics['micro_f1']:.2%}\")\n",
    "        \n",
    "        print(f\"\\n[Macro-averaged Metrics] (쿼리별 평균)\")\n",
    "        print(f\"  - Precision: {metrics['macro_precision']:.2%}\")\n",
    "        print(f\"  - Recall: {metrics['macro_recall']:.2%}\")\n",
    "        print(f\"  - F1 Score: {metrics['macro_f1']:.2%}\")\n",
    "        \n",
    "        print(f\"\\n[추가 지표]\")\n",
    "        print(f\"  - Exact Match Rate: {metrics['exact_match_rate']:.2%}\")\n",
    "        print(f\"  - Average Jaccard Similarity: {metrics['avg_jaccard']:.2%}\")\n",
    "        \n",
    "        if metrics['category_metrics'] is not None:\n",
    "            print(f\"\\n[카테고리별 성능]\")\n",
    "            print(metrics['category_metrics'].to_string())\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    def save_results(self, filename: str = None):\n",
    "        \"\"\"\n",
    "        평가 결과를 파일로 저장\n",
    "        \n",
    "        Args:\n",
    "            filename: 저장할 파일명 (기본값: text2sql_eval_YYYYMMDD_HHMMSS.csv)\n",
    "        \"\"\"\n",
    "        if not self.evaluation_results:\n",
    "            print(\"저장할 평가 결과가 없습니다.\")\n",
    "            return\n",
    "        \n",
    "        if filename is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"text2sql_eval_{timestamp}.csv\"\n",
    "        \n",
    "        df = pd.DataFrame(self.evaluation_results)\n",
    "        # 중복 제거\n",
    "        df = df.drop_duplicates(subset=['query_number'], keep='last')\n",
    "        df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"✅ 평가 결과가 {filename}에 저장되었습니다. (총 {len(df)}개 쿼리)\")\n",
    "    \n",
    "    def reset_results(self):\n",
    "        \"\"\"평가 결과 초기화\"\"\"\n",
    "        self.evaluation_results = []\n",
    "        print(\"평가 결과가 초기화되었습니다.\")\n",
    "    \n",
    "    def get_results_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        현재까지의 평가 결과를 DataFrame으로 반환\n",
    "        \n",
    "        Returns:\n",
    "            평가 결과 DataFrame\n",
    "        \"\"\"\n",
    "        if not self.evaluation_results:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        df = pd.DataFrame(self.evaluation_results)\n",
    "        # 중복 제거\n",
    "        df = df.drop_duplicates(subset=['query_number'], keep='last')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ho4l4evvwa",
   "metadata": {},
   "source": [
    "## 사용 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7l73zlacder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 데이터베이스 연결 성공\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가기 인스턴스 생성\n",
    "evaluator = Text2SQLEvaluator()\n",
    "\n",
    "# 데이터베이스 연결\n",
    "evaluator.connect_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o9tsc77xg1j",
   "metadata": {},
   "source": [
    "### 1. 단일 쿼리 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9z5diwxsn0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "쿼리 번호: 1\n",
      "카테고리: 가격\n",
      "============================================================\n",
      "예측된 product_id 수: 4\n",
      "정답 product_id 수: 18\n",
      "\n",
      "[평가 지표]\n",
      "  - TP (True Positives): 0\n",
      "  - FP (False Positives): 4\n",
      "  - FN (False Negatives): 18\n",
      "  - Precision: 0.00%\n",
      "  - Recall: 0.00%\n",
      "  - F1 Score: 0.00%\n",
      "  - Exact Match: ❌\n",
      "  - Jaccard Similarity: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# 예시: 쿼리 번호 1에 대한 평가\n",
    "query_number = 1\n",
    "predicted_product_ids = ['P001', 'P002', 'P003', 'P005']  # 예측된 product_id 리스트\n",
    "# predicted_product_ids = ['P001', 'P002', 'P003', 'P005', 'G000430069']  # 예측된 product_id 리스트\n",
    "\n",
    "# 단일 쿼리 평가 실행\n",
    "result = evaluator.evaluate_single_query(query_number, predicted_product_ids, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raozhv24zih",
   "metadata": {},
   "source": [
    "### 2. 여러 쿼리 일괄 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "145wath4pf8j",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n평가 결과 DataFrame:\n",
      "   query_number category                   instruction  TP  FP  FN  Precision  \\\n",
      "0             1       가격              갤럭시 S25 가격 알려주세요   0   3  18        0.0   \n",
      "1             2       가격   갤럭시 S25 512GB 옵션은 가격이 얼마예요?   0   4   7        0.0   \n",
      "2             3       가격  갤럭시 S25 512GB 핑크골드는 가격이 얼마야?   0   2   1        0.0   \n",
      "\n",
      "   Recall  F1_Score  Exact_Match  Jaccard_Similarity  Predicted_Count  \\\n",
      "0     0.0         0            0                 0.0                3   \n",
      "1     0.0         0            0                 0.0                4   \n",
      "2     0.0         0            0                 0.0                2   \n",
      "\n",
      "   Ground_Truth_Count  \n",
      "0                  18  \n",
      "1                   7  \n",
      "2                   1  \n"
     ]
    }
   ],
   "source": [
    "# 여러 쿼리에 대한 예측 결과 준비\n",
    "# 형식: [(query_number, [predicted_product_ids]), ...]\n",
    "batch_predictions = [\n",
    "    (1, ['P001', 'P002', 'P003']),\n",
    "    (2, ['P010', 'P011', 'P012', 'P013']),\n",
    "    (3, ['P020', 'P021']),\n",
    "    # ... 더 많은 쿼리들\n",
    "]\n",
    "\n",
    "# 일괄 평가 실행 (verbose=False로 개별 출력 생략)\n",
    "results_df = evaluator.evaluate_batch(batch_predictions, verbose=False)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"\\\\n평가 결과 DataFrame:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olaud9o4gag",
   "metadata": {},
   "source": [
    "### 3. 100개 쿼리 평가 시뮬레이션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ednrddpqsc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평가 결과가 초기화되었습니다.\n",
      "100개 쿼리 평가 시작...\n",
      "✅ 100개 쿼리 평가 완료\n",
      "\\n[처음 5개 쿼리 평가 결과]\n",
      "   query_number category                   instruction  TP  FP  FN  Precision  \\\n",
      "0             1       가격              갤럭시 S25 가격 알려주세요   0   4  18        0.0   \n",
      "1             2       가격   갤럭시 S25 512GB 옵션은 가격이 얼마예요?   0   3   7        0.0   \n",
      "2             3       가격  갤럭시 S25 512GB 핑크골드는 가격이 얼마야?   0   4   1        0.0   \n",
      "3             4       가격                비스포크 ai 세탁기 가격   0   7  32        0.0   \n",
      "4             5       가격            S25랑 플러스 가격 비교해주세여   0   7  36        0.0   \n",
      "\n",
      "   Recall  F1_Score  Exact_Match  Jaccard_Similarity  Predicted_Count  \\\n",
      "0     0.0         0            0                 0.0                4   \n",
      "1     0.0         0            0                 0.0                3   \n",
      "2     0.0         0            0                 0.0                4   \n",
      "3     0.0         0            0                 0.0                7   \n",
      "4     0.0         0            0                 0.0                7   \n",
      "\n",
      "   Ground_Truth_Count  \n",
      "0                  18  \n",
      "1                   7  \n",
      "2                   1  \n",
      "3                  32  \n",
      "4                  36  \n"
     ]
    }
   ],
   "source": [
    "# 100개 쿼리에 대한 평가 예시\n",
    "# 실제 사용시에는 Text2SQL 모델의 예측 결과를 입력으로 사용\n",
    "\n",
    "# 예시: 100개 쿼리에 대한 예측 결과 (여기서는 더미 데이터 생성)\n",
    "all_predictions = []\n",
    "for query_num in range(1, 101):\n",
    "    # 실제로는 각 쿼리에 대한 Text2SQL 예측 결과를 여기에 입력\n",
    "    predicted_ids = [f'P{query_num:03d}_{i}' for i in range(np.random.randint(1, 10))]\n",
    "    all_predictions.append((query_num, predicted_ids))\n",
    "\n",
    "# 평가 결과 초기화 (이전 결과 삭제)\n",
    "evaluator.reset_results()\n",
    "\n",
    "# 100개 쿼리 일괄 평가\n",
    "print(\"100개 쿼리 평가 시작...\")\n",
    "results_df_100 = evaluator.evaluate_batch(all_predictions[:100], verbose=False)\n",
    "print(f\"✅ {len(results_df_100)}개 쿼리 평가 완료\")\n",
    "\n",
    "# 개별 쿼리 결과 샘플 확인\n",
    "print(\"\\\\n[처음 5개 쿼리 평가 결과]\")\n",
    "print(results_df_100.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ic41gcxuxjn",
   "metadata": {},
   "source": [
    "### 4. 전체 평가 결과 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dkaxxahcp5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "                         📊 전체 평가 결과\n",
      "======================================================================\n",
      "\n",
      "[평가 대상]\n",
      "  총 쿼리 수: 100개\n",
      "\n",
      "[전체 통계]\n",
      "  - Total TP: 0\n",
      "  - Total FP: 514\n",
      "  - Total FN: 2846\n",
      "\n",
      "[Micro-averaged Metrics] (전체 TP, FP, FN 기준)\n",
      "  - Precision: 0.00%\n",
      "  - Recall: 0.00%\n",
      "  - F1 Score: 0.00%\n",
      "\n",
      "[Macro-averaged Metrics] (쿼리별 평균)\n",
      "  - Precision: 0.00%\n",
      "  - Recall: 0.00%\n",
      "  - F1 Score: 0.00%\n",
      "\n",
      "[추가 지표]\n",
      "  - Exact Match Rate: 0.00%\n",
      "  - Average Jaccard Similarity: 0.00%\n",
      "\n",
      "[카테고리별 성능]\n",
      "          Precision  Recall  F1_Score  Exact_Match  Count\n",
      "category                                                 \n",
      "가격              0.0     0.0       0.0          0.0     25\n",
      "비교              0.0     0.0       0.0          0.0     25\n",
      "스펙              0.0     0.0       0.0          0.0     25\n",
      "추천              0.0     0.0       0.0          0.0     25\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# 전체 평가 리포트 출력\n",
    "evaluator.print_overall_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "t4pk7ouz5s8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n[핵심 지표 요약]\n",
      "Total Queries: 100\n",
      "Micro F1 Score: 0.00%\n",
      "Macro F1 Score: 0.00%\n",
      "Exact Match Rate: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# 전체 평가 지표 딕셔너리로 받기\n",
    "overall_metrics = evaluator.get_overall_metrics()\n",
    "\n",
    "print(\"\\\\n[핵심 지표 요약]\")\n",
    "print(f\"Total Queries: {overall_metrics['total_queries']}\")\n",
    "print(f\"Micro F1 Score: {overall_metrics['micro_f1']:.2%}\")\n",
    "print(f\"Macro F1 Score: {overall_metrics['macro_f1']:.2%}\")\n",
    "print(f\"Exact Match Rate: {overall_metrics['exact_match_rate']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qsksgk3sa1r",
   "metadata": {},
   "source": [
    "### 5. 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sr4hih6r1uo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일로 저장\n",
    "evaluator.save_results()  # 자동으로 타임스탬프가 포함된 파일명 생성\n",
    "\n",
    "# 또는 원하는 파일명으로 저장\n",
    "# evaluator.save_results(\"my_evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6rqmdatx3q8",
   "metadata": {},
   "source": [
    "### 6. 실제 사용 예시 (Text2SQL 모델과 통합)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w2dkd1jr2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_text2sql_model(query_number: int, user_query: str):\n",
    "    \"\"\"\n",
    "    Text2SQL 모델 평가를 위한 전체 파이프라인\n",
    "    \n",
    "    Args:\n",
    "        query_number: 쿼리 번호\n",
    "        user_query: 사용자의 자연어 쿼리\n",
    "    \n",
    "    Returns:\n",
    "        평가 결과\n",
    "    \"\"\"\n",
    "    # 1. Text2SQL 모델로 SQL 생성 (여기에 실제 모델 호출 코드 추가)\n",
    "    # generated_sql = your_text2sql_model(user_query)\n",
    "    \n",
    "    # 2. 생성된 SQL 실행하여 product_id 리스트 추출 (예시)\n",
    "    # predicted_product_ids = execute_sql_and_get_products(generated_sql)\n",
    "    \n",
    "    # 예시 데이터 (실제로는 위의 과정으로 얻은 결과 사용)\n",
    "    predicted_product_ids = ['P001', 'P002', 'P003', 'P004']\n",
    "    \n",
    "    # 3. 평가 실행\n",
    "    result = evaluator.evaluate_single_query(query_number, predicted_product_ids)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 사용 예시\n",
    "# result = evaluate_text2sql_model(1, \"최근 3개월간 가장 많이 팔린 상품을 알려주세요\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4g9e0wh3vp7",
   "metadata": {},
   "source": [
    "### 7. 연결 종료"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c824e0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rgd6bwe8eq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 작업 완료 후 데이터베이스 연결 종료\n",
    "evaluator.close_connection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
