{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8061368d",
   "metadata": {},
   "source": [
    "## Mockdb ê´€ë ¨ ìŠ¤í¬ë¦½íŠ¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3907d0e1",
   "metadata": {},
   "source": [
    "### Postgresql ì—°ê²° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "769adcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL ì—°ê²° ì •ë³´:\n",
      "   Host: dev-rubicon-postgresql.postgres.database.azure.com\n",
      "   Port: 5432\n",
      "\n",
      "âœ… PostgreSQL ì„¤ì • ë¡œë“œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "load_dotenv('.env')\n",
    "\n",
    "# PostgreSQL ì„¤ì • ë¡œë“œ\n",
    "PG_HOST = os.getenv('PG_HOST')\n",
    "PG_PORT = os.getenv('PG_PORT')\n",
    "PG_DATABASE = os.getenv('PG_DATABASE')\n",
    "PG_USER = os.getenv('PG_USER')\n",
    "PG_PASSWORD = os.getenv('PG_PASSWORD')\n",
    "\n",
    "print(f\"PostgreSQL ì—°ê²° ì •ë³´:\")\n",
    "print(f\"   Host: {PG_HOST}\")\n",
    "print(f\"   Port: {PG_PORT}\")\n",
    "\n",
    "# SQLAlchemy ì—°ê²° ë¬¸ìì—´ ìƒì„±\n",
    "POSTGRES_URL = f\"postgresql://{PG_USER}:{PG_PASSWORD}@{PG_HOST}:{PG_PORT}/{PG_DATABASE}\"\n",
    "\n",
    "print(f\"\\nâœ… PostgreSQL ì„¤ì • ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfed6316",
   "metadata": {},
   "source": [
    "### PostgreSQL ì—°ê²° ë° í…Œì´ë¸” ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "018d8c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PostgreSQL ì—°ê²° ë° í…Œì´ë¸” ìƒì„±\n",
      "============================================================\n",
      "âœ… PostgreSQL ì—°ê²° ì„±ê³µ!\n",
      "   ë²„ì „: PostgreSQL 17.5 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 11.2.0, 64-bit\n",
      "   í˜„ì¬ DB: postgres\n",
      "   ì‚¬ìš©ì: rubicon\n",
      "\n",
      "ğŸ“Š ê¸°ì¡´ í…Œì´ë¸” ìˆ˜: 0ê°œ\n",
      "   âœ… kt_merged_product_20251015 í…Œì´ë¸” ìƒì„±\n",
      "   âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ\n",
      "   âœ… ì»¤ë©˜íŠ¸ ìƒì„± ì™„ë£Œ\n",
      "\n",
      "âœ… ëª¨ë“  í…Œì´ë¸” ìƒì„± ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "def postgresql_connection_and_create_tables():\n",
    "    \"\"\"PostgreSQL ì—°ê²° ë° í…Œì´ë¸” ìƒì„±\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"PostgreSQL ì—°ê²° ë° í…Œì´ë¸” ìƒì„±\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # SQLAlchemy ì—”ì§„ ìƒì„±\n",
    "        engine = create_engine(POSTGRES_URL)\n",
    "        # engine = get_db_connection()\n",
    "        \n",
    "        # ì—°ê²° í…ŒìŠ¤íŠ¸\n",
    "        with engine.connect() as conn:\n",
    "            # ê¸°ë³¸ ì—°ê²° í…ŒìŠ¤íŠ¸\n",
    "            result = conn.execute(text(\"SELECT version();\"))\n",
    "            version = result.fetchone()[0]\n",
    "            print(f\"âœ… PostgreSQL ì—°ê²° ì„±ê³µ!\")\n",
    "            print(f\"   ë²„ì „: {version}\")\n",
    "            \n",
    "            # í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´\n",
    "            result = conn.execute(text(\"SELECT current_database(), current_user;\"))\n",
    "            db_info = result.fetchone()\n",
    "            print(f\"   í˜„ì¬ DB: {db_info[0]}\")\n",
    "            print(f\"   ì‚¬ìš©ì: {db_info[1]}\")\n",
    "            \n",
    "            existing_tables = [row[0] for row in result.fetchall()]\n",
    "            print(f\"\\nğŸ“Š ê¸°ì¡´ í…Œì´ë¸” ìˆ˜: {len(existing_tables)}ê°œ\")\n",
    "            if existing_tables:\n",
    "                for table in existing_tables:\n",
    "                    print(f\"   â€¢ {table}\")\n",
    "        \n",
    "        with engine.connect() as conn:\n",
    "            # íŠ¸ëœì­ì…˜ ì‹œì‘\n",
    "            trans = conn.begin()\n",
    "            \n",
    "            try:\n",
    "                # 1. ìƒí’ˆ ì •ë³´ í…Œì´ë¸” ìƒì„±\n",
    "                conn.execute(text(\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS kt_merged_product_20251015 (\n",
    "                        product_id VARCHAR(15),\n",
    "                        model_code VARCHAR(100),\n",
    "                        is_bespoke_goods VARCHAR(1),\n",
    "                        model_name VARCHAR(100),\n",
    "                        product_name VARCHAR(1000),\n",
    "                        display_category_major VARCHAR(1000),\n",
    "                        display_category_middle VARCHAR(1000),\n",
    "                        display_category_minor VARCHAR(1000),\n",
    "                        product_category_major VARCHAR(1000),\n",
    "                        product_category_middle VARCHAR(1000),\n",
    "                        product_category_minor VARCHAR(1000),\n",
    "                        product_color VARCHAR(1000),\n",
    "                        release_date DATE,\n",
    "                        is_ai_subscription_eligible VARCHAR(1),\n",
    "                        is_smart_subscription_eligible VARCHAR(1),\n",
    "                        is_galaxy_club_eligible VARCHAR(1),\n",
    "                        is_installment_payment_available VARCHAR(1),\n",
    "                        product_detail_url TEXT,\n",
    "                        site_code VARCHAR(10),\n",
    "                        unique_selling_point VARCHAR[],\n",
    "                        review_count INT4,\n",
    "                        review_rating_score NUMERIC(5,2),\n",
    "                        standard_price NUMERIC(10),\n",
    "                        member_price NUMERIC(10),\n",
    "                        benefit_price NUMERIC(10),\n",
    "                        product_specification JSONB,\n",
    "                        web_coupon_discount_amount NUMERIC(10),\n",
    "                        stock_quantity INT4,\n",
    "                        bundle_component_model_code VARCHAR[],\n",
    "                        is_bundle_product TEXT,\n",
    "                        final_price NUMERIC,\n",
    "                        review_text_collection JSONB,\n",
    "                        category_rank_recommend INT4,\n",
    "                        category_rank_quantity INT4,\n",
    "                        category_rank_rating INT4,\n",
    "                        total_sale_amount NUMERIC(15),\n",
    "                        total_sale_quantity INT4,\n",
    "                        event_info JSONB,\n",
    "                        coupon_info JSONB,\n",
    "                        promotion_info JSONB,\n",
    "                        payment_benefit_info JSONB\n",
    "                    );\n",
    "                \"\"\"))\n",
    "                print(\"   âœ… kt_merged_product_20251015 í…Œì´ë¸” ìƒì„±\")\n",
    "\n",
    "\n",
    "                # ì¸ë±ìŠ¤ ìƒì„±\n",
    "                conn.execute(text(\"CREATE INDEX IF NOT EXISTS idx_model_code ON kt_merged_product_20251015(model_code);\"))\n",
    "                conn.execute(text(\"CREATE INDEX IF NOT EXISTS idx_site_code ON kt_merged_product_20251015(site_code);\"))\n",
    "                conn.execute(text(\"CREATE INDEX IF NOT EXISTS idx_model_code_site_code ON kt_merged_product_20251015(model_code, site_code);\"))\n",
    "                conn.execute(text(\"CREATE INDEX IF NOT EXISTS idx_product_id ON kt_merged_product_20251015(product_id);\"))\n",
    "                conn.execute(text(\"CREATE INDEX IF NOT EXISTS idx_product_name ON kt_merged_product_20251015(product_name);\"))\n",
    "                print(\"   âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "\n",
    "                # ì»¤ë©˜íŠ¸ ìƒì„±\n",
    "                conn.execute(text(\"COMMENT ON TABLE kt_merged_product_20251015 IS 'ìƒí’ˆ í†µí•© ì •ë³´ í…Œì´ë¸”';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.product_id IS 'ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê° ì œí’ˆì„ ì‹ë³„í•˜ê¸° ìœ„í•œ ê³ ìœ  ID';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.model_code IS 'ì œí’ˆì˜ ì„¸ë¶€ ì‚¬ì–‘(ìƒ‰ìƒ, ìš©ëŸ‰ ë“±)ì„ í¬í•¨í•˜ëŠ” ê³ ìœ  ëª¨ë¸ ì½”ë“œ (ì˜ˆ: SM-S928NZ...)';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.is_bespoke_goods IS 'ë¹„ìŠ¤í¬í¬ ìƒí’ˆ ì—¬ë¶€';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.model_name IS 'ì œí’ˆì˜ ê³µì‹ ëª¨ë¸ëª… (ì˜ˆ: Galaxy S24 Ultra)';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.product_name IS 'ì›¹ì‚¬ì´íŠ¸ì— ìµœì¢…ì ìœ¼ë¡œ í‘œì‹œë˜ëŠ” ì œí’ˆëª… (ì˜ˆ: ê°¤ëŸ­ì‹œ S24 Ultra ìê¸‰ì œ 256GB í‹°íƒ€ëŠ„ ê·¸ë ˆì´)';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.display_category_major IS 'ì›¹ì‚¬ì´íŠ¸ì— ë…¸ì¶œë˜ëŠ” ì œí’ˆì˜ ëŒ€ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ëª… (ì˜ˆ: ëª¨ë°”ì¼, TV & ì˜¤ë””ì˜¤, ê°€ì „)';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.display_category_middle IS 'ì›¹ì‚¬ì´íŠ¸ì— ë…¸ì¶œë˜ëŠ” ì œí’ˆì˜ ì¤‘ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ëª… (ì˜ˆ: ìŠ¤ë§ˆíŠ¸í°, QLED, ë¹„ìŠ¤í¬í¬)';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.display_category_minor IS 'ì›¹ì‚¬ì´íŠ¸ì— ë…¸ì¶œë˜ëŠ” ì œí’ˆì˜ ì†Œë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ëª… (ì˜ˆ: Galaxy S, Neo QLED, ëƒ‰ì¥ê³ )';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.product_category_major IS 'ë‚´ë¶€ ì‹œìŠ¤í…œì—ì„œ ê´€ë¦¬í•˜ëŠ” ì œí’ˆì˜ ëŒ€ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ ì½”ë“œ ë˜ëŠ” ì´ë¦„';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.product_category_middle IS 'ë‚´ë¶€ ì‹œìŠ¤í…œì—ì„œ ê´€ë¦¬í•˜ëŠ” ì œí’ˆì˜ ì¤‘ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ ì½”ë“œ ë˜ëŠ” ì´ë¦„';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.product_category_minor IS 'ë‚´ë¶€ ì‹œìŠ¤í…œì—ì„œ ê´€ë¦¬í•˜ëŠ” ì œí’ˆì˜ ì†Œë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ ì½”ë“œ ë˜ëŠ” ì´ë¦„';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.product_color IS 'ì œí’ˆì˜ ìƒ‰ìƒëª… (ì˜ˆ: í‹°íƒ€ëŠ„ ë¸”ë™, ì½”íŠ¼ í™”ì´íŠ¸)';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.release_date IS 'ì œí’ˆì˜ ê³µì‹ ì¶œì‹œì¼';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.is_ai_subscription_eligible IS 'Galaxy AIì™€ ê°™ì€ AI ê´€ë ¨ êµ¬ë… ì„œë¹„ìŠ¤ ê°€ì… ê°€ëŠ¥ ì—¬ë¶€';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.is_smart_subscription_eligible IS 'ìŠ¤ë§ˆíŠ¸ ê¸°ê¸° êµ¬ë… ì„œë¹„ìŠ¤(ì‚¼ì„±ë‹·ì»´ êµ¬ë…) ê°€ì… ê°€ëŠ¥ ì—¬ë¶€';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.is_galaxy_club_eligible IS 'ê°¤ëŸ­ì‹œ í´ëŸ½ ê°€ì… ë˜ëŠ” í˜œíƒ ì ìš© ê°€ëŠ¥ ì—¬ë¶€';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.is_installment_payment_available IS 'í• ë¶€ ê²°ì œ ê°€ëŠ¥ ì—¬ë¶€';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.product_detail_url IS 'ì œí’ˆì˜ ìƒì„¸ í˜ì´ì§€ë¡œ ì—°ê²°ë˜ëŠ” ì „ì²´ URL ì£¼ì†Œ';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.site_code IS 'íŒë§¤ ì±„ë„ì„ êµ¬ë¶„í•˜ëŠ” ì½”ë“œ (ì˜ˆ: ì˜¨ë¼ì¸, B2B, íŠ¹ì • í”„ë¡œëª¨ì…˜)';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.unique_selling_point IS 'ì œí’ˆì˜ í•µì‹¬ íŠ¹ì¥ì ì„ ìš”ì•½í•œ ë¬¸êµ¬ ëª¨ìŒ (ì˜ˆ: Galaxy AI íƒ‘ì¬, 2ì–µ í™”ì†Œ ì¹´ë©”ë¼)';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.review_count IS 'í•´ë‹¹ ì œí’ˆì— ë‹¬ë¦° ë¦¬ë·°ì˜ ì´ ê°œìˆ˜';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.review_rating_score IS 'í•´ë‹¹ ì œí’ˆì˜ í‰ê·  ë¦¬ë·° ë³„ì  (ì˜ˆ: 4.8)';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.standard_price IS 'í• ì¸ì´ ì ìš©ë˜ì§€ ì•Šì€ ì •ìƒ íŒë§¤ê°€';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.member_price IS 'ì‚¼ì„±ë‹·ì»´ íšŒì›ì—ê²Œë§Œ ì ìš©ë˜ëŠ” í• ì¸ê°€';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.benefit_price IS 'ì¦‰ì‹œ í• ì¸, ì¿ í° ë“± ëª¨ë“  í˜œíƒì´ ì ìš©ëœ ìµœì¢… ê°€ê²©(í˜œíƒê°€)';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.product_specification IS 'ì œí’ˆì˜ ìƒì„¸ ê¸°ìˆ  ì‚¬ì–‘(ìŠ¤í™) ë°ì´í„° (JSON í˜•íƒœ)';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.web_coupon_discount_amount IS 'ì›¹ ì¿ í° ì ìš© ì‹œ í• ì¸ë˜ëŠ” ê¸ˆì•¡';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.stock_quantity IS 'íŒë§¤ ê°€ëŠ¥í•œ ì¬ê³  ìˆ˜ëŸ‰';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.bundle_component_model_code IS 'ë²ˆë“¤(íŒ¨í‚¤ì§€) ìƒí’ˆì¼ ê²½ìš°, êµ¬ì„±í’ˆ ê°ê°ì˜ ëª¨ë¸ ì½”ë“œ ëª©ë¡';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.is_bundle_product IS 'ë²ˆë“¤ ìƒí’ˆ ì—¬ë¶€';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.final_price IS 'ì›¹ ì¿ í° í• ì¸ ê¸ˆì•¡ì´ ì ìš©ëœ ê°€ê²©';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.review_text_collection IS 'ì‚¬ìš©ìë“¤ì´ ì‘ì„±í•œ ë¦¬ë·° í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ëª¨ìŒ';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.category_rank_recommend IS 'ì¹´í…Œê³ ë¦¬ ë‚´ì—ì„œ ì¶”ì²œìˆœì— ë”°ë¥¸ ì œí’ˆ ìˆœìœ„';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.category_rank_quantity IS 'ì¹´í…Œê³ ë¦¬ ë‚´ì—ì„œ íŒë§¤ëŸ‰ìˆœì— ë”°ë¥¸ ì œí’ˆ ìˆœìœ„';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.category_rank_rating IS 'ì¹´í…Œê³ ë¦¬ ë‚´ì—ì„œ í‰ì ìˆœì— ë”°ë¥¸ ì œí’ˆ ìˆœìœ„';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.total_sale_amount IS 'í•´ë‹¹ ì œí’ˆì˜ ì´ íŒë§¤ ê¸ˆì•¡';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.total_sale_quantity IS 'í•´ë‹¹ ì œí’ˆì˜ ì´ íŒë§¤ ìˆ˜ëŸ‰';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.event_info IS 'í•´ë‹¹ ì œí’ˆì´ í¬í•¨ë˜ëŠ” ì´ë²¤íŠ¸ì™€ í˜œíƒ ì •ë³´ ëª©ë¡';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.coupon_info IS 'í•´ë‹¹ ì œí’ˆë³„ì— ì ìš©í•  ìˆ˜ ìˆëŠ” ì¿ í° í˜œíƒ ëª©ë¡';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.promotion_info IS 'í•´ë‹¹ ì œí’ˆì„ ë¬¶ìŒìœ¼ë¡œ êµ¬ë§¤ ì‹œ ì œê³µë˜ëŠ” í”„ë¡œëª¨ì…˜ í˜œíƒ ëª©ë¡';\"))\n",
    "                conn.execute(text(\"COMMENT ON COLUMN kt_merged_product_20251015.payment_benefit_info IS 'ê²°ì œì™€ ê´€ë ¨ë˜ì–´ ì œê³µë˜ëŠ” í˜œíƒì˜ ëª©ë¡ (ì˜ˆ: ì‚¼ì„±ì¹´ë“œ 5% ì²­êµ¬í• ì¸)';\"))\n",
    "                print(\"   âœ… ì»¤ë©˜íŠ¸ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "                # íŠ¸ëœì­ì…˜ ì»¤ë°‹\n",
    "                trans.commit()\n",
    "                print(\"\\nâœ… ëª¨ë“  í…Œì´ë¸” ìƒì„± ì™„ë£Œ!\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                trans.rollback()\n",
    "                print(f\"âŒ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "                return None\n",
    "        \n",
    "        return engine\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ PostgreSQL ì—°ê²° ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "# PostgreSQL ì—°ê²° ë° í…Œì´ë¸” ìƒì„±\n",
    "pg_engine = postgresql_connection_and_create_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e34550",
   "metadata": {},
   "source": [
    "### ë¡œì»¬ íŒŒì¼ì˜ ë°ì´í„°ë¥¼ postgresqlì— ì—…ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c6f7363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL ì—°ê²° ì •ë³´: rubicon@dev-rubicon-postgresql.postgres.database.azure.com:5432/postgres\n",
      "TSV íŒŒì¼ ì½ê¸° ì‹œì‘: /Users/toby/prog/kt/rubicon/data/kt_merged_product_251015_20251015.tsv\n",
      "íŒŒì¼ í˜•ì‹ ê°ì§€: tsv, êµ¬ë¶„ì: '\t'\n",
      "DB í…Œì´ë¸”ì—ì„œ 41ê°œ ì»¬ëŸ¼ íƒ€ì… ì •ë³´ ì¡°íšŒ ì™„ë£Œ\n",
      "íŒŒì¼ ì½ê¸° ì™„ë£Œ: 2814 í–‰, 41 ì—´\n",
      "ì›ë³¸ ì»¬ëŸ¼: ['product_id', 'model_code', 'is_bespoke_goods', 'model_name', 'product_name', 'display_category_major', 'display_category_middle', 'display_category_minor', 'product_category_major', 'product_category_middle']...\n",
      "ì»¬ëŸ¼ ë§¤í•‘ ì™„ë£Œ: 41ê°œ ì»¬ëŸ¼\n",
      "ìµœì¢… DataFrame: 2814 í–‰, 41 ì—´\n",
      "DB í…Œì´ë¸”ì—ì„œ 41ê°œ ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ ì™„ë£Œ\n",
      "ë°ì´í„° ì‚½ì… ì‹œì‘: 2814ê°œ ë ˆì½”ë“œ\n",
      "ì§„í–‰: 100/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 200/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 300/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 400/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 500/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 600/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 700/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 800/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 900/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 1000/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 1100/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 1200/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 1300/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 1400/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 1500/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 1600/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 1700/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 1800/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 1900/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 2000/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 2100/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 2200/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 2300/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 2400/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 2500/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 2600/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 2700/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 2800/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "ì§„í–‰: 2814/2814 ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\n",
      "\n",
      "ë°ì´í„° ì‚½ì… ì™„ë£Œ:\n",
      "  - ì„±ê³µ: 2814ê°œ\n",
      "  - ì‹¤íŒ¨: 0ê°œ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_batch, Json\n",
    "from datetime import datetime\n",
    "import json\n",
    "from sqlalchemy import create_engine, inspect\n",
    "import csv\n",
    "\n",
    "# .env íŒŒì¼ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# PostgreSQL ì—°ê²° ì •ë³´\n",
    "PG_HOST = os.getenv('PG_HOST', 'localhost')\n",
    "PG_PORT = os.getenv('PG_PORT', '5432')\n",
    "PG_DATABASE = os.getenv('PG_DATABASE', 'postgres')\n",
    "PG_USER = os.getenv('PG_USER', 'postgres')\n",
    "PG_PASSWORD = os.getenv('PG_PASSWORD', '')\n",
    "\n",
    "print(f\"PostgreSQL ì—°ê²° ì •ë³´: {PG_USER}@{PG_HOST}:{PG_PORT}/{PG_DATABASE}\")\n",
    "\n",
    "def detect_file_format(file_path: str):\n",
    "    \"\"\"íŒŒì¼ í˜•ì‹ì„ ìë™ìœ¼ë¡œ ê°ì§€í•©ë‹ˆë‹¤.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # ì²« ëª‡ ì¤„ì„ ì½ì–´ì„œ í˜•ì‹ íŒë‹¨\n",
    "        first_line = file.readline()\n",
    "        \n",
    "        # CSV í˜•ì‹ (ë”°ì˜´í‘œë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ì‰¼í‘œ êµ¬ë¶„)\n",
    "        if '\",\"' in first_line:\n",
    "            return 'csv', ','\n",
    "        # TSV í˜•ì‹ (íƒ­ êµ¬ë¶„)\n",
    "        elif '\\t' in first_line:\n",
    "            return 'tsv', '\\t'\n",
    "        # ì‰¼í‘œ êµ¬ë¶„\n",
    "        elif ',' in first_line:\n",
    "            return 'csv', ','\n",
    "        else:\n",
    "            # ê¸°ë³¸ê°’ìœ¼ë¡œ CSV ì²˜ë¦¬\n",
    "            return 'csv', ','\n",
    "\n",
    "def parse_json_field(value):\n",
    "    \"\"\"JSON í•„ë“œë¥¼ íŒŒì‹±í•˜ì—¬ PostgreSQL Json ê°ì²´ë¡œ ë³€í™˜\"\"\"\n",
    "    # None ë˜ëŠ” NaN ì²˜ë¦¬\n",
    "    if pd.isna(value) or value is None:\n",
    "        return None\n",
    "    \n",
    "    # ì´ë¯¸ dict ë˜ëŠ” listì¸ ê²½ìš°\n",
    "    if isinstance(value, (dict, list)):\n",
    "        return Json(value)\n",
    "        \n",
    "    # ë¬¸ìì—´ì¸ ê²½ìš°\n",
    "    if isinstance(value, str):\n",
    "        value = value.strip()\n",
    "        \n",
    "        # ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬\n",
    "        if not value:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # JSON ë°°ì—´ [] í˜•ì‹ ì²˜ë¦¬\n",
    "            if value.startswith('['):\n",
    "                parsed = json.loads(value)\n",
    "                return Json(parsed)\n",
    "            # JSON ê°ì²´ {} í˜•ì‹ ì²˜ë¦¬\n",
    "            elif value.startswith('{'):\n",
    "                parsed = json.loads(value)\n",
    "                return Json(parsed)\n",
    "            # ë¹ˆ ë°°ì—´ì´ë‚˜ ê°ì²´ ë¬¸ìì—´ ì²˜ë¦¬\n",
    "            elif value in ['[]', '{}']:\n",
    "                parsed = json.loads(value)\n",
    "                return Json(parsed)\n",
    "            else:\n",
    "                # JSON í˜•ì‹ì´ ì•„ë‹Œ ê²½ìš° ë¬¸ìì—´ì„ ê·¸ëŒ€ë¡œ JSON ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "                return Json({\"value\": value})\n",
    "        except json.JSONDecodeError:\n",
    "            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬\n",
    "            # ì‘ì€ë”°ì˜´í‘œë¥¼ í°ë”°ì˜´í‘œë¡œ ë³€ê²½ í›„ ì¬ì‹œë„\n",
    "            try:\n",
    "                if value.startswith('[') or value.startswith('{'):\n",
    "                    value_cleaned = value.replace(\"'\", '\"')\n",
    "                    parsed = json.loads(value_cleaned)\n",
    "                    return Json(parsed)\n",
    "                else:\n",
    "                    # JSON í˜•ì‹ì´ ì•„ë‹ˆë©´ ë¬¸ìì—´ë¡œ ê°ì‹¸ì„œ ë°˜í™˜\n",
    "                    return Json({\"value\": value})\n",
    "            except:\n",
    "                # ê·¸ë˜ë„ ì‹¤íŒ¨í•˜ë©´ ë¬¸ìì—´ë¡œ ê°ì‹¸ì„œ ë°˜í™˜\n",
    "                return Json({\"value\": value})\n",
    "    \n",
    "    # ê¸°íƒ€ íƒ€ì…ì€ None ë°˜í™˜\n",
    "    return None\n",
    "\n",
    "def read_tsv_file(file_path: str, table_name: str = 'kt_merged_product_20251015') -> pd.DataFrame:\n",
    "    \"\"\"TSV íŒŒì¼ì„ ì½ê³  ì§€ì •ëœ í…Œì´ë¸” êµ¬ì¡°ì— ë§ê²Œ ë°ì´í„° íƒ€ì…ì„ ì„¤ì •í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    # íŒŒì¼ ê²½ë¡œ í™•ì¸ ë° ì°¾ê¸°\n",
    "    if not os.path.exists(file_path):\n",
    "        # í™˜ê²½ë³€ìˆ˜ì—ì„œ íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°\n",
    "        env_path = os.getenv('PG_UPLOAD_FILE_PATH')\n",
    "        if env_path and os.path.exists(env_path):\n",
    "            file_path = env_path\n",
    "        else:\n",
    "            # ê¸°ë³¸ ê²½ë¡œë“¤ í™•ì¸\n",
    "            possible_paths = [\n",
    "                '/Users/toby/prog/kt/rubicon/data/sr_merged_product_202509231550.tsv',\n",
    "                './data/sr_merged_product_202509231550.tsv',\n",
    "                '../data/sr_merged_product_202509231550.tsv'\n",
    "            ]\n",
    "            for path in possible_paths:\n",
    "                if os.path.exists(path):\n",
    "                    file_path = path\n",
    "                    break\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "    \n",
    "    print(f\"TSV íŒŒì¼ ì½ê¸° ì‹œì‘: {file_path}\")\n",
    "    \n",
    "    # íŒŒì¼ í˜•ì‹ ìë™ ê°ì§€\n",
    "    file_format, delimiter = detect_file_format(file_path)\n",
    "    print(f\"íŒŒì¼ í˜•ì‹ ê°ì§€: {file_format}, êµ¬ë¶„ì: '{delimiter}'\")\n",
    "    \n",
    "    # DB ì—°ê²°í•˜ì—¬ í…Œì´ë¸” êµ¬ì¡° í™•ì¸\n",
    "    POSTGRES_URL = f\"postgresql://{PG_USER}:{PG_PASSWORD}@{PG_HOST}:{PG_PORT}/{PG_DATABASE}\"\n",
    "    engine = create_engine(POSTGRES_URL)\n",
    "    \n",
    "    try:\n",
    "        inspector = inspect(engine)\n",
    "        columns_info = inspector.get_columns(table_name)\n",
    "        \n",
    "        # DB ì»¬ëŸ¼ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜\n",
    "        db_dtype_dict = {}\n",
    "        for col in columns_info:\n",
    "            col_name = col['name']\n",
    "            col_type = str(col['type'])\n",
    "            \n",
    "            # SQLAlchemy íƒ€ì…ì„ pandas dtypeìœ¼ë¡œ ë³€í™˜\n",
    "            if 'VARCHAR' in col_type.upper() or 'TEXT' in col_type.upper():\n",
    "                db_dtype_dict[col_name] = 'object'\n",
    "            elif 'INT' in col_type.upper() or 'SERIAL' in col_type.upper():\n",
    "                db_dtype_dict[col_name] = 'Int64'  # nullable integer\n",
    "            elif 'NUMERIC' in col_type.upper() or 'DECIMAL' in col_type.upper():\n",
    "                db_dtype_dict[col_name] = 'float64'\n",
    "            elif 'BOOL' in col_type.upper():\n",
    "                db_dtype_dict[col_name] = 'object'  # Y/N ì²˜ë¦¬ë¥¼ ìœ„í•´\n",
    "            elif 'DATE' in col_type.upper() or 'TIMESTAMP' in col_type.upper():\n",
    "                db_dtype_dict[col_name] = 'object'  # ë‚ ì§œëŠ” ë‚˜ì¤‘ì— íŒŒì‹±\n",
    "            elif 'JSON' in col_type.upper():\n",
    "                db_dtype_dict[col_name] = 'object'  # JSONì€ objectë¡œ\n",
    "            else:\n",
    "                db_dtype_dict[col_name] = 'object'\n",
    "                \n",
    "        print(f\"DB í…Œì´ë¸”ì—ì„œ {len(db_dtype_dict)}ê°œ ì»¬ëŸ¼ íƒ€ì… ì •ë³´ ì¡°íšŒ ì™„ë£Œ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"DB í…Œì´ë¸” êµ¬ì¡° ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        engine.dispose()\n",
    "    \n",
    "    # ì»¬ëŸ¼ ë§¤í•‘ ì •ì˜ (ì›ë³¸ íŒŒì¼ ì»¬ëŸ¼ -> DB í…Œì´ë¸” ì»¬ëŸ¼)\n",
    "    column_mapping = {\n",
    "        'disp_lv1': 'display_category_major',\n",
    "        'disp_lv2': 'display_category_middle', \n",
    "        'disp_lv3': 'display_category_minor',\n",
    "        'product_category_lv1': 'product_category_major',\n",
    "        'product_category_lv2': 'product_category_middle',\n",
    "        'product_category_lv3': 'product_category_minor',\n",
    "        'mdl_code': 'model_code',\n",
    "        'goods_id': 'product_id',\n",
    "        'goods_nm': 'product_name',\n",
    "        'color': 'product_color',\n",
    "        'release_dt': 'release_date',\n",
    "        'ai_eligibility': 'is_ai_subscription_eligible',\n",
    "        'smart_eligibility': 'is_smart_subscription_eligible', \n",
    "        'galaxy_eligibility': 'is_galaxy_club_eligible',\n",
    "        'installment_payment': 'is_installment_payment_available',\n",
    "        'pd_url': 'product_detail_url',\n",
    "        'selling_pt': 'unique_selling_point',\n",
    "        'review_qty': 'review_count',\n",
    "        'review_score': 'review_rating_score',\n",
    "        'sale_prc1': 'standard_price',\n",
    "        'sale_prc2': 'member_price',\n",
    "        'sale_prc3': 'benefit_price',\n",
    "        'sale_prc': 'final_price',\n",
    "        'review_content': 'review_text_collection',\n",
    "        'spec': 'product_specification',  # spec -> product_specification ë§¤í•‘ ì¶”ê°€\n",
    "        'web_cp_dc_amt': 'web_coupon_discount',\n",
    "        'stock_qty': 'stock_quantity',\n",
    "        'ctg_rank_recommend': 'category_rank_recommend',\n",
    "        'ctg_rank_qty': 'category_rank_quantity',\n",
    "        'ctg_rank_score': 'category_rank_rating',\n",
    "        'card_promotion': 'payment_benefit_info'  # card_promotion -> payment_benefit_info ë§¤í•‘ ì¶”ê°€\n",
    "    }\n",
    "    \n",
    "    # CSV íŒŒì¼ ì½ê¸° (ë”°ì˜´í‘œ ì²˜ë¦¬ í¬í•¨)\n",
    "    try:\n",
    "        if file_format == 'csv':\n",
    "            df = pd.read_csv(file_path, \n",
    "                           sep=delimiter,\n",
    "                           encoding='utf-8',\n",
    "                           quotechar='\"',\n",
    "                           quoting=csv.QUOTE_ALL,\n",
    "                           on_bad_lines='skip',\n",
    "                           engine='python')\n",
    "        else:\n",
    "            df = pd.read_csv(file_path,\n",
    "                           sep=delimiter, \n",
    "                           encoding='utf-8',\n",
    "                           on_bad_lines='skip',\n",
    "                           engine='python')\n",
    "                           \n",
    "        print(f\"íŒŒì¼ ì½ê¸° ì™„ë£Œ: {len(df)} í–‰, {len(df.columns)} ì—´\")\n",
    "        print(f\"ì›ë³¸ ì»¬ëŸ¼: {list(df.columns[:10])}...\")\n",
    "        \n",
    "        # ì»¬ëŸ¼ëª… ë§¤í•‘ ì ìš©\n",
    "        df = df.rename(columns=column_mapping)\n",
    "        print(f\"ì»¬ëŸ¼ ë§¤í•‘ ì™„ë£Œ: {len(df.columns)}ê°œ ì»¬ëŸ¼\")\n",
    "        \n",
    "        # DBì— ìˆëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ (êµì§‘í•©)\n",
    "        available_columns = list(set(df.columns) & set(db_dtype_dict.keys()))\n",
    "        df = df[available_columns]\n",
    "        \n",
    "        # DBì—ëŠ” ìˆì§€ë§Œ ë°ì´í„°ì— ì—†ëŠ” ì»¬ëŸ¼ ì¶”ê°€\n",
    "        missing_columns = set(db_dtype_dict.keys()) - set(df.columns)\n",
    "        for col in missing_columns:\n",
    "            df[col] = None\n",
    "            \n",
    "        # ì»¬ëŸ¼ ìˆœì„œë¥¼ DB í…Œì´ë¸”ê³¼ ë™ì¼í•˜ê²Œ ì •ë ¬\n",
    "        df = df[list(db_dtype_dict.keys())]\n",
    "        \n",
    "        print(f\"ìµœì¢… DataFrame: {len(df)} í–‰, {len(df.columns)} ì—´\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}\")\n",
    "        print(f\"ì²« 5ì¤„ ë‚´ìš© í™•ì¸:\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= 5:\n",
    "                    break\n",
    "                print(f\"Line {i+1}: {line[:200]}...\")\n",
    "        raise\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_data_for_insert(df: pd.DataFrame, table_name: str = 'kt_merged_product_20251015') -> List[tuple]:\n",
    "    \"\"\"DataFrameì„ PostgreSQL ì‚½ì…ìš© íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\"\"\"\n",
    "    records = []\n",
    "    \n",
    "    # DB ì—°ê²°í•˜ì—¬ í…Œì´ë¸” êµ¬ì¡° í™•ì¸\n",
    "    POSTGRES_URL = f\"postgresql://{PG_USER}:{PG_PASSWORD}@{PG_HOST}:{PG_PORT}/{PG_DATABASE}\"\n",
    "    engine = create_engine(POSTGRES_URL)\n",
    "    \n",
    "    try:\n",
    "        inspector = inspect(engine)\n",
    "        columns_info = inspector.get_columns(table_name)\n",
    "        \n",
    "        # DB í…Œì´ë¸” ì»¬ëŸ¼ ìˆœì„œì™€ íƒ€ì… ì •ë³´ ìˆ˜ì§‘\n",
    "        db_columns = []\n",
    "        column_types = {}\n",
    "        \n",
    "        for col in columns_info:\n",
    "            col_name = col['name']\n",
    "            col_type = str(col['type'])\n",
    "            db_columns.append(col_name)\n",
    "            column_types[col_name] = col_type\n",
    "            \n",
    "        print(f\"DB í…Œì´ë¸”ì—ì„œ {len(db_columns)}ê°œ ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ ì™„ë£Œ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"DB í…Œì´ë¸” êµ¬ì¡° ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        engine.dispose()\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        record = []\n",
    "        \n",
    "        try:\n",
    "            for table_col in db_columns:\n",
    "                # DataFrameì—ì„œ í•´ë‹¹ ì»¬ëŸ¼ ì°¾ê¸°\n",
    "                if table_col in df.columns:\n",
    "                    value = row[table_col]\n",
    "                else:\n",
    "                    # íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì²˜ë¦¬\n",
    "                    if table_col == 'is_bespoke_goods':\n",
    "                        # bespoke ê´€ë ¨ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ None\n",
    "                        value = None\n",
    "                    elif table_col == 'is_bundle_product':\n",
    "                        # bundle_component_model_codeê°€ ìˆìœ¼ë©´ Y, ì—†ìœ¼ë©´ N\n",
    "                        if 'bundle_component_model_code' in df.columns:\n",
    "                            bundle_val = row.get('bundle_component_model_code')\n",
    "                            value = 'Y' if pd.notna(bundle_val) and str(bundle_val).strip() else 'N'\n",
    "                        else:\n",
    "                            value = 'N'\n",
    "                    elif table_col == 'web_coupon_discount_amount':\n",
    "                        # web_coupon_discountì™€ ë™ì¼\n",
    "                        value = row.get('web_coupon_discount', None)\n",
    "                    elif table_col == 'event_info':\n",
    "                        # event_infoëŠ” ì›ë³¸ì— ì—†ìœ¼ë¯€ë¡œ ë¹ˆ JSON ê°ì²´ ìƒì„±\n",
    "                        value = '{}'\n",
    "                    elif table_col == 'coupon_info':\n",
    "                        # coupon_infoëŠ” ì›ë³¸ì— ì—†ìœ¼ë¯€ë¡œ ë¹ˆ JSON ê°ì²´ ìƒì„±\n",
    "                        value = '{}'\n",
    "                    elif table_col == 'promotion_info':\n",
    "                        # promotion_infoëŠ” ì›ë³¸ì— ì—†ìœ¼ë¯€ë¡œ ë¹ˆ JSON ê°ì²´ ìƒì„±  \n",
    "                        value = '{}'\n",
    "                    elif table_col in ['total_sale_amount', 'total_sale_quantity']:\n",
    "                        # íŒë§¤ ì´ì•¡/ìˆ˜ëŸ‰ - ì—†ìœ¼ë©´ None\n",
    "                        value = None\n",
    "                    else:\n",
    "                        record.append(None)\n",
    "                        continue\n",
    "                \n",
    "                # ë°ì´í„° íƒ€ì…ë³„ ì²˜ë¦¬\n",
    "                col_type = column_types[table_col].upper()\n",
    "                \n",
    "                # ë¬¸ìì—´ ì²˜ë¦¬\n",
    "                if 'VARCHAR' in col_type or 'TEXT' in col_type or 'CHAR' in col_type:\n",
    "                    if pd.notna(value):\n",
    "                        # Boolean íƒ€ì…ì˜ íŠ¹ìˆ˜ ì²˜ë¦¬ (Y/N ê°’)\n",
    "                        if table_col in ['is_bespoke_goods', 'is_ai_subscription_eligible', \n",
    "                                        'is_smart_subscription_eligible', 'is_galaxy_club_eligible',\n",
    "                                        'is_installment_payment_available', 'is_bundle_product']:\n",
    "                            val_str = str(value).strip().upper()\n",
    "                            if val_str in ['TRUE', '1', 'Y', 'YES', 'T']:\n",
    "                                record.append('Y')\n",
    "                            elif val_str in ['FALSE', '0', 'N', 'NO', 'F']:\n",
    "                                record.append('N')\n",
    "                            else:\n",
    "                                record.append(None)\n",
    "                        else:\n",
    "                            # ë¬¸ìì—´ ì •ë¦¬\n",
    "                            val = str(value).strip()\n",
    "                            # ë”°ì˜´í‘œ ì œê±°\n",
    "                            if val.startswith('\"') and val.endswith('\"'):\n",
    "                                val = val[1:-1]\n",
    "                            record.append(val if val else None)\n",
    "                    else:\n",
    "                        record.append(None)\n",
    "                        \n",
    "                # ì •ìˆ˜ ì²˜ë¦¬\n",
    "                elif 'INT' in col_type or 'SERIAL' in col_type:\n",
    "                    if pd.notna(value):\n",
    "                        try:\n",
    "                            # ë¬¸ìì—´ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ\n",
    "                            val_str = str(value).strip().replace(',', '').replace('\"', '')\n",
    "                            if val_str and val_str != 'nan':\n",
    "                                record.append(int(float(val_str)))\n",
    "                            else:\n",
    "                                record.append(None)\n",
    "                        except:\n",
    "                            record.append(None)\n",
    "                    else:\n",
    "                        record.append(None)\n",
    "                        \n",
    "                # ì‹¤ìˆ˜ ì²˜ë¦¬  \n",
    "                elif 'NUMERIC' in col_type or 'DECIMAL' in col_type or 'FLOAT' in col_type or 'DOUBLE' in col_type:\n",
    "                    if pd.notna(value):\n",
    "                        try:\n",
    "                            val_str = str(value).strip().replace(',', '').replace('\"', '')\n",
    "                            if val_str and val_str != 'nan':\n",
    "                                record.append(float(val_str))\n",
    "                            else:\n",
    "                                record.append(None)\n",
    "                        except:\n",
    "                            record.append(None)\n",
    "                    else:\n",
    "                        record.append(None)\n",
    "                        \n",
    "                # ë‚ ì§œ ì²˜ë¦¬\n",
    "                elif 'DATE' in col_type or 'TIMESTAMP' in col_type:\n",
    "                    if pd.notna(value):\n",
    "                        try:\n",
    "                            # ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹±\n",
    "                            val_str = str(value).strip().replace('\"', '')\n",
    "                            if val_str and val_str != 'nan':\n",
    "                                # YYYYMMDD í˜•ì‹ ì²˜ë¦¬\n",
    "                                if len(val_str) == 8 and val_str.isdigit():\n",
    "                                    date_val = datetime.strptime(val_str, '%Y%m%d').date()\n",
    "                                    record.append(date_val)\n",
    "                                # YYYY-MM-DD í˜•ì‹ ì²˜ë¦¬\n",
    "                                elif '-' in val_str:\n",
    "                                    date_val = datetime.strptime(val_str.split(' ')[0], '%Y-%m-%d').date()\n",
    "                                    record.append(date_val)\n",
    "                                else:\n",
    "                                    record.append(None)\n",
    "                            else:\n",
    "                                record.append(None)\n",
    "                        except:\n",
    "                            record.append(None)\n",
    "                    else:\n",
    "                        record.append(None)\n",
    "                        \n",
    "                # JSONB ì²˜ë¦¬\n",
    "                elif 'JSON' in col_type:\n",
    "                    json_value = parse_json_field(value)\n",
    "                    record.append(json_value)\n",
    "                    \n",
    "                else:\n",
    "                    record.append(None)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"ë ˆì½”ë“œ {idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "            continue\n",
    "            \n",
    "        if len(record) == len(db_columns):\n",
    "            records.append(tuple(record))\n",
    "        else:\n",
    "            print(f\"ë ˆì½”ë“œ {idx}: ì»¬ëŸ¼ ìˆ˜ ë¶ˆì¼ì¹˜ ({len(record)} vs {len(db_columns)})\")\n",
    "    \n",
    "    return records\n",
    "\n",
    "def insert_data_to_postgres(records: List[tuple], table_name: str = 'kt_merged_product_20251015', batch_size: int = 100):\n",
    "    \"\"\"ë°ì´í„°ë¥¼ PostgreSQLì— ì‚½ì…\"\"\"\n",
    "    conn = None\n",
    "    cur = None\n",
    "    \n",
    "    try:\n",
    "        # PostgreSQL ì—°ê²°\n",
    "        conn = psycopg2.connect(\n",
    "            host=PG_HOST,\n",
    "            port=PG_PORT,\n",
    "            database=PG_DATABASE,\n",
    "            user=PG_USER,\n",
    "            password=PG_PASSWORD\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # í…Œì´ë¸” ì»¬ëŸ¼ ëª©ë¡ ì¡°íšŒ\n",
    "        cur.execute(f\"\"\"\n",
    "            SELECT column_name \n",
    "            FROM information_schema.columns\n",
    "            WHERE table_name = '{table_name}'\n",
    "            ORDER BY ordinal_position\n",
    "        \"\"\")\n",
    "        columns = [row[0] for row in cur.fetchall()]\n",
    "        \n",
    "        # INSERT ì¿¼ë¦¬ ìƒì„±\n",
    "        placeholders = ','.join(['%s'] * len(columns))\n",
    "        insert_query = f\"INSERT INTO {table_name} ({','.join(columns)}) VALUES ({placeholders})\"\n",
    "        \n",
    "        print(f\"ë°ì´í„° ì‚½ì… ì‹œì‘: {len(records)}ê°œ ë ˆì½”ë“œ\")\n",
    "        \n",
    "        # ë°°ì¹˜ ì‚½ì…\n",
    "        success_count = 0\n",
    "        error_count = 0\n",
    "        \n",
    "        for i in range(0, len(records), batch_size):\n",
    "            batch = records[i:i+batch_size]\n",
    "            try:\n",
    "                execute_batch(cur, insert_query, batch, page_size=batch_size)\n",
    "                conn.commit()\n",
    "                success_count += len(batch)\n",
    "                print(f\"ì§„í–‰: {success_count}/{len(records)} ë ˆì½”ë“œ ì‚½ì… ì™„ë£Œ\")\n",
    "            except Exception as batch_error:\n",
    "                conn.rollback()\n",
    "                print(f\"ë°°ì¹˜ {i//batch_size + 1} ì‚½ì… ì‹¤íŒ¨: {batch_error}\")\n",
    "                \n",
    "                # ì‹¤íŒ¨í•œ ë°°ì¹˜ëŠ” ê°œë³„ ì‚½ì… ì‹œë„\n",
    "                for record in batch:\n",
    "                    try:\n",
    "                        cur.execute(insert_query, record)\n",
    "                        conn.commit()\n",
    "                        success_count += 1\n",
    "                    except Exception as record_error:\n",
    "                        conn.rollback()\n",
    "                        error_count += 1\n",
    "                        print(f\"ë ˆì½”ë“œ ì‚½ì… ì‹¤íŒ¨: {record_error}\")\n",
    "        \n",
    "        print(f\"\\në°ì´í„° ì‚½ì… ì™„ë£Œ:\")\n",
    "        print(f\"  - ì„±ê³µ: {success_count}ê°œ\")\n",
    "        print(f\"  - ì‹¤íŒ¨: {error_count}ê°œ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {e}\")\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "    finally:\n",
    "        if cur:\n",
    "            cur.close()\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # TSV íŒŒì¼ ì½ê¸°\n",
    "        file_path = os.getenv('PG_UPLOAD_FILE_PATH', '/Users/toby/prog/kt/rubicon/data/sr_merged_product_202509231550.tsv')\n",
    "        df = read_tsv_file(file_path)\n",
    "        \n",
    "        # ë°ì´í„° ì¤€ë¹„\n",
    "        records = prepare_data_for_insert(df)\n",
    "        \n",
    "        # PostgreSQLì— ì‚½ì…\n",
    "        if records:\n",
    "            insert_data_to_postgres(records)\n",
    "        else:\n",
    "            print(\"ì‚½ì…í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dxao8x3xx48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== í‰íƒ„í™” í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ===\n",
      "\n",
      "ì¤‘ë³µ í‚¤ í…ŒìŠ¤íŠ¸:\n",
      "ì…ë ¥: {'ìƒ‰ìƒ': 'ë¸”ë™', 'ì¶”ê°€': {'ìƒ‰ìƒ': 'ë¸”ë™'}}\n",
      "ê²°ê³¼: {'ìƒ‰ìƒ': ['ë¸”ë™', 'ë¸”ë™']}\n",
      "ì˜ˆìƒ: {'ìƒ‰ìƒ': ['ë¸”ë™', 'ë¸”ë™']}\n",
      "\n",
      "ë³µí•© ì¤‘ì²© êµ¬ì¡°:\n",
      "ì…ë ¥: {'ì‚¬ìš´ë“œ': {'ê°ë„': '100dB', 'ìƒ‰ìƒ': 'ì‹¤ë²„'}, 'ìƒ‰ìƒ': 'ë¸”ë™', 'ì¶”ê°€': {'ìƒ‰ìƒ': 'í™”ì´íŠ¸'}}\n",
      "ê²°ê³¼: {'ìƒ‰ìƒ': ['ë¸”ë™', 'ì‹¤ë²„', 'í™”ì´íŠ¸'], 'ê°ë„': ['100dB']}\n",
      "ì˜ˆìƒ: {'ê°ë„': ['100dB'], 'ìƒ‰ìƒ': ['ë¸”ë™', 'ì‹¤ë²„', 'í™”ì´íŠ¸']}\n",
      "\n",
      "=== ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ ì²˜ë¦¬ ===\n",
      "í…Œì´ë¸” kt_merged_product_20251015_modì˜ product_specification ì»¬ëŸ¼ ì²˜ë¦¬ ì‹œì‘...\n",
      "í˜„ì¬ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼: ['ori_product_specification', 'product_specification']\n",
      "ê¸°ì¡´ ë°ì´í„°ë¥¼ ori_product_specificationìœ¼ë¡œ ë³µì‚¬ ì¤‘...\n",
      "ë³µì‚¬ëœ í–‰ ìˆ˜: 0\n",
      "ë°ì´í„° ì¡°íšŒ ë° í‰íƒ„í™” ì²˜ë¦¬ ì‹œì‘...\n",
      "ì²˜ë¦¬ ì¤‘... (1000 í–‰ ì™„ë£Œ, 0 ì˜¤ë¥˜)\n",
      "ì²˜ë¦¬ ì¤‘... (2000 í–‰ ì™„ë£Œ, 0 ì˜¤ë¥˜)\n",
      "ì²˜ë¦¬ ì¤‘... (2812 í–‰ ì™„ë£Œ, 0 ì˜¤ë¥˜)\n",
      "ì´ 2812ê°œ í–‰ì˜ product_specification ì»¬ëŸ¼ í‰íƒ„í™” ì™„ë£Œ\n",
      "\n",
      "í‰íƒ„í™”ëœ ë°ì´í„° ìƒ˜í”Œ í™•ì¸:\n",
      "\n",
      "Product ID: G000192665\n",
      "ì›ë³¸ êµ¬ì¡° í‚¤: ['ìƒ‰ìƒ', 'ì¼ë°˜', 'ì¶”ê°€', 'ì‚¬ìš´ë“œ', 'ì™¸ê´€ ì‚¬ì–‘']\n",
      "í‰íƒ„í™”ëœ êµ¬ì¡° í‚¤ ê°œìˆ˜: 18\n",
      "  ê°ë„: ['97dB SPL @ 1kHz/1mW'] (ë¦¬ìŠ¤íŠ¸, 1ê°œ í•­ëª©)\n",
      "  ê¸°íƒ€: ['TwistLockâ„¢ ê¸°ìˆ  ì ìš©', 'Quantum ì‹œê·¸ë‹ˆì²˜ ì‚¬ìš´ë“œ'] (ë¦¬ìŠ¤íŠ¸, 2ê°œ í•­ëª©)\n",
      "  ë¬´ê²Œ: ['21.5 g'] (ë¦¬ìŠ¤íŠ¸, 1ê°œ í•­ëª©)\n",
      "  ìƒ‰ìƒ: ['ë¸”ë™', 'ë¸”ë™'] (ë¦¬ìŠ¤íŠ¸, 2ê°œ í•­ëª©)\n",
      "  ì œì¡°êµ­: ['ì¤‘êµ­'] (ë¦¬ìŠ¤íŠ¸, 1ê°œ í•­ëª©)\n",
      "\n",
      "Product ID: G000192695\n",
      "ì›ë³¸ êµ¬ì¡° í‚¤: ['ë³µì‚¬', 'ìŠ¤ìº”', 'ì¸ì‡„', 'ì¶”ê°€', 'ì†Œëª¨í’ˆ']\n",
      "í‰íƒ„í™”ëœ êµ¬ì¡° í‚¤ ê°œìˆ˜: 34\n",
      "  ê¸°íƒ€: ['ìµœì´ˆ êµ¬ì…ì‹œ í‘ë°± 240ì¥, ì»¬ëŸ¬ 200ì¥ / ì‰í¬ìš©ëŸ‰ ì¸¡ì •ê¸°ì¤€ ISO/IEC 24711'] (ë¦¬ìŠ¤íŠ¸, 1ê°œ í•­ëª©)\n",
      "  ë¬´ê²Œ: ['3.42 kg'] (ë¦¬ìŠ¤íŠ¸, 1ê°œ í•­ëª©)\n",
      "  ë°©ì‹: ['CIS / Flatbed'] (ë¦¬ìŠ¤íŠ¸, 1ê°œ í•­ëª©)\n",
      "  ìƒ‰ìƒ: ['í™”ì´íŠ¸'] (ë¦¬ìŠ¤íŠ¸, 1ê°œ í•­ëª©)\n",
      "  ì¶œì‹œì¼: ['2021ë…„ 5ì›”'] (ë¦¬ìŠ¤íŠ¸, 1ê°œ í•­ëª©)\n",
      "\n",
      "Product ID: G000192705\n",
      "ì›ë³¸ êµ¬ì¡° í‚¤: ['ì¶”ê°€', 'ê¸°ë³¸ ì‚¬ì–‘']\n",
      "í‰íƒ„í™”ëœ êµ¬ì¡° í‚¤ ê°œìˆ˜: 4\n",
      "  ë§¤ìˆ˜: ['240ì¥'] (ë¦¬ìŠ¤íŠ¸, 1ê°œ í•­ëª©)\n",
      "  ìƒ‰ìƒ: ['ë¸”ë™'] (ë¦¬ìŠ¤íŠ¸, 1ê°œ í•­ëª©)\n",
      "  í˜¸í™˜ ê¸°ê¸°: ['SL-J1680/1683/1685'] (ë¦¬ìŠ¤íŠ¸, 1ê°œ í•­ëª©)\n",
      "  ì†Œëª¨í’ˆ ì»¬ëŸ¬: ['ë¸”ë™'] (ë¦¬ìŠ¤íŠ¸, 1ê°œ í•­ëª©)\n",
      "\n",
      "ì¤‘ë³µ í‚¤ê°€ ë³‘í•©ëœ ì˜ˆì‹œ í™•ì¸:\n",
      "\n",
      "Product ID G000192665ì—ì„œ ì¤‘ë³µ ë³‘í•©ëœ í‚¤:\n",
      "  ê¸°íƒ€: ['TwistLockâ„¢ ê¸°ìˆ  ì ìš©', 'Quantum ì‹œê·¸ë‹ˆì²˜ ì‚¬ìš´ë“œ']\n",
      "  ìƒ‰ìƒ: ['ë¸”ë™', 'ë¸”ë™']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_batch, Json\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "\n",
    "# .env íŒŒì¼ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    return psycopg2.connect(\n",
    "        host=os.getenv('PG_HOST'),\n",
    "        port=os.getenv('PG_PORT', 5432),\n",
    "        database=os.getenv('PG_DATABASE'),\n",
    "        user=os.getenv('PG_USER'),\n",
    "        password=os.getenv('PG_PASSWORD')\n",
    "    )\n",
    "\n",
    "def flatten_jsonb_structure(jsonb_data):\n",
    "    \"\"\"\n",
    "    ì¤‘ì²©ëœ JSONB êµ¬ì¡°ë¥¼ í‰íƒ„í™”í•˜ì—¬ ìµœí•˜ìœ„ key-valueë§Œ ìµœìƒìœ„ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    ëª¨ë“  ê°’ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³ , ë™ì¼í•œ í‚¤ì˜ ê°’ë“¤ì€ ë°°ì—´ì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        jsonb_data: JSON ë¬¸ìì—´, ë”•ì…”ë„ˆë¦¬, ë˜ëŠ” psycopg2ì˜ JSONB ê°ì²´\n",
    "    \n",
    "    Returns:\n",
    "        í‰íƒ„í™”ëœ ë”•ì…”ë„ˆë¦¬ (ëª¨ë“  ê°’ì´ ë¦¬ìŠ¤íŠ¸)\n",
    "    \"\"\"\n",
    "    # None ì²´í¬\n",
    "    if jsonb_data is None:\n",
    "        return {}\n",
    "    \n",
    "    # numpy/pandas NaN ì²´í¬\n",
    "    try:\n",
    "        if pd.isna(jsonb_data):\n",
    "            return {}\n",
    "    except (TypeError, ValueError):\n",
    "        # pd.isnaê°€ ì‹¤íŒ¨í•˜ëŠ” ê²½ìš° (ì˜ˆ: dict, list ë“±)\n",
    "        pass\n",
    "    \n",
    "    # ë¹ˆ ë¬¸ìì—´ ì²´í¬\n",
    "    if jsonb_data == '':\n",
    "        return {}\n",
    "    \n",
    "    # psycopg2ì—ì„œ ë°˜í™˜ëœ ë”•ì…”ë„ˆë¦¬ë‚˜ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "    if isinstance(jsonb_data, (dict, list)):\n",
    "        data = jsonb_data\n",
    "    # ë¬¸ìì—´ì¸ ê²½ìš° JSONìœ¼ë¡œ íŒŒì‹±\n",
    "    elif isinstance(jsonb_data, str):\n",
    "        try:\n",
    "            data = json.loads(jsonb_data)\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            return {}\n",
    "    else:\n",
    "        # ê¸°íƒ€ íƒ€ì…ì€ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜\n",
    "        return {}\n",
    "    \n",
    "    # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œê°€ ë”•ì…”ë„ˆë¦¬ë©´ ì²˜ë¦¬, ì•„ë‹ˆë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬\n",
    "    if isinstance(data, list):\n",
    "        if len(data) > 0 and isinstance(data[0], dict):\n",
    "            data = data[0]\n",
    "        else:\n",
    "            return {}\n",
    "    \n",
    "    # ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš° ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜\n",
    "    if not isinstance(data, dict):\n",
    "        return {}\n",
    "    \n",
    "    # ì¤‘ë³µ í‚¤ì˜ ê°’ë“¤ì„ ìˆ˜ì§‘í•˜ê¸° ìœ„í•œ ë”•ì…”ë„ˆë¦¬\n",
    "    collected_values = {}\n",
    "    \n",
    "    def extract_leaf_values(obj, parent_key=''):\n",
    "        \"\"\"ì¬ê·€ì ìœ¼ë¡œ ìµœí•˜ìœ„ leaf ë…¸ë“œì˜ key-valueë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                # valueê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° ì¬ê·€ í˜¸ì¶œ\n",
    "                if isinstance(value, dict):\n",
    "                    extract_leaf_values(value, key)\n",
    "                else:\n",
    "                    # leaf ë…¸ë“œì¸ ê²½ìš° collected_valuesì— ì¶”ê°€\n",
    "                    if key not in collected_values:\n",
    "                        collected_values[key] = []\n",
    "                    \n",
    "                    # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê° ìš”ì†Œë¥¼ ì¶”ê°€, ì•„ë‹Œ ê²½ìš° ë‹¨ì¼ ê°’ ì¶”ê°€\n",
    "                    if isinstance(value, list):\n",
    "                        collected_values[key].extend(value)\n",
    "                    elif value is not None:\n",
    "                        collected_values[key].append(value)\n",
    "        else:\n",
    "            # ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš°\n",
    "            if parent_key:\n",
    "                if parent_key not in collected_values:\n",
    "                    collected_values[parent_key] = []\n",
    "                    \n",
    "                if isinstance(obj, list):\n",
    "                    collected_values[parent_key].extend(obj)\n",
    "                elif obj is not None:\n",
    "                    collected_values[parent_key].append(obj)\n",
    "    \n",
    "    # ìµœìƒìœ„ ë ˆë²¨ë¶€í„° ì²˜ë¦¬\n",
    "    if isinstance(data, dict):\n",
    "        # ë¨¼ì € ìµœìƒìœ„ ë ˆë²¨ì˜ ëª¨ë“  í‚¤-ê°’ ì²˜ë¦¬\n",
    "        for key, value in data.items():\n",
    "            if not isinstance(value, dict):\n",
    "                # ìµœìƒìœ„ ë ˆë²¨ì˜ non-dict ê°’ë“¤ì„ ë¨¼ì € ìˆ˜ì§‘\n",
    "                if key not in collected_values:\n",
    "                    collected_values[key] = []\n",
    "                    \n",
    "                if isinstance(value, list):\n",
    "                    collected_values[key].extend(value)\n",
    "                elif value is not None:\n",
    "                    collected_values[key].append(value)\n",
    "        \n",
    "        # ê·¸ ë‹¤ìŒ ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬ ì²˜ë¦¬\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, dict):\n",
    "                extract_leaf_values(value)\n",
    "    \n",
    "    # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì§„ í‚¤ëŠ” ì œê±°í•˜ì§€ ì•Šê³  ìœ ì§€\n",
    "    flattened = {k: v if v else [] for k, v in collected_values.items()}\n",
    "    \n",
    "    return flattened\n",
    "\n",
    "def update_product_specification_column(table_name='kt_merged_product_20251015', batch_size=1000):\n",
    "    \"\"\"\n",
    "    product_specification ì»¬ëŸ¼ì„ ori_product_specificationìœ¼ë¡œ ë³€ê²½í•˜ê³ ,\n",
    "    í‰íƒ„í™”ëœ êµ¬ì¡°ë¥¼ ìƒˆë¡œìš´ product_specification ì»¬ëŸ¼ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    ëª¨ë“  ê°’ì€ ë°°ì—´ í˜•íƒœë¡œ ì €ì¥ë˜ë©°, ë™ì¼ í‚¤ì˜ ê°’ë“¤ì€ ë³‘í•©ë©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    cursor = None\n",
    "    \n",
    "    try:\n",
    "        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°\n",
    "        conn = get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        print(f\"í…Œì´ë¸” {table_name}ì˜ product_specification ì»¬ëŸ¼ ì²˜ë¦¬ ì‹œì‘...\")\n",
    "        \n",
    "        # 1. ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT column_name \n",
    "            FROM information_schema.columns \n",
    "            WHERE table_name = %s \n",
    "            AND column_name IN ('product_specification', 'ori_product_specification')\n",
    "        \"\"\", (table_name,))\n",
    "        \n",
    "        existing_columns = [row[0] for row in cursor.fetchall()]\n",
    "        print(f\"í˜„ì¬ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼: {existing_columns}\")\n",
    "        \n",
    "        # 2. ori_product_specification ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¶”ê°€\n",
    "        if 'ori_product_specification' not in existing_columns:\n",
    "            print(\"ori_product_specification ì»¬ëŸ¼ ì¶”ê°€ ì¤‘...\")\n",
    "            cursor.execute(f\"\"\"\n",
    "                ALTER TABLE {table_name} \n",
    "                ADD COLUMN IF NOT EXISTS ori_product_specification JSONB\n",
    "            \"\"\")\n",
    "            conn.commit()\n",
    "        \n",
    "        # 3. ê¸°ì¡´ product_specification ë°ì´í„°ë¥¼ ori_product_specificationìœ¼ë¡œ ë³µì‚¬\n",
    "        if 'product_specification' in existing_columns:\n",
    "            print(\"ê¸°ì¡´ ë°ì´í„°ë¥¼ ori_product_specificationìœ¼ë¡œ ë³µì‚¬ ì¤‘...\")\n",
    "            cursor.execute(f\"\"\"\n",
    "                UPDATE {table_name}\n",
    "                SET ori_product_specification = product_specification\n",
    "                WHERE ori_product_specification IS NULL \n",
    "                AND product_specification IS NOT NULL\n",
    "            \"\"\")\n",
    "            conn.commit()\n",
    "            print(f\"ë³µì‚¬ëœ í–‰ ìˆ˜: {cursor.rowcount}\")\n",
    "        \n",
    "        # 4. ëª¨ë“  ë°ì´í„° ì¡°íšŒ ë° í‰íƒ„í™” ì²˜ë¦¬\n",
    "        print(\"ë°ì´í„° ì¡°íšŒ ë° í‰íƒ„í™” ì²˜ë¦¬ ì‹œì‘...\")\n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT product_id, ori_product_specification\n",
    "            FROM {table_name}\n",
    "            WHERE ori_product_specification IS NOT NULL\n",
    "            ORDER BY product_id\n",
    "        \"\"\")\n",
    "        \n",
    "        updates = []\n",
    "        processed_count = 0\n",
    "        error_count = 0\n",
    "        \n",
    "        while True:\n",
    "            rows = cursor.fetchmany(batch_size)\n",
    "            if not rows:\n",
    "                break\n",
    "            \n",
    "            for product_id, ori_spec in rows:\n",
    "                try:\n",
    "                    # JSONB ë°ì´í„° í‰íƒ„í™” (ëª¨ë“  ê°’ì„ ë¦¬ìŠ¤íŠ¸ë¡œ, ì¤‘ë³µ í‚¤ ë³‘í•©)\n",
    "                    flattened_spec = flatten_jsonb_structure(ori_spec)\n",
    "                    updates.append((Json(flattened_spec), product_id))\n",
    "                    processed_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Product ID {product_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "                    error_count += 1\n",
    "                    continue\n",
    "            \n",
    "            # ë°°ì¹˜ ì—…ë°ì´íŠ¸\n",
    "            if updates:\n",
    "                print(f\"ì²˜ë¦¬ ì¤‘... ({processed_count} í–‰ ì™„ë£Œ, {error_count} ì˜¤ë¥˜)\")\n",
    "                update_cursor = conn.cursor()\n",
    "                execute_batch(\n",
    "                    update_cursor,\n",
    "                    f\"\"\"\n",
    "                    UPDATE {table_name}\n",
    "                    SET product_specification = %s\n",
    "                    WHERE product_id = %s\n",
    "                    \"\"\",\n",
    "                    updates\n",
    "                )\n",
    "                conn.commit()\n",
    "                updates = []\n",
    "                update_cursor.close()\n",
    "        \n",
    "        print(f\"ì´ {processed_count}ê°œ í–‰ì˜ product_specification ì»¬ëŸ¼ í‰íƒ„í™” ì™„ë£Œ\")\n",
    "        if error_count > 0:\n",
    "            print(f\"ì˜¤ë¥˜ ë°œìƒ í–‰ ìˆ˜: {error_count}\")\n",
    "        \n",
    "        # 5. ìƒ˜í”Œ ë°ì´í„° í™•ì¸\n",
    "        print(\"\\ní‰íƒ„í™”ëœ ë°ì´í„° ìƒ˜í”Œ í™•ì¸:\")\n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT product_id, \n",
    "                   ori_product_specification,\n",
    "                   product_specification\n",
    "            FROM {table_name}\n",
    "            WHERE ori_product_specification IS NOT NULL\n",
    "            LIMIT 3\n",
    "        \"\"\")\n",
    "        \n",
    "        for product_id, ori_spec, new_spec in cursor.fetchall():\n",
    "            print(f\"\\nProduct ID: {product_id}\")\n",
    "            if ori_spec:\n",
    "                print(f\"ì›ë³¸ êµ¬ì¡° í‚¤: {list(ori_spec.keys())[:5] if isinstance(ori_spec, dict) else type(ori_spec)}\")\n",
    "            if new_spec:\n",
    "                print(f\"í‰íƒ„í™”ëœ êµ¬ì¡° í‚¤ ê°œìˆ˜: {len(new_spec)}\")\n",
    "                # ì²˜ìŒ 5ê°œ í‚¤-ê°’ë§Œ ì¶œë ¥\n",
    "                sample_items = list(new_spec.items())[:5]\n",
    "                for key, value in sample_items:\n",
    "                    print(f\"  {key}: {value} (ë¦¬ìŠ¤íŠ¸, {len(value)}ê°œ í•­ëª©)\")\n",
    "        \n",
    "        # 6. ì¤‘ë³µ í‚¤ ë³‘í•© ì˜ˆì‹œ í™•ì¸\n",
    "        print(\"\\nì¤‘ë³µ í‚¤ê°€ ë³‘í•©ëœ ì˜ˆì‹œ í™•ì¸:\")\n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT product_id, product_specification\n",
    "            FROM {table_name}\n",
    "            WHERE product_specification IS NOT NULL\n",
    "            AND jsonb_typeof(product_specification) = 'object'\n",
    "            LIMIT 5\n",
    "        \"\"\")\n",
    "        \n",
    "        for product_id, spec in cursor.fetchall():\n",
    "            if spec and isinstance(spec, dict):\n",
    "                # 2ê°œ ì´ìƒì˜ ê°’ì„ ê°€ì§„ í‚¤ ì°¾ê¸°\n",
    "                multi_value_keys = {k: v for k, v in spec.items() if isinstance(v, list) and len(v) > 1}\n",
    "                if multi_value_keys:\n",
    "                    print(f\"\\nProduct ID {product_id}ì—ì„œ ì¤‘ë³µ ë³‘í•©ëœ í‚¤:\")\n",
    "                    for key, values in list(multi_value_keys.items())[:3]:\n",
    "                        print(f\"  {key}: {values}\")\n",
    "                    break\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ìƒ˜í”Œ ë°ì´í„°ë¡œ í•¨ìˆ˜ ë™ì‘ í™•ì¸\n",
    "def test_flatten():\n",
    "    \"\"\"í‰íƒ„í™” í•¨ìˆ˜ í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"ì¤‘ë³µ í‚¤ í…ŒìŠ¤íŠ¸\",\n",
    "            \"input\": {\n",
    "                \"ìƒ‰ìƒ\": \"ë¸”ë™\",\n",
    "                \"ì¶”ê°€\": {\"ìƒ‰ìƒ\": \"ë¸”ë™\"}\n",
    "            },\n",
    "            \"expected\": {\n",
    "                \"ìƒ‰ìƒ\": [\"ë¸”ë™\", \"ë¸”ë™\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ë³µí•© ì¤‘ì²© êµ¬ì¡°\",\n",
    "            \"input\": {\n",
    "                \"ì‚¬ìš´ë“œ\": {\n",
    "                    \"ê°ë„\": \"100dB\",\n",
    "                    \"ìƒ‰ìƒ\": \"ì‹¤ë²„\"\n",
    "                },\n",
    "                \"ìƒ‰ìƒ\": \"ë¸”ë™\",\n",
    "                \"ì¶”ê°€\": {\n",
    "                    \"ìƒ‰ìƒ\": \"í™”ì´íŠ¸\"\n",
    "                }\n",
    "            },\n",
    "            \"expected\": {\n",
    "                \"ê°ë„\": [\"100dB\"],\n",
    "                \"ìƒ‰ìƒ\": [\"ë¸”ë™\", \"ì‹¤ë²„\", \"í™”ì´íŠ¸\"]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for test in test_cases:\n",
    "        print(f\"\\n{test['name']}:\")\n",
    "        print(f\"ì…ë ¥: {test['input']}\")\n",
    "        result = flatten_jsonb_structure(test['input'])\n",
    "        print(f\"ê²°ê³¼: {result}\")\n",
    "        print(f\"ì˜ˆìƒ: {test['expected']}\")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "print(\"=== í‰íƒ„í™” í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ===\")\n",
    "test_flatten()\n",
    "\n",
    "print(\"\\n=== ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ ì²˜ë¦¬ ===\")\n",
    "# í•¨ìˆ˜ ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    # í…Œì´ë¸” ì´ë¦„ì„ í™˜ê²½ë³€ìˆ˜ë‚˜ ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤\n",
    "    update_product_specification_column(table_name='kt_merged_product_20251015_mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef062fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spec depth ìˆëŠ” ë¶€ë¶„ì„ depth 1ìœ¼ë¡œ ë³€ê²½ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873e6726",
   "metadata": {},
   "source": [
    "### Mongodb ì—°ê²°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab4681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Cosmos DB for MongoDB ì„¤ì • (Microsoft ê³µì‹ ë°©ì‹)\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import pymongo\n",
    "\n",
    "load_dotenv('.env')\n",
    "\n",
    "# Microsoft ê³µì‹ ë°©ì‹: COSMOS_CONNECTION_STRING ë˜ëŠ” MONGODB_CONNECTION_STRING ì‚¬ìš©\n",
    "CONNECTION_STRING = os.getenv('COSMOS_CONNECTION_STRING') or os.getenv('MONGODB_CONNECTION_STRING')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Azure Cosmos DB for MongoDB ì—°ê²° ì„¤ì • (MS ê³µì‹ ë°©ì‹)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ì—°ê²° ë¬¸ìì—´ í™•ì¸\n",
    "if CONNECTION_STRING:\n",
    "    print(\"âœ… ì—°ê²° ë¬¸ìì—´ ë¡œë“œ ì™„ë£Œ\")\n",
    "    # ë³´ì•ˆì„ ìœ„í•´ ì¼ë¶€ë§Œ ì¶œë ¥\n",
    "    if \"mongodb://\" in CONNECTION_STRING or \"mongodb+srv://\" in CONNECTION_STRING:\n",
    "        parts = CONNECTION_STRING.split('@')\n",
    "        if len(parts) > 1:\n",
    "            host_info = parts[1].split('?')[0] if '?' in parts[1] else parts[1].split('/')[0]\n",
    "            print(f\"   Host: {host_info}\")\n",
    "else:\n",
    "    print(\"âŒ ì—°ê²° ë¬¸ìì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"   .env íŒŒì¼ì— COSMOS_CONNECTION_STRING ë˜ëŠ” MONGODB_CONNECTION_STRINGì„ ì„¤ì •í•´ì£¼ì„¸ìš”.\")\n",
    "    print(\"\\nğŸ’¡ .env íŒŒì¼ ì˜ˆì‹œ:\")\n",
    "    print(\"   COSMOS_CONNECTION_STRING=mongodb://username:password@host:port/database?ssl=true&replicaSet=globaldb&retryWrites=false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc65710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB ì—°ê²° (Microsoft ê³µì‹ ë°©ì‹)\n",
    "def mongodb_connection_ms_official():\n",
    "    \"\"\"Microsoft ê³µì‹ ë°©ì‹ìœ¼ë¡œ Azure Cosmos DB for MongoDB ì—°ê²°\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"MongoDB ì—°ê²° í…ŒìŠ¤íŠ¸ (MS ê³µì‹ ë°©ì‹)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not CONNECTION_STRING:\n",
    "        print(\"âŒ ì—°ê²° ë¬¸ìì—´ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Microsoft ê³µì‹ ë°©ì‹: ë‹¨ìˆœí•œ MongoClient ìƒì„±\n",
    "        client = pymongo.MongoClient(CONNECTION_STRING)\n",
    "        print(CONNECTION_STRING)\n",
    "        \n",
    "        # í´ë¼ì´ì–¸íŠ¸ ì˜µì…˜ í™•ì¸ (ë””ë²„ê¹…ìš©)\n",
    "        print(\"ğŸ” í´ë¼ì´ì–¸íŠ¸ ì˜µì…˜:\")\n",
    "        for prop, value in vars(client.options).items():\n",
    "            if value is not None:  # Noneì´ ì•„ë‹Œ ê°’ë§Œ í‘œì‹œ\n",
    "                print(f\"   {prop}: {value}\")\n",
    "        \n",
    "        print(\"\\nğŸ”— ì—°ê²° ê²€ì¦ ì¤‘...\")\n",
    "        \n",
    "        # Microsoft ê³µì‹ ë°©ì‹: server_info()ë¡œ ì—°ê²° ê²€ì¦\n",
    "        try:\n",
    "            server_info = client.server_info()\n",
    "            print(\"âœ… ì—°ê²° ì„±ê³µ!\")\n",
    "            print(f\"   ì„œë²„ ë²„ì „: {server_info.get('version', 'N/A')}\")\n",
    "            \n",
    "            # ì¶”ê°€ ì—°ê²° ì •ë³´\n",
    "            if 'buildInfo' in server_info:\n",
    "                build_info = server_info.get('buildInfo', {})\n",
    "                print(f\"   ë¹Œë“œ ì •ë³´: {build_info.get('gitVersion', 'N/A')}\")\n",
    "            \n",
    "        except (pymongo.errors.OperationFailure, \n",
    "                pymongo.errors.ConnectionFailure, \n",
    "                pymongo.errors.ExecutionTimeout) as err:\n",
    "            print(f\"âŒ ì—°ê²° ì‹¤íŒ¨: {err}\")\n",
    "            return None\n",
    "        \n",
    "        # ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ (ì„ íƒì )\n",
    "        try:\n",
    "            db_list = client.list_database_names()\n",
    "            print(f\"\\nğŸ“Š ì ‘ê·¼ ê°€ëŠ¥í•œ ë°ì´í„°ë² ì´ìŠ¤ ({len(db_list)}ê°œ):\")\n",
    "            for db in db_list[:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ\n",
    "                print(f\"   â€¢ {db}\")\n",
    "        except Exception as db_error:\n",
    "            print(f\"   â„¹ï¸  ë°ì´í„°ë² ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ ì œí•œ: {db_error}\")\n",
    "        \n",
    "        print(\"\\nâœ… Azure Cosmos DB for MongoDB ì—°ê²° ì™„ë£Œ!\")\n",
    "        return client\n",
    "        \n",
    "    except Exception as err:\n",
    "        print(f\"âŒ ì „ì²´ì ì¸ ì—°ê²° ì˜¤ë¥˜: {err}\")\n",
    "        \n",
    "        # ìƒì„¸í•œ ì˜¤ë¥˜ ì •ë³´ ì œê³µ\n",
    "        error_type = type(err).__name__\n",
    "        print(f\"\\nğŸ” ì˜¤ë¥˜ ìœ í˜•: {error_type}\")\n",
    "        \n",
    "        if \"timeout\" in str(err).lower():\n",
    "            print(\"ğŸ’¡ íƒ€ì„ì•„ì›ƒ ë¬¸ì œ - ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë˜ëŠ” ë°©í™”ë²½ í™•ì¸\")\n",
    "        elif \"authentication\" in str(err).lower():\n",
    "            print(\"ğŸ’¡ ì¸ì¦ ë¬¸ì œ - ì‚¬ìš©ìëª…/ë¹„ë°€ë²ˆí˜¸ í™•ì¸\")\n",
    "        elif \"ssl\" in str(err).lower():\n",
    "            print(\"ğŸ’¡ SSL ë¬¸ì œ - ì—°ê²° ë¬¸ìì—´ì˜ SSL ì„¤ì • í™•ì¸\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Microsoft ê³µì‹ ë°©ì‹ìœ¼ë¡œ ì—°ê²° ì‹œë„\n",
    "mongo_client = mongodb_connection_ms_official()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eylamnwsu1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB ì»¬ë ‰ì…˜(í…Œì´ë¸”) ìƒì„± ë° ì¸ë±ìŠ¤ ì„¤ì • - PostgreSQL êµ¬ì¡° ê¸°ë°˜\n",
    "def create_mongodb_collections():\n",
    "    \"\"\"MongoDB ì»¬ë ‰ì…˜ ìƒì„± ë° ì¸ë±ìŠ¤ ì„¤ì • (PostgreSQL kt_merged_product_20251001 êµ¬ì¡° ê¸°ë°˜)\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"MongoDB ì»¬ë ‰ì…˜ ìƒì„± (PostgreSQL êµ¬ì¡° ê¸°ë°˜)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not mongo_client:\n",
    "        print(\"âŒ MongoDB í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì—°ê²°ì„ ë¨¼ì € ìˆ˜í–‰í•˜ì„¸ìš”.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # rubicon ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ ë˜ëŠ” ìƒì„±\n",
    "        db_name = \"rubicon\"\n",
    "        db = mongo_client[db_name]\n",
    "        print(f\"ğŸ“š ë°ì´í„°ë² ì´ìŠ¤ '{db_name}' ì„ íƒ/ìƒì„±\")\n",
    "        \n",
    "        # kt_merged_product_20251001 ì»¬ë ‰ì…˜ ìƒì„± (PostgreSQL í…Œì´ë¸”ê³¼ ë™ì¼í•œ ì´ë¦„)\n",
    "        collection_name = \"kt_merged_product_20251001\"\n",
    "        kt_merged_product_20251001 = db[collection_name]\n",
    "        \n",
    "        # ê¸°ì¡´ ì»¬ë ‰ì…˜ í™•ì¸\n",
    "        existing_collections = db.list_collection_names()\n",
    "        if collection_name in existing_collections:\n",
    "            print(f\"   â„¹ï¸  '{collection_name}' ì»¬ë ‰ì…˜ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.\")\n",
    "            doc_count = kt_merged_product_20251001.count_documents({})\n",
    "            print(f\"      í˜„ì¬ ë¬¸ì„œ ìˆ˜: {doc_count}ê°œ\")\n",
    "            \n",
    "            # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì—¬ë¶€ í™•ì¸ (ì„ íƒì )\n",
    "            # kt_merged_product_20251001.drop()\n",
    "            # print(f\"   âœ… ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ì¬ìƒì„±\")\n",
    "        else:\n",
    "            print(f\"   âœ… '{collection_name}' ì»¬ë ‰ì…˜ ìƒì„±\")\n",
    "        \n",
    "        # PostgreSQLê³¼ ë™ì¼í•œ ì¸ë±ìŠ¤ ìƒì„±\n",
    "        print(\"\\nğŸ“ ì¸ë±ìŠ¤ ìƒì„± (PostgreSQL êµ¬ì¡° ê¸°ë°˜):\")\n",
    "        \n",
    "        # ë‹¨ì¼ í•„ë“œ ì¸ë±ìŠ¤\n",
    "        kt_merged_product_20251001.create_index(\"model_code\", name=\"idx_model_code\")\n",
    "        print(\"   âœ… model_code ì¸ë±ìŠ¤ ìƒì„±\")\n",
    "        \n",
    "        kt_merged_product_20251001.create_index(\"site_code\", name=\"idx_site_code\")\n",
    "        print(\"   âœ… site_code ì¸ë±ìŠ¤ ìƒì„±\")\n",
    "        \n",
    "        kt_merged_product_20251001.create_index(\"product_id\", name=\"idx_product_id\")\n",
    "        print(\"   âœ… product_id ì¸ë±ìŠ¤ ìƒì„±\")\n",
    "        \n",
    "        kt_merged_product_20251001.create_index(\"product_name\", name=\"idx_product_name\")\n",
    "        print(\"   âœ… product_name ì¸ë±ìŠ¤ ìƒì„±\")\n",
    "        \n",
    "        # ë³µí•© ì¸ë±ìŠ¤\n",
    "        kt_merged_product_20251001.create_index(\n",
    "            [(\"model_code\", 1), (\"site_ccode\", 1)],\n",
    "            name=\"idx_model_code_site_code\"\n",
    "        )\n",
    "        print(\"   âœ… model_code + site_ccode ë³µí•© ì¸ë±ìŠ¤ ìƒì„±\")\n",
    "        \n",
    "        # ì¶”ê°€ ì„±ëŠ¥ ìµœì í™” ì¸ë±ìŠ¤\n",
    "        kt_merged_product_20251001.create_index(\"release_date\", name=\"idx_release_date\")\n",
    "        print(\"   âœ… release_date ì¸ë±ìŠ¤ ìƒì„±\")\n",
    "        \n",
    "        kt_merged_product_20251001.create_index(\"final_price\", name=\"idx_final_price\")\n",
    "        print(\"   âœ… final_price ì¸ë±ìŠ¤ ìƒì„±\")\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¸ë±ìŠ¤ (MongoDB íŠ¹í™”)\n",
    "        try:\n",
    "            kt_merged_product_20251001.create_index(\n",
    "                [(\"product_name\", \"text\"), (\"model_name\", \"text\"), (\"product_specification\", \"text\")],\n",
    "                name=\"text_search_index\",\n",
    "                default_language=\"korean\"  # í•œêµ­ì–´ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì§€ì›\n",
    "            )\n",
    "            print(\"   âœ… í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„± (product_name, model_name, product_specification)\")\n",
    "        except Exception as text_idx_error:\n",
    "            print(f\"   âš ï¸  í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨ (ì´ë¯¸ ì¡´ì¬í•  ìˆ˜ ìˆìŒ): {text_idx_error}\")\n",
    "        \n",
    "        # ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê·œì¹™ ì •ì˜ (MongoDB 3.6+)\n",
    "        # PostgreSQL kt_merged_product_20251001 í…Œì´ë¸”ê³¼ ë™ì¼í•œ êµ¬ì¡°\n",
    "        validation_rules = {\n",
    "            \"$jsonSchema\": {\n",
    "                \"bsonType\": \"object\",\n",
    "                \"title\": \"ìƒí’ˆ í†µí•© ì •ë³´ (KT)\",\n",
    "                \"description\": \"PostgreSQL kt_merged_product_20251001 í…Œì´ë¸”ê³¼ ë™ì¼í•œ êµ¬ì¡°\",\n",
    "                \"properties\": {\n",
    "                    # ì œí’ˆ ê¸°ë³¸ ì •ë³´\n",
    "                    \"product_id\": {\"bsonType\": \"string\", \"maxLength\": 15, \"description\": \"ìƒí’ˆ ì•„ì´ë””\"},\n",
    "                    \"model_code\": {\"bsonType\": \"string\", \"maxLength\": 100, \"description\": \"ëª¨ë¸ ì½”ë“œ\"},\n",
    "                    \"is_bespoke_goods\": {\"bsonType\": \"string\", \"maxLength\": 1, \"description\": \"ë¹„ìŠ¤í¬í¬ ìƒí’ˆ ì—¬ë¶€\"},\n",
    "                    \"model_name\": {\"bsonType\": \"string\", \"description\": \"ëª¨ë¸ ëª…(ëª¨ë¸ ì½”ë“œ ìƒìœ„ ì§‘í•©)\"},\n",
    "                    \"product_name\": {\"bsonType\": \"string\", \"description\": \"ìƒí’ˆ ëª…\"},\n",
    "                    \n",
    "                    # ì „ì‹œ ì¹´í…Œê³ ë¦¬\n",
    "                    \"display_category_major\": {\"bsonType\": \"string\", \"description\": \"ì „ì‹œ ëŒ€ë¶„ë¥˜\"},\n",
    "                    \"display_category_middle\": {\"bsonType\": \"string\", \"description\": \"ì „ì‹œ ì¤‘ë¶„ë¥˜\"},\n",
    "                    \"display_category_minor\": {\"bsonType\": \"string\", \"description\": \"ì „ì‹œ ì†Œë¶„ë¥˜\"},\n",
    "                    \n",
    "                    # ì œí’ˆ ì¹´í…Œê³ ë¦¬\n",
    "                    \"product_category_major\": {\"bsonType\": \"string\", \"description\": \"ì¹´í…Œê³ ë¦¬ ëŒ€ë¶„ë¥˜\"},\n",
    "                    \"product_category_middle\": {\"bsonType\": \"string\", \"description\": \"ì¹´í…Œê³ ë¦¬ ì¤‘ë¶„ë¥˜\"},\n",
    "                    \"product_category_minor\": {\"bsonType\": \"string\", \"description\": \"ì¹´í…Œê³ ë¦¬ ì†Œë¶„ë¥˜\"},\n",
    "                    \n",
    "                    # ì œí’ˆ ì†ì„±\n",
    "                    \"product_color\": {\"bsonType\": \"string\", \"description\": \"ìƒ‰ìƒ\"},\n",
    "                    \"release_date\": {\"bsonType\": [\"date\", \"string\"], \"description\": \"ì¶œì‹œì¼\"},\n",
    "                    \n",
    "                    # í”Œë˜ê·¸ í•„ë“œ (VARCHAR(1))\n",
    "                    \"is_ai_subscription_eligible\": {\"bsonType\": \"string\", \"maxLength\": 1, \"description\": \"AI êµ¬ë… ëŒ€ìƒ ì—¬ë¶€\"},\n",
    "                    \"is_smart_subscription_eligible\": {\"bsonType\": \"string\", \"maxLength\": 1, \"description\": \"ìŠ¤ë§ˆíŠ¸ êµ¬ë… ëŒ€ìƒ ì—¬ë¶€\"},\n",
    "                    \"is_galaxy_club_eligible\": {\"bsonType\": \"string\", \"maxLength\": 1, \"description\": \"ê°¤ëŸ­ì‹œ í´ëŸ½ ëŒ€ìƒ ì—¬ë¶€\"},\n",
    "                    \"is_installment_payment_available\": {\"bsonType\": \"string\", \"maxLength\": 1, \"description\": \"í• ë¶€ ê²°ì œ ê°€ëŠ¥ ì—¬ë¶€\"},\n",
    "                    \"is_bundle_product\": {\"bsonType\": \"string\", \"maxLength\": 1, \"description\": \"ë²ˆë“¤ ìƒí’ˆ ì—¬ë¶€\"},\n",
    "                    \n",
    "                    # URL ë° í…ìŠ¤íŠ¸\n",
    "                    \"product_detail_url\": {\"bsonType\": \"string\", \"description\": \"ì œí’ˆ ìƒì„¸ URL\"},\n",
    "                    \"site_code\": {\"bsonType\": \"string\", \"maxLength\": 10, \"description\": \"ì‚¬ì´íŠ¸ ì½”ë“œ\"},\n",
    "                    \"unique_selling_point\": {\"bsonType\": \"string\", \"description\": \"ëª¨ë¸ ì¹´ë“œ ì£¼ìš” íŒë§¤ í¬ì¸íŠ¸\"},\n",
    "                    \n",
    "                    # ë¦¬ë·° ì •ë³´\n",
    "                    \"review_count\": {\"bsonType\": [\"int\", \"long\"], \"description\": \"ë¦¬ë·° ê°œìˆ˜\"},\n",
    "                    \"review_rating_score\": {\"bsonType\": [\"double\", \"decimal\"], \"description\": \"ë¦¬ë·° í‰ì \"},\n",
    "                    \"review_text_collection\": {\"bsonType\": \"object\", \"description\": \"ë¦¬ë·° í…ìŠ¤íŠ¸ ëª¨ìŒ (JSONB)\"},\n",
    "                    \n",
    "                    # ê°€ê²© ì •ë³´\n",
    "                    \"standard_price\": {\"bsonType\": [\"double\", \"decimal\", \"long\"], \"description\": \"ê¸°ì¤€ê°€\"},\n",
    "                    \"member_price\": {\"bsonType\": [\"double\", \"decimal\", \"long\"], \"description\": \"íšŒì›ê°€\"},\n",
    "                    \"benefit_price\": {\"bsonType\": [\"double\", \"decimal\", \"long\"], \"description\": \"í˜œíƒê°€\"},\n",
    "                    \"web_coupon_discount\": {\"bsonType\": [\"double\", \"decimal\", \"long\"], \"description\": \"ì›¹ ì¿ í° í• ì¸ ê¸ˆì•¡\"},\n",
    "                    \"final_price\": {\"bsonType\": [\"double\", \"decimal\", \"long\"], \"description\": \"ìµœì¢… ê°€ê²©\"},\n",
    "                    \n",
    "                    # ì¬ê³  ë° íŒë§¤ ì •ë³´\n",
    "                    \"stock_quantity\": {\"bsonType\": [\"int\", \"long\"], \"description\": \"ì¬ê³  ìˆ˜ëŸ‰\"},\n",
    "                    \"total_sale_amount\": {\"bsonType\": [\"double\", \"decimal\", \"long\"], \"description\": \"ì´ íŒë§¤ ê¸ˆì•¡\"},\n",
    "                    \"total_sale_quantity\": {\"bsonType\": [\"int\", \"long\"], \"description\": \"ì´ íŒë§¤ ìˆ˜ëŸ‰\"},\n",
    "                    \n",
    "                    # ë²ˆë“¤ ì •ë³´\n",
    "                    \"bundle_component_model_code\": {\n",
    "                        \"bsonType\": \"array\",\n",
    "                        \"items\": {\"bsonType\": \"string\"},\n",
    "                        \"description\": \"ë²ˆë“¤ êµ¬ì„± ìƒí’ˆ ëª¨ë¸ ì½”ë“œ ëª©ë¡\"\n",
    "                    },\n",
    "                    \n",
    "                    # ì¹´í…Œê³ ë¦¬ ë­í‚¹\n",
    "                    \"category_rank_recommend\": {\"bsonType\": [\"int\", \"long\"], \"description\": \"ì „ì‹œ ì†Œë¶„ë¥˜ ë‚´ ì¶”ì²œìˆœ ìˆœìœ„\"},\n",
    "                    \"category_rank_quantity\": {\"bsonType\": [\"int\", \"long\"], \"description\": \"ì „ì‹œ ì†Œë¶„ë¥˜ ë‚´ íŒë§¤ëŸ‰ìˆœ ìˆœìœ„\"},\n",
    "                    \"category_rank_rating\": {\"bsonType\": [\"int\", \"long\"], \"description\": \"ì „ì‹œ ì†Œë¶„ë¥˜ ë‚´ ë³„ì ìˆœ ìˆœìœ„\"},\n",
    "                    \n",
    "                    # JSON í•„ë“œ (PostgreSQL JSONB -> MongoDB Object)\n",
    "                    \"product_specification\": {\"bsonType\": \"object\", \"description\": \"ì œí’ˆ ì‚¬ì–‘ ì •ë³´ (JSONB)\"},\n",
    "                    \"event_info\": {\"bsonType\": \"object\", \"description\": \"ì´ë²¤íŠ¸ ì •ë³´ (JSONB)\"},\n",
    "                    \"coupon_info\": {\"bsonType\": \"object\", \"description\": \"ì¿ í° ì •ë³´ (JSONB)\"},\n",
    "                    \"promotion_info\": {\"bsonType\": \"object\", \"description\": \"í”„ë¡œëª¨ì…˜ ì •ë³´ (JSONB)\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì„¤ì • (ì„ íƒì  - Cosmos DBì—ì„œ ì§€ì›í•˜ëŠ” ê²½ìš°)\n",
    "        try:\n",
    "            db.command({\n",
    "                \"collMod\": collection_name,\n",
    "                \"validator\": validation_rules,\n",
    "                \"validationLevel\": \"moderate\"  # ê¸°ì¡´ ë¬¸ì„œëŠ” ê²€ì¦ ì•ˆí•¨, ì‹ ê·œ/ìˆ˜ì •ë§Œ ê²€ì¦\n",
    "            })\n",
    "            print(\"\\nâœ… ìŠ¤í‚¤ë§ˆ ê²€ì¦ ê·œì¹™ ì„¤ì • ì™„ë£Œ\")\n",
    "        except Exception as validation_error:\n",
    "            print(f\"\\nâš ï¸  ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì„¤ì • ì‹¤íŒ¨ (Cosmos DB ë²„ì „ í™•ì¸): {validation_error}\")\n",
    "        \n",
    "        # ì¸ë±ìŠ¤ ì •ë³´ ì¶œë ¥\n",
    "        print(\"\\nğŸ“Š ìƒì„±ëœ ì¸ë±ìŠ¤ ëª©ë¡:\")\n",
    "        indexes = kt_merged_product_20251001.list_indexes()\n",
    "        for idx in indexes:\n",
    "            print(f\"   â€¢ {idx['name']}: {idx['key']}\")\n",
    "        \n",
    "        print(\"\\nâœ… MongoDB ì»¬ë ‰ì…˜ ë° ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!\")\n",
    "        print(f\"   ë°ì´í„°ë² ì´ìŠ¤: {db_name}\")\n",
    "        print(f\"   ì»¬ë ‰ì…˜: {collection_name}\")\n",
    "        \n",
    "        # ìƒì„±ëœ ì»¬ë ‰ì…˜ ì •ë³´ ë°˜í™˜\n",
    "        return {\n",
    "            \"database\": db,\n",
    "            \"kt_merged_product_20251001\": kt_merged_product_20251001\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì»¬ë ‰ì…˜ ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# ì»¬ë ‰ì…˜ ìƒì„± ì‹¤í–‰\n",
    "collections = create_mongodb_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2calixmzmiv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL ë°ì´í„° íŒŒì¼ì„ ì½ì–´ì„œ MongoDBìš© ë°ì´í„°ë¡œ ë³€í™˜\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('.env')\n",
    "PG_UPLOAD_FILE_PATH = os.getenv('PG_UPLOAD_FILE_PATH')\n",
    "\n",
    "def analyze_file_structure(file_path):\n",
    "    \"\"\"íŒŒì¼ êµ¬ì¡° ë¶„ì„\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"íŒŒì¼ êµ¬ì¡° ë¶„ì„\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # íŒŒì¼ ì²« ëª‡ ì¤„ ì½ê¸°\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = [f.readline().strip() for _ in range(5)]\n",
    "        \n",
    "        print(f\"íŒŒì¼ ì²« 5ì¤„:\")\n",
    "        for i, line in enumerate(lines):\n",
    "            print(f\"  [{i}] {line[:100]}{'...' if len(line) > 100 else ''}\")\n",
    "        \n",
    "        # êµ¬ë¶„ì ìë™ ê°ì§€\n",
    "        first_line = lines[0] if lines else \"\"\n",
    "        tab_count = first_line.count('\\t')\n",
    "        comma_count = first_line.count(',')\n",
    "        semicolon_count = first_line.count(';')\n",
    "        \n",
    "        print(f\"\\nêµ¬ë¶„ì ë¶„ì„:\")\n",
    "        print(f\"  íƒ­(\\\\t): {tab_count}ê°œ\")\n",
    "        print(f\"  ì‰¼í‘œ(,): {comma_count}ê°œ\")\n",
    "        print(f\"  ì„¸ë¯¸ì½œë¡ (;): {semicolon_count}ê°œ\")\n",
    "        \n",
    "        # ìµœì  êµ¬ë¶„ì ê²°ì •\n",
    "        if tab_count > comma_count and tab_count > semicolon_count:\n",
    "            delimiter = '\\t'\n",
    "            delimiter_name = 'TAB'\n",
    "        elif comma_count > semicolon_count:\n",
    "            delimiter = ','\n",
    "            delimiter_name = 'COMMA'\n",
    "        else:\n",
    "            delimiter = ';'\n",
    "            delimiter_name = 'SEMICOLON'\n",
    "        \n",
    "        print(f\"  ê¶Œì¥ êµ¬ë¶„ì: {delimiter_name}\")\n",
    "        \n",
    "        return delimiter, lines\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "        return None, []\n",
    "\n",
    "def parse_json_field_safe(value):\n",
    "    \"\"\"JSON í•„ë“œ ì•ˆì „ íŒŒì‹±\"\"\"\n",
    "    if pd.isna(value) or value == '' or value is None:\n",
    "        return {}\n",
    "        \n",
    "    # ì´ë¯¸ dictì¸ ê²½ìš°\n",
    "    if isinstance(value, dict):\n",
    "        return value\n",
    "        \n",
    "    # ë¬¸ìì—´ì¸ ê²½ìš°\n",
    "    if isinstance(value, str):\n",
    "        value = value.strip()\n",
    "        if not value or value in ['{}', 'null', 'None']:\n",
    "            return {}\n",
    "            \n",
    "        try:\n",
    "            # JSON ë¬¸ìì—´ íŒŒì‹±\n",
    "            if value.startswith('{') and value.endswith('}'):\n",
    "                parsed = json.loads(value)\n",
    "                return parsed if isinstance(parsed, dict) else {}\n",
    "        except json.JSONDecodeError:\n",
    "            # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜\n",
    "            return {}\n",
    "    \n",
    "    return {}\n",
    "\n",
    "def load_and_transform_pg_data():\n",
    "    \"\"\"PostgreSQL CSV/TSV íŒŒì¼ì„ MongoDBìš© ë°ì´í„°ë¡œ ë³€í™˜\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"PostgreSQL ë°ì´í„° íŒŒì¼ ë¡œë“œ ë° ë³€í™˜\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not PG_UPLOAD_FILE_PATH:\n",
    "        print(\"âŒ PG_UPLOAD_FILE_PATHê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        print(\"   .env íŒŒì¼ì— PG_UPLOAD_FILE_PATHë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(f\"ğŸ“ íŒŒì¼ ê²½ë¡œ: {PG_UPLOAD_FILE_PATH}\")\n",
    "        \n",
    "        # íŒŒì¼ ì¡´ì¬ í™•ì¸\n",
    "        if not os.path.exists(PG_UPLOAD_FILE_PATH):\n",
    "            print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {PG_UPLOAD_FILE_PATH}\")\n",
    "            return None\n",
    "        \n",
    "        # íŒŒì¼ í¬ê¸° í™•ì¸\n",
    "        file_size = os.path.getsize(PG_UPLOAD_FILE_PATH)\n",
    "        print(f\"ğŸ“ íŒŒì¼ í¬ê¸°: {file_size:,} bytes ({file_size/1024:.1f} KB)\")\n",
    "        \n",
    "        # íŒŒì¼ êµ¬ì¡° ë¶„ì„\n",
    "        delimiter, sample_lines = analyze_file_structure(PG_UPLOAD_FILE_PATH)\n",
    "        \n",
    "        if delimiter is None:\n",
    "            print(\"âŒ íŒŒì¼ êµ¬ì¡°ë¥¼ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return None\n",
    "        \n",
    "        # ì—¬ëŸ¬ êµ¬ë¶„ìë¡œ ì‹œë„\n",
    "        delimiters_to_try = [delimiter, '\\t', ',', ';']\n",
    "        df = None\n",
    "        \n",
    "        for delim in delimiters_to_try:\n",
    "            try:\n",
    "                print(f\"\\nğŸ”„ êµ¬ë¶„ì '{delim}' ì‹œë„...\")\n",
    "                \n",
    "                # íŒŒì¼ ì½ê¸°\n",
    "                test_df = pd.read_csv(\n",
    "                    PG_UPLOAD_FILE_PATH,\n",
    "                    encoding='utf-8',\n",
    "                    delimiter=delim,\n",
    "                    quotechar='\"',\n",
    "                    quoting=1,  # QUOTE_MINIMAL\n",
    "                    na_values=['', 'NULL', 'null', 'None', 'NaN'],\n",
    "                    keep_default_na=True,\n",
    "                    nrows=5  # ì²˜ìŒ 5í–‰ë§Œ í…ŒìŠ¤íŠ¸\n",
    "                )\n",
    "                \n",
    "                # ì„±ê³µì ìœ¼ë¡œ íŒŒì‹±ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
    "                if len(test_df.columns) > 1 and len(test_df) > 0:\n",
    "                    print(f\"   âœ… ì„±ê³µ: {len(test_df.columns)}ê°œ ì»¬ëŸ¼, {len(test_df)}ê°œ í–‰\")\n",
    "                    print(f\"   ì»¬ëŸ¼ëª…: {list(test_df.columns)[:10]}\")\n",
    "                    \n",
    "                    # ì „ì²´ íŒŒì¼ ì½ê¸°\n",
    "                    df = pd.read_csv(\n",
    "                        PG_UPLOAD_FILE_PATH,\n",
    "                        encoding='utf-8',\n",
    "                        delimiter=delim,\n",
    "                        quotechar='\"',\n",
    "                        quoting=1,\n",
    "                        na_values=['', 'NULL', 'null', 'None', 'NaN'],\n",
    "                        keep_default_na=True\n",
    "                    )\n",
    "                    print(f\"   ì „ì²´ íŒŒì¼ ë¡œë“œ: {len(df)}ê°œ ë ˆì½”ë“œ\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"   âŒ ì‹¤íŒ¨: {len(test_df.columns)}ê°œ ì»¬ëŸ¼ë§Œ ì¸ì‹\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ êµ¬ë¶„ì '{delim}' ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "        if df is None or df.empty:\n",
    "            print(\"âŒ ëª¨ë“  êµ¬ë¶„ìë¡œ íŒŒì‹± ì‹¤íŒ¨\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\nâœ… íŒŒì¼ íŒŒì‹± ì„±ê³µ!\")\n",
    "        print(f\"   ì´ ë ˆì½”ë“œ: {len(df)}ê°œ\")\n",
    "        print(f\"   ì´ ì»¬ëŸ¼: {len(df.columns)}ê°œ\")\n",
    "        print(f\"   ì»¬ëŸ¼ ëª©ë¡: {list(df.columns)}\")\n",
    "        \n",
    "        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸\n",
    "        required_cols = ['mdl_code', 'goods_id', 'goods_nm']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"âš ï¸  í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_cols}\")\n",
    "            print(f\"   ì‹¤ì œ ì»¬ëŸ¼: {list(df.columns)[:10]}\")\n",
    "        \n",
    "        # ë°ì´í„° ìƒ˜í”Œ í™•ì¸\n",
    "        print(f\"\\nğŸ“‹ ì›ë³¸ ë°ì´í„° ìƒ˜í”Œ (ì²« 3í–‰):\")\n",
    "        for idx in range(min(3, len(df))):\n",
    "            print(f\"  í–‰ {idx}:\")\n",
    "            for col in list(df.columns)[:5]:  # ì²˜ìŒ 5ê°œ ì»¬ëŸ¼ë§Œ\n",
    "                value = df.iloc[idx][col]\n",
    "                print(f\"    {col}: {value}\")\n",
    "        \n",
    "        # MongoDB ë¬¸ì„œë¡œ ë³€í™˜\n",
    "        products_data = []\n",
    "        conversion_errors = 0\n",
    "        \n",
    "        print(f\"\\nğŸ“¦ ë°ì´í„° ë³€í™˜ ì‹œì‘...\")\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            try:\n",
    "                # MongoDB ë¬¸ì„œ ìƒì„±\n",
    "                product = {}\n",
    "                \n",
    "                # ëª¨ë“  ì»¬ëŸ¼ì„ ì²˜ë¦¬\n",
    "                for col_name in df.columns:\n",
    "                    value = row[col_name]\n",
    "                    \n",
    "                    # NaNì´ë‚˜ None ê°’ ì²˜ë¦¬\n",
    "                    if pd.isna(value):\n",
    "                        continue\n",
    "                    \n",
    "                    # ë¹ˆ ë¬¸ìì—´ ì œì™¸\n",
    "                    if isinstance(value, str) and value.strip() == '':\n",
    "                        continue\n",
    "                    \n",
    "                    # ì»¬ëŸ¼ë³„ íŠ¹ìˆ˜ ì²˜ë¦¬\n",
    "                    if col_name == 'spec':\n",
    "                        # JSON í•„ë“œ ì²˜ë¦¬\n",
    "                        parsed_spec = parse_json_field_safe(value)\n",
    "                        if parsed_spec:\n",
    "                            product['spec'] = parsed_spec\n",
    "                    \n",
    "                    elif col_name == 'card_promotion':\n",
    "                        # JSON í•„ë“œ ì²˜ë¦¬\n",
    "                        parsed_promo = parse_json_field_safe(value)\n",
    "                        if parsed_promo:\n",
    "                            product['card_promotion'] = parsed_promo\n",
    "                    \n",
    "                    elif col_name == 'release_date':\n",
    "                        # ë‚ ì§œ í•„ë“œ ì²˜ë¦¬\n",
    "                        try:\n",
    "                            date_obj = pd.to_datetime(value)\n",
    "                            product['release_date'] = date_obj.isoformat()\n",
    "                        except:\n",
    "                            product['release_date'] = str(value)\n",
    "                    \n",
    "                    elif col_name in ['review_num', 'stock_qty', 'ctg_rank_recommend', \n",
    "                                      'ctg_rank_quantity', 'ctg_rank_rating']:\n",
    "                        # ì •ìˆ˜ í•„ë“œ ì²˜ë¦¬\n",
    "                        try:\n",
    "                            product[col_name] = int(float(value))\n",
    "                        except (ValueError, TypeError):\n",
    "                            product[col_name] = 0\n",
    "                    \n",
    "                    elif col_name in ['sale_prc', 'sale_prc1', 'sale_prc2', 'sale_prc3', \n",
    "                                      'web_cd_dc_amt', 'web_cp_dc_amt', 'estm_score']:\n",
    "                        # ì‹¤ìˆ˜ í•„ë“œ ì²˜ë¦¬\n",
    "                        try:\n",
    "                            # web_cp_dc_amtëŠ” web_cd_dc_amtë¡œ ë³€í™˜\n",
    "                            field_name = 'web_cd_dc_amt' if col_name == 'web_cp_dc_amt' else col_name\n",
    "                            product[field_name] = float(value)\n",
    "                        except (ValueError, TypeError):\n",
    "                            pass\n",
    "                    \n",
    "                    elif col_name in ['aisc_yn', 'sc_yn', 'gc_yn', 'div_pay_apl_yn', 'show_yn']:\n",
    "                        # Y/N í•„ë“œ ì²˜ë¦¬\n",
    "                        str_value = str(value).strip().upper()\n",
    "                        if str_value in ['Y', 'N']:\n",
    "                            product[col_name] = str_value\n",
    "                    \n",
    "                    else:\n",
    "                        # ê¸°ë³¸ ë¬¸ìì—´ ì²˜ë¦¬\n",
    "                        product[col_name] = str(value).strip()\n",
    "                \n",
    "                # ì¶”ê°€ ë©”íƒ€ë°ì´í„°\n",
    "                product.update({\n",
    "                    \"imported_from\": \"PostgreSQL\",\n",
    "                    \"imported_at\": datetime.now().isoformat(),\n",
    "                    \"source_file\": os.path.basename(PG_UPLOAD_FILE_PATH),\n",
    "                    \"record_index\": idx\n",
    "                })\n",
    "                \n",
    "                # í•„ìˆ˜ í•„ë“œ í™•ì¸ (ë” ê´€ëŒ€í•˜ê²Œ)\n",
    "                has_required = any(key in product and product[key] for key in ['mdl_code', 'goods_id', 'goods_nm'])\n",
    "                \n",
    "                if has_required or len(product) > 4:  # ë©”íƒ€ë°ì´í„° 4ê°œ + ì‹¤ì œ ë°ì´í„°\n",
    "                    products_data.append(product)\n",
    "                else:\n",
    "                    print(f\"   âš ï¸  í–‰ {idx}: ìœ íš¨í•œ ë°ì´í„° ì—†ìŒ (í•„ë“œ ìˆ˜: {len(product)})\")\n",
    "                    conversion_errors += 1\n",
    "                \n",
    "                # ì²˜ìŒ ëª‡ ê°œ ë³€í™˜ ê²°ê³¼ ì¶œë ¥\n",
    "                if idx < 3:\n",
    "                    print(f\"   í–‰ {idx} ë³€í™˜ ê²°ê³¼: {len(product)}ê°œ í•„ë“œ\")\n",
    "                    for key, val in list(product.items())[:5]:\n",
    "                        print(f\"      {key}: {val}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                conversion_errors += 1\n",
    "                if conversion_errors <= 5:\n",
    "                    print(f\"   âš ï¸  í–‰ {idx} ë³€í™˜ ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "        print(f\"\\nâœ… ë³€í™˜ ì™„ë£Œ:\")\n",
    "        print(f\"   ì„±ê³µ: {len(products_data)}ê°œ ë¬¸ì„œ\")\n",
    "        print(f\"   ì‹¤íŒ¨: {conversion_errors}ê°œ ë¬¸ì„œ\")\n",
    "        \n",
    "        return {\n",
    "            \"products\": products_data,\n",
    "            \"customers\": [],\n",
    "            \"orders\": []\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# PostgreSQL ë°ì´í„° ë¡œë“œ ë° ë³€í™˜\n",
    "pg_data = load_and_transform_pg_data()\n",
    "\n",
    "# ê²°ê³¼ ìš”ì•½\n",
    "if pg_data and pg_data['products']:\n",
    "    print(f\"\\nğŸ‰ ìµœì¢… ê²°ê³¼:\")\n",
    "    print(f\"   ë³€í™˜ëœ ì œí’ˆ ìˆ˜: {len(pg_data['products'])}ê°œ\")\n",
    "    \n",
    "    if pg_data['products']:\n",
    "        sample = pg_data['products'][0]\n",
    "        print(f\"   ì²« ë²ˆì§¸ ì œí’ˆ í•„ë“œ ìˆ˜: {len(sample)}ê°œ\")\n",
    "        print(f\"   ì£¼ìš” í•„ë“œ: {[k for k in sample.keys() if not k.startswith('imported')][:10]}\")\n",
    "else:\n",
    "    print(f\"\\nâŒ ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨\")\n",
    "    pg_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46448e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965wywhzxfh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDBì— PostgreSQL ë°ì´í„° ì‚½ì… (kt_merged_product_20251001)\n",
    "def insert_pg_data_to_mongodb(collections, pg_data):\n",
    "    \"\"\"PostgreSQL ë°ì´í„°ë¥¼ MongoDB kt_merged_product_20251001 ì»¬ë ‰ì…˜ì— ì‚½ì…\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"PostgreSQL ë°ì´í„°ë¥¼ MongoDBì— ì‚½ì…\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if collections is None:\n",
    "        print(\"âŒ ì»¬ë ‰ì…˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return False\n",
    "    \n",
    "    if pg_data is None:\n",
    "        print(\"âŒ ì‚½ì…í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # kt_merged_product_20251001 ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°\n",
    "        kt_merged_product_20251001 = collections.get(\"kt_merged_product_20251001\")\n",
    "        \n",
    "        # MongoDB Collection ê°ì²´ëŠ” Noneê³¼ ë¹„êµí•´ì•¼ í•¨\n",
    "        if kt_merged_product_20251001 is None:\n",
    "            print(\"âŒ kt_merged_product_20251001 ì»¬ë ‰ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return False\n",
    "        \n",
    "        # products ë°ì´í„° í™•ì¸ ë° ì‚½ì…\n",
    "        if pg_data.get(\"products\") and len(pg_data[\"products\"]) > 0:\n",
    "            # ê¸°ì¡´ PostgreSQL ë°ì´í„° ì‚­ì œ (ì„ íƒì )\n",
    "            try:\n",
    "                delete_result = kt_merged_product_20251001.delete_many({\"imported_from\": \"PostgreSQL\"})\n",
    "                if delete_result.deleted_count > 0:\n",
    "                    print(f\"   â„¹ï¸  ê¸°ì¡´ PostgreSQL ë°ì´í„° {delete_result.deleted_count}ê°œ ì‚­ì œ\")\n",
    "            except Exception as delete_error:\n",
    "                print(f\"   âš ï¸  ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {delete_error}\")\n",
    "            \n",
    "            # ë°°ì¹˜ í¬ê¸° ì„¤ì • (Cosmos DBëŠ” ì‘ì€ ë°°ì¹˜ê°€ íš¨ìœ¨ì )\n",
    "            batch_size = 50  # Cosmos DB ìµœì í™”ë¥¼ ìœ„í•´ ì‘ì€ ë°°ì¹˜ ì‚¬ìš©\n",
    "            total_products = len(pg_data[\"products\"])\n",
    "            inserted_count = 0\n",
    "            failed_count = 0\n",
    "            failed_samples = []\n",
    "            \n",
    "            print(f\"\\nğŸ“¤ ë°ì´í„° ì‚½ì… ì‹œì‘ ({total_products}ê°œ ë¬¸ì„œ)\")\n",
    "            \n",
    "            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì‚½ì…\n",
    "            for i in range(0, total_products, batch_size):\n",
    "                batch = pg_data[\"products\"][i:i + batch_size]\n",
    "                try:\n",
    "                    result = kt_merged_product_20251001.insert_many(batch, ordered=False)\n",
    "                    inserted_count += len(result.inserted_ids)\n",
    "                    \n",
    "                    # ì§„í–‰ ìƒí™© í‘œì‹œ\n",
    "                    progress = min(i + batch_size, total_products)\n",
    "                    print(f\"   ì§„í–‰ì¤‘: {progress}/{total_products} ({progress*100//total_products}%)\")\n",
    "                    \n",
    "                except Exception as batch_error:\n",
    "                    error_msg = str(batch_error)\n",
    "                    if \"duplicate key\" in error_msg.lower():\n",
    "                        print(f\"   âš ï¸  ë°°ì¹˜ì— ì¤‘ë³µ í‚¤ ì¡´ì¬ - ê°œë³„ ì‚½ì… ì‹œë„\")\n",
    "                    else:\n",
    "                        print(f\"   âš ï¸  ë°°ì¹˜ ì‚½ì… ì¼ë¶€ ì‹¤íŒ¨: {error_msg[:100]}\")\n",
    "                    \n",
    "                    # ê°œë³„ ë¬¸ì„œ ì‚½ì… ì‹œë„\n",
    "                    for doc in batch:\n",
    "                        try:\n",
    "                            kt_merged_product_20251001.insert_one(doc)\n",
    "                            inserted_count += 1\n",
    "                        except Exception as doc_error:\n",
    "                            failed_count += 1\n",
    "                            if failed_count <= 3:  # ì²˜ìŒ 3ê°œ ì˜¤ë¥˜ë§Œ ìƒ˜í”Œë¡œ ì €ì¥\n",
    "                                failed_samples.append({\n",
    "                                    'mdl_code': doc.get('mdl_code', 'Unknown'),\n",
    "                                    'error': str(doc_error)[:100]\n",
    "                                })\n",
    "            \n",
    "            print(f\"\\nâœ… ë°ì´í„° ì‚½ì… ì™„ë£Œ:\")\n",
    "            print(f\"   ì„±ê³µ: {inserted_count}ê°œ ë¬¸ì„œ\")\n",
    "            print(f\"   ì‹¤íŒ¨: {failed_count}ê°œ ë¬¸ì„œ\")\n",
    "            \n",
    "            if failed_samples:\n",
    "                print(f\"\\nâš ï¸  ì‹¤íŒ¨ ìƒ˜í”Œ:\")\n",
    "                for sample in failed_samples:\n",
    "                    print(f\"   â€¢ {sample['mdl_code']}: {sample['error']}\")\n",
    "            \n",
    "            # ì‚½ì… í›„ í†µê³„\n",
    "            print(\"\\nğŸ“Š ì»¬ë ‰ì…˜ í†µê³„:\")\n",
    "            try:\n",
    "                total_docs = kt_merged_product_20251001.count_documents({})\n",
    "                pg_docs = kt_merged_product_20251001.count_documents({\"imported_from\": \"PostgreSQL\"})\n",
    "                print(f\"   ì „ì²´ ë¬¸ì„œ: {total_docs}ê°œ\")\n",
    "                print(f\"   PostgreSQL ë°ì´í„°: {pg_docs}ê°œ\")\n",
    "                \n",
    "                # spec í•„ë“œê°€ ìˆëŠ” ë¬¸ì„œ í™•ì¸\n",
    "                spec_docs = kt_merged_product_20251001.count_documents({\n",
    "                    \"spec\": {\"$exists\": True, \"$ne\": {}}\n",
    "                })\n",
    "                print(f\"   spec í•„ë“œ ë³´ìœ : {spec_docs}ê°œ\")\n",
    "                \n",
    "                # card_promotion í•„ë“œê°€ ìˆëŠ” ë¬¸ì„œ í™•ì¸\n",
    "                card_promo_docs = kt_merged_product_20251001.count_documents({\n",
    "                    \"card_promotion\": {\"$exists\": True, \"$ne\": {}}\n",
    "                })\n",
    "                print(f\"   card_promotion í•„ë“œ ë³´ìœ : {card_promo_docs}ê°œ\")\n",
    "            except Exception as stat_error:\n",
    "                print(f\"   âš ï¸  í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {stat_error}\")\n",
    "            \n",
    "            # ìƒ˜í”Œ ë¬¸ì„œ í™•ì¸\n",
    "            try:\n",
    "                sample = kt_merged_product_20251001.find_one({\n",
    "                    \"imported_from\": \"PostgreSQL\",\n",
    "                    \"spec\": {\"$exists\": True, \"$ne\": {}}\n",
    "                })\n",
    "                \n",
    "                if sample:\n",
    "                    print(\"\\nğŸ“‹ ì‚½ì…ëœ ë°ì´í„° ìƒ˜í”Œ:\")\n",
    "                    print(f\"   ëª¨ë¸ì½”ë“œ: {sample.get('mdl_code')}\")\n",
    "                    print(f\"   ìƒí’ˆëª…: {sample.get('goods_nm')}\")\n",
    "                    print(f\"   ì‚¬ì´íŠ¸: {sample.get('site_cd')}\")\n",
    "                    \n",
    "                    sale_prc = sample.get('sale_prc')\n",
    "                    if sale_prc is not None:\n",
    "                        print(f\"   ê°€ê²©: {sale_prc:,}ì›\")\n",
    "                    \n",
    "                    if isinstance(sample.get('spec'), dict):\n",
    "                        spec_keys = list(sample.get('spec', {}).keys())\n",
    "                        print(f\"   spec í•„ë“œ í‚¤: {spec_keys[:5]}\")\n",
    "                    \n",
    "                    if isinstance(sample.get('card_promotion'), dict):\n",
    "                        promo_keys = list(sample.get('card_promotion', {}).keys())\n",
    "                        print(f\"   card_promotion í‚¤: {promo_keys}\")\n",
    "            except Exception as sample_error:\n",
    "                print(f\"   âš ï¸  ìƒ˜í”Œ ì¡°íšŒ ì‹¤íŒ¨: {sample_error}\")\n",
    "            \n",
    "            # ì¸ë±ìŠ¤ í†µê³„ (ì„ íƒì )\n",
    "            try:\n",
    "                index_stats = kt_merged_product_20251001.index_information()\n",
    "                print(f\"\\nğŸ“ í™œì„± ì¸ë±ìŠ¤: {len(index_stats)}ê°œ\")\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(\"âŒ Products ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë°ì´í„° ì‚½ì… ì‹¤íŒ¨: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# PostgreSQL ë°ì´í„° ì‚½ì… ì‹¤í–‰\n",
    "if collections is not None and pg_data is not None:\n",
    "    insert_result = insert_pg_data_to_mongodb(collections, pg_data)\n",
    "else:\n",
    "    print(\"ì»¬ë ‰ì…˜ ë˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:\")\n",
    "    print(\"1. MongoDB ì—°ê²°ì´ ì„±ê³µí–ˆëŠ”ì§€\")\n",
    "    print(\"2. ì»¬ë ‰ì…˜ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€\")\n",
    "    print(\"3. PostgreSQL ë°ì´í„°ê°€ ë¡œë“œë˜ì—ˆëŠ”ì§€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adlv1syf3y",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB kt_merged_product_20251001 ë°ì´í„° ì¡°íšŒ ë° ê²€ì¦\n",
    "def query_mongodb_kt_merged_product_20251001(collections):\n",
    "    \"\"\"MongoDB kt_merged_product_20251001 ì»¬ë ‰ì…˜ì—ì„œ ë°ì´í„° ì¡°íšŒ\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"MongoDB kt_merged_product_20251001 ë°ì´í„° ì¡°íšŒ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if collections is None:\n",
    "        print(\"âŒ ì»¬ë ‰ì…˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        kt_merged_product_20251001 = collections.get(\"kt_merged_product_20251001\")\n",
    "        \n",
    "        # MongoDB Collection ê°ì²´ëŠ” Noneê³¼ ë¹„êµ\n",
    "        if kt_merged_product_20251001 is None:\n",
    "            print(\"âŒ kt_merged_product_20251001 ì»¬ë ‰ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "        \n",
    "        # 1. ê¸°ë³¸ í†µê³„\n",
    "        total_docs = kt_merged_product_20251001.count_documents({})\n",
    "        pg_docs = kt_merged_product_20251001.count_documents({\"imported_from\": \"PostgreSQL\"})\n",
    "        \n",
    "        print(\"\\nğŸ“Š 1. ì»¬ë ‰ì…˜ í†µê³„:\")\n",
    "        print(f\"   ì „ì²´ ë¬¸ì„œ ìˆ˜: {total_docs:,}\")\n",
    "        print(f\"   PostgreSQL ë°ì´í„°: {pg_docs:,}\")\n",
    "        \n",
    "        if total_docs == 0:\n",
    "            print(\"âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ì‚½ì…ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "            return\n",
    "        \n",
    "        # 2. ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ\n",
    "        print(\"\\nğŸ“‹ 2. ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 3ê°œ):\")\n",
    "        samples = list(kt_merged_product_20251001.find().limit(3))\n",
    "        \n",
    "        for idx, product in enumerate(samples, 1):\n",
    "            print(f\"\\n   [{idx}] ì œí’ˆ ì •ë³´:\")\n",
    "            print(f\"       ìƒí’ˆëª…: {product.get('goods_nm', 'N/A')}\")\n",
    "            print(f\"       ëª¨ë¸ì½”ë“œ: {product.get('mdl_code', 'N/A')}\")\n",
    "            print(f\"       ìƒí’ˆID: {product.get('goods_id', 'N/A')}\")\n",
    "            print(f\"       ì‚¬ì´íŠ¸: {product.get('site_cd', 'N/A')}\")\n",
    "            print(f\"       ì¹´í…Œê³ ë¦¬: {product.get('disp_lv1', '')}/{product.get('disp_lv2', '')}/{product.get('disp_lv3', '')}\")\n",
    "            \n",
    "            if product.get('sale_prc') is not None:\n",
    "                print(f\"       ê°€ê²©: {product.get('sale_prc'):,}ì›\")\n",
    "            \n",
    "            if product.get('stock_qty') is not None:\n",
    "                print(f\"       ì¬ê³ : {product.get('stock_qty'):,}ê°œ\")\n",
    "            \n",
    "            # spec ì •ë³´\n",
    "            if product.get('spec') and isinstance(product.get('spec'), dict):\n",
    "                spec_keys = list(product.get('spec').keys())[:3]\n",
    "                print(f\"       spec ì •ë³´: {spec_keys}\")\n",
    "        \n",
    "        # 3. ê°€ê²© ë²”ìœ„ë³„ ë¶„í¬\n",
    "        print(\"\\nğŸ’° 3. ê°€ê²© ë²”ìœ„ë³„ ì œí’ˆ ë¶„í¬:\")\n",
    "        price_pipeline = [\n",
    "            {\"$match\": {\"sale_prc\": {\"$exists\": True, \"$ne\": None}}},\n",
    "            {\"$bucket\": {\n",
    "                \"groupBy\": \"$sale_prc\",\n",
    "                \"boundaries\": [0, 500000, 1000000, 2000000, 5000000, float('inf')],\n",
    "                \"default\": \"ê¸°íƒ€\",\n",
    "                \"output\": {\n",
    "                    \"count\": {\"$sum\": 1},\n",
    "                    \"avg_price\": {\"$avg\": \"$sale_prc\"}\n",
    "                }\n",
    "            }}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            price_results = list(kt_merged_product_20251001.aggregate(price_pipeline))\n",
    "            price_labels = [\"~50ë§Œì›\", \"50~100ë§Œì›\", \"100~200ë§Œì›\", \"200~500ë§Œì›\", \"500ë§Œì›~\"]\n",
    "            \n",
    "            for idx, result in enumerate(price_results):\n",
    "                if result['_id'] != \"ê¸°íƒ€\":\n",
    "                    label = price_labels[min(idx, len(price_labels)-1)]\n",
    "                    print(f\"   {label}: {result['count']:,}ê°œ (í‰ê· : {result['avg_price']:,.0f}ì›)\")\n",
    "        except Exception as price_error:\n",
    "            print(f\"   âš ï¸  ê°€ê²© ë¶„í¬ ë¶„ì„ ì‹¤íŒ¨: {price_error}\")\n",
    "        \n",
    "        # 4. ì‚¬ì´íŠ¸ë³„ í†µê³„\n",
    "        print(\"\\nğŸ“ˆ 4. ì‚¬ì´íŠ¸ë³„ ì œí’ˆ í†µê³„:\")\n",
    "        site_pipeline = [\n",
    "            {\"$match\": {\"site_cd\": {\"$exists\": True, \"$ne\": None}}},\n",
    "            {\"$group\": {\n",
    "                \"_id\": \"$site_cd\",\n",
    "                \"count\": {\"$sum\": 1},\n",
    "                \"avg_price\": {\"$avg\": \"$sale_prc\"},\n",
    "                \"max_price\": {\"$max\": \"$sale_prc\"},\n",
    "                \"min_price\": {\"$min\": \"$sale_prc\"},\n",
    "                \"avg_stock\": {\"$avg\": \"$stock_qty\"}\n",
    "            }},\n",
    "            {\"$sort\": {\"count\": -1}},\n",
    "            {\"$limit\": 10}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            site_results = list(kt_merged_product_20251001.aggregate(site_pipeline))\n",
    "            for result in site_results:\n",
    "                print(f\"   ì‚¬ì´íŠ¸ '{result['_id']}':\")\n",
    "                print(f\"      ì œí’ˆ ìˆ˜: {result['count']:,}ê°œ\")\n",
    "                if result.get('avg_price') is not None:\n",
    "                    print(f\"      í‰ê·  ê°€ê²©: {result['avg_price']:,.0f}ì›\")\n",
    "                if result.get('avg_stock') is not None:\n",
    "                    print(f\"      í‰ê·  ì¬ê³ : {result['avg_stock']:.0f}ê°œ\")\n",
    "        except Exception as site_error:\n",
    "            print(f\"   âš ï¸  ì‚¬ì´íŠ¸ë³„ í†µê³„ ì‹¤íŒ¨: {site_error}\")\n",
    "        \n",
    "        # 5. ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ (disp_lv1 ê¸°ì¤€)\n",
    "        print(\"\\nğŸ“¦ 5. ëŒ€ë¶„ë¥˜ë³„ ì œí’ˆ í†µê³„ (ìƒìœ„ 10ê°œ):\")\n",
    "        category_pipeline = [\n",
    "            {\"$match\": {\"disp_lv1\": {\"$exists\": True, \"$ne\": \"\"}}},\n",
    "            {\"$group\": {\n",
    "                \"_id\": \"$disp_lv1\",\n",
    "                \"count\": {\"$sum\": 1},\n",
    "                \"avg_price\": {\"$avg\": \"$sale_prc\"},\n",
    "                \"brands\": {\"$addToSet\": \"$disp_lv2\"}\n",
    "            }},\n",
    "            {\"$sort\": {\"count\": -1}},\n",
    "            {\"$limit\": 10}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            category_results = list(kt_merged_product_20251001.aggregate(category_pipeline))\n",
    "            for result in category_results:\n",
    "                print(f\"   {result['_id']}: {result['count']:,}ê°œ\")\n",
    "                if result.get('avg_price') is not None:\n",
    "                    print(f\"      í‰ê·  ê°€ê²©: {result['avg_price']:,.0f}ì›\")\n",
    "                if result.get('brands'):\n",
    "                    print(f\"      í•˜ìœ„ ì¹´í…Œê³ ë¦¬: {len(result['brands'])}ê°œ\")\n",
    "        except Exception as cat_error:\n",
    "            print(f\"   âš ï¸  ì¹´í…Œê³ ë¦¬ í†µê³„ ì‹¤íŒ¨: {cat_error}\")\n",
    "        \n",
    "        # 6. spec í•„ë“œ ë¶„ì„\n",
    "        print(\"\\nğŸ”§ 6. spec í•„ë“œ ë¶„ì„:\")\n",
    "        try:\n",
    "            spec_count = kt_merged_product_20251001.count_documents({\n",
    "                \"spec\": {\"$exists\": True, \"$ne\": {}}\n",
    "            })\n",
    "            print(f\"   spec í•„ë“œ ë³´ìœ  ì œí’ˆ: {spec_count:,}ê°œ ({spec_count*100//total_docs if total_docs else 0}%)\")\n",
    "            \n",
    "            # spec í‚¤ í†µê³„\n",
    "            if spec_count > 0:\n",
    "                spec_sample = kt_merged_product_20251001.find_one({\"spec\": {\"$exists\": True, \"$ne\": {}}})\n",
    "                if spec_sample and spec_sample.get('spec'):\n",
    "                    print(f\"   spec ìƒ˜í”Œ í‚¤: {list(spec_sample['spec'].keys())[:10]}\")\n",
    "        except Exception as spec_error:\n",
    "            print(f\"   âš ï¸  spec ë¶„ì„ ì‹¤íŒ¨: {spec_error}\")\n",
    "        \n",
    "        # 7. ë‚ ì§œ í•„ë“œ ë¶„ì„\n",
    "        print(\"\\nğŸ“… 7. ì¶œì‹œì¼ ë¶„ì„:\")\n",
    "        try:\n",
    "            date_count = kt_merged_product_20251001.count_documents({\n",
    "                \"release_date\": {\"$exists\": True, \"$ne\": None}\n",
    "            })\n",
    "            print(f\"   ì¶œì‹œì¼ ì •ë³´ ë³´ìœ : {date_count:,}ê°œ ({date_count*100//total_docs if total_docs else 0}%)\")\n",
    "        except Exception as date_error:\n",
    "            print(f\"   âš ï¸  ë‚ ì§œ ë¶„ì„ ì‹¤íŒ¨: {date_error}\")\n",
    "        \n",
    "        # 8. ì¬ê³  ìƒíƒœ ë¶„ì„\n",
    "        print(\"\\nğŸ“¦ 8. ì¬ê³  ìƒíƒœ:\")\n",
    "        stock_pipeline = [\n",
    "            {\"$match\": {\"stock_qty\": {\"$exists\": True}}},\n",
    "            {\"$group\": {\n",
    "                \"_id\": None,\n",
    "                \"total_stock\": {\"$sum\": \"$stock_qty\"},\n",
    "                \"avg_stock\": {\"$avg\": \"$stock_qty\"},\n",
    "                \"max_stock\": {\"$max\": \"$stock_qty\"},\n",
    "                \"out_of_stock\": {\"$sum\": {\"$cond\": [{\"$eq\": [\"$stock_qty\", 0]}, 1, 0]}},\n",
    "                \"low_stock\": {\"$sum\": {\"$cond\": [{\"$and\": [{\"$gt\": [\"$stock_qty\", 0]}, {\"$lte\": [\"$stock_qty\", 10]}]}, 1, 0]}}\n",
    "            }}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            stock_results = list(kt_merged_product_20251001.aggregate(stock_pipeline))\n",
    "            if stock_results:\n",
    "                result = stock_results[0]\n",
    "                print(f\"   ì´ ì¬ê³ : {result.get('total_stock', 0):,}ê°œ\")\n",
    "                print(f\"   í‰ê·  ì¬ê³ : {result.get('avg_stock', 0):.1f}ê°œ\")\n",
    "                print(f\"   ìµœëŒ€ ì¬ê³ : {result.get('max_stock', 0):,}ê°œ\")\n",
    "                print(f\"   í’ˆì ˆ ìƒí’ˆ: {result.get('out_of_stock', 0):,}ê°œ\")\n",
    "                print(f\"   ì¬ê³  ë¶€ì¡±(10ê°œ ì´í•˜): {result.get('low_stock', 0):,}ê°œ\")\n",
    "        except Exception as stock_error:\n",
    "            print(f\"   âš ï¸  ì¬ê³  ë¶„ì„ ì‹¤íŒ¨: {stock_error}\")\n",
    "        \n",
    "        # 9. ê²€ìƒ‰ ì˜ˆì‹œ\n",
    "        print(\"\\nğŸ” 9. í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì˜ˆì‹œ:\")\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ ê²€ìƒ‰ (ì¸ë±ìŠ¤ê°€ ìˆëŠ” ê²½ìš°)\n",
    "        try:\n",
    "            search_results = kt_merged_product_20251001.find(\n",
    "                {\"$text\": {\"$search\": \"ê°¤ëŸ­ì‹œ\"}},\n",
    "                {\"score\": {\"$meta\": \"textScore\"}}\n",
    "            ).sort([(\"score\", {\"$meta\": \"textScore\"})]).limit(5)\n",
    "            \n",
    "            search_count = 0\n",
    "            for product in search_results:\n",
    "                search_count += 1\n",
    "                print(f\"   â€¢ {product.get('goods_nm')} (ì ìˆ˜: {product.get('score', 0):.2f})\")\n",
    "            \n",
    "            if search_count == 0:\n",
    "                raise Exception(\"í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ\")\n",
    "                \n",
    "        except:\n",
    "            # ì •ê·œì‹ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´\n",
    "            try:\n",
    "                regex_results = kt_merged_product_20251001.find(\n",
    "                    {\"goods_nm\": {\"$regex\": \"ê°¤ëŸ­ì‹œ\", \"$options\": \"i\"}}\n",
    "                ).limit(5)\n",
    "                \n",
    "                for product in regex_results:\n",
    "                    print(f\"   â€¢ {product.get('goods_nm')} ({product.get('mdl_code')})\")\n",
    "            except Exception as search_error:\n",
    "                print(f\"   âš ï¸  ê²€ìƒ‰ ì‹¤íŒ¨: {search_error}\")\n",
    "        \n",
    "        # 10. ë³µì¡í•œ ì¿¼ë¦¬ ì˜ˆì‹œ - ê³ ê°€ ì œí’ˆ ì¤‘ ì¬ê³ ê°€ ìˆëŠ” ì œí’ˆ\n",
    "        print(\"\\nğŸ’ 10. ê³ ê°€ ì œí’ˆ (200ë§Œì› ì´ìƒ, ì¬ê³  ìˆìŒ):\")\n",
    "        try:\n",
    "            premium_products = kt_merged_product_20251001.find({\n",
    "                \"sale_prc\": {\"$gte\": 2000000},\n",
    "                \"stock_qty\": {\"$gt\": 0}\n",
    "            }).sort(\"sale_prc\", -1).limit(5)\n",
    "            \n",
    "            premium_count = 0\n",
    "            for product in premium_products:\n",
    "                premium_count += 1\n",
    "                print(f\"   â€¢ {product.get('goods_nm')}\")\n",
    "                print(f\"      ê°€ê²©: {product.get('sale_prc'):,}ì›\")\n",
    "                print(f\"      ì¬ê³ : {product.get('stock_qty'):,}ê°œ\")\n",
    "                print(f\"      ì‚¬ì´íŠ¸: {product.get('site_cd')}\")\n",
    "            \n",
    "            if premium_count == 0:\n",
    "                print(\"   í•´ë‹¹ ì¡°ê±´ì˜ ì œí’ˆì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        except Exception as premium_error:\n",
    "            print(f\"   âš ï¸  ê³ ê°€ ì œí’ˆ ì¡°íšŒ ì‹¤íŒ¨: {premium_error}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# ë°ì´í„° ì¡°íšŒ ì‹¤í–‰\n",
    "if collections is not None:\n",
    "    query_mongodb_kt_merged_product_20251001(collections)\n",
    "else:\n",
    "    print(\"ì»¬ë ‰ì…˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. MongoDB ì—°ê²°ê³¼ ì»¬ë ‰ì…˜ ìƒì„±ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c1ca4",
   "metadata": {},
   "source": [
    "### Appendix (v_spec_type_check_table_20251015) ì¶”ê°€ - DBeaverë¡œ ë°ì´í„° ëˆ„ë½ì´ ë˜ì–´ ì‘ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6610429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ 11474ê°œ í–‰ì´ ì„±ê³µì ìœ¼ë¡œ ì‚½ì…ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# CREATE TABLE IF NOT EXISTS v_spec_type_check_table_20251015 (\n",
    "# \tdisp_lv1 VARCHAR(1000),\n",
    "# \tdisp_lv2 VARCHAR(1000),\n",
    "# \tdisp_lv3 VARCHAR(1000),\n",
    "# \tdisp_nm1 VARCHAR(1000),\n",
    "# \tdisp_nm2 VARCHAR(1000),\n",
    "# \ttotal_count INT4,\n",
    "# \tcnt_numeric INT4,\n",
    "# \tcnt_non_numeric INT4,\n",
    "# \tsymbols VARCHAR(100),\n",
    "# \tnumericvalue TEXT,\n",
    "# \tnonnumericvalue TEXT,\n",
    "# \tcnt_ck INT4\n",
    "# );\n",
    "\n",
    "import csv\n",
    "import psycopg2\n",
    "from psycopg2 import extras\n",
    "\n",
    "def load_tsv_to_postgresql(tsv_file_path):\n",
    "    \"\"\"\n",
    "    TSV íŒŒì¼ì„ ì½ì–´ì„œ PostgreSQL í…Œì´ë¸”ì— ì‚½ì…\n",
    "    \n",
    "    Args:\n",
    "        tsv_file_path: TSV íŒŒì¼ ê²½ë¡œ\n",
    "    \"\"\"\n",
    "    \n",
    "    # PostgreSQL ì—°ê²°\n",
    "    conn = psycopg2.connect(\n",
    "        host=PG_HOST,\n",
    "        port=PG_PORT,\n",
    "        database=PG_DATABASE,\n",
    "        user=PG_USER,\n",
    "        password=PG_PASSWORD\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # TSV íŒŒì¼ ì½ê¸°\n",
    "        with open(tsv_file_path, 'r', encoding='utf-8') as f:\n",
    "            # TSV íŒŒì¼ íŒŒì‹± (íƒ­ êµ¬ë¶„ì)\n",
    "            tsv_reader = csv.reader(f, delimiter='\\t')\n",
    "            \n",
    "            # í—¤ë”ê°€ ìˆë‹¤ë©´ ìŠ¤í‚µ (í•„ìš”ì‹œ)\n",
    "            next(tsv_reader)\n",
    "            \n",
    "            # ë°ì´í„° ì¤€ë¹„\n",
    "            data_to_insert = []\n",
    "            for row in tsv_reader:\n",
    "                # ë¹ˆ ë¬¸ìì—´(\"\")ì„ Noneìœ¼ë¡œ ë³€í™˜ (NULL ì²˜ë¦¬)\n",
    "                processed_row = [\n",
    "                    None if val == '\"\"' or val == '' else val \n",
    "                    for val in row\n",
    "                ]\n",
    "                data_to_insert.append(tuple(processed_row))\n",
    "            \n",
    "            # INSERT ì¿¼ë¦¬\n",
    "            insert_query = \"\"\"\n",
    "                INSERT INTO v_spec_type_check_table_20251015 \n",
    "                (disp_lv1, disp_lv2, disp_lv3, disp_nm1, disp_nm2, total_count, cnt_numeric, cnt_non_numeric, symbols, numericvalue, nonnumericvalue, cnt_ck)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "            \n",
    "            # ë°°ì¹˜ ì‚½ì… (ì„±ëŠ¥ í–¥ìƒ)\n",
    "            extras.execute_batch(cursor, insert_query, data_to_insert)\n",
    "            \n",
    "            # ì»¤ë°‹\n",
    "            conn.commit()\n",
    "            print(f\"âœ“ {len(data_to_insert)}ê°œ í–‰ì´ ì„±ê³µì ìœ¼ë¡œ ì‚½ì…ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(f\"âœ— ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "if __name__ == \"__main__\":\n",
    "    # TSV íŒŒì¼ ê²½ë¡œ\n",
    "    tsv_file = '/Users/toby/prog/kt/rubicon/data/v_spec_type_check_table_20251015.tsv'\n",
    "    \n",
    "    # ì‹¤í–‰\n",
    "    load_tsv_to_postgresql(tsv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d697c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
