{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1otlvs5st1k",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SQLAlchemy 엔진 생성 성공\n",
      "\n",
      "================================================================================\n",
      "📥 데이터 로드 중...\n",
      "================================================================================\n",
      "✅ 테이블 'test_spec_01'에서 955개 행 로드 완료\n",
      "✅ allowed_disp_nm1로 필터링: 625개 행\n",
      "\n",
      "================================================================================\n",
      "🔄 데이터 파싱 중...\n",
      "================================================================================\n",
      "✅ 파싱 성공 (확실): 80개 행\n",
      "⚠️  파싱 성공 (체크 필요): 1769개 행\n",
      "❌ 파싱 실패: 5개 행\n",
      "📈 전체 대비 파싱률: 295.8%\n",
      "\n",
      "================================================================================\n",
      "💾 데이터 저장 중...\n",
      "================================================================================\n",
      "✅ 소스 테이블 'test_spec_01' 스키마 읽기 완료 (13개 컬럼)\n",
      "✅ 테이블 'test_spec_02' 생성/확인 완료\n",
      "   추가된 컬럼: dimension_type, parsed_value, needs_check\n",
      "✅ 테이블 'test_spec_02'의 기존 데이터 삭제 완료\n",
      "✅ 소스 테이블 'test_spec_01' 스키마 읽기 완료 (13개 컬럼)\n",
      "✅ 테이블 'test_spec_02'에 1849개 행 저장 완료\n",
      "\n",
      "================================================================================\n",
      "✅ 전체 작업 완료!\n",
      "================================================================================\n",
      "📊 요약:\n",
      "  - 소스 테이블: test_spec_01\n",
      "  - 타겟 테이블: test_spec_02\n",
      "  - 저장된 데이터: 1849개 행\n",
      "  - 기존 데이터 삭제: 예\n",
      "  - 타겟 테이블 스키마: 소스 테이블 컬럼 + dimension_type, parsed_value, needs_check\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 실행 예시: PostgreSQL에서 데이터 로드 및 파싱 후 저장\n",
    "# ============================================\n",
    "\n",
    "# 설정\n",
    "SOURCE_TABLE = \"test_spec_01\"  # 소스 테이블명 (여기를 수정하세요)\n",
    "TARGET_TABLE = \"test_spec_02\"  # 타겟 테이블명 (여기를 수정하세요)\n",
    "TRUNCATE_BEFORE_INSERT = True  # True: 기존 데이터 삭제 후 삽입, False: 기존 데이터 유지하고 추가\n",
    "\n",
    "# 1. SQLAlchemy 엔진 생성\n",
    "engine = get_sqlalchemy_engine()\n",
    "\n",
    "if engine is None:\n",
    "    print(\"❌ 엔진 생성 실패. 작업을 중단합니다.\")\n",
    "else:\n",
    "    # 2. 데이터 로드\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📥 데이터 로드 중...\")\n",
    "    print(\"=\"*80)\n",
    "    df_filtered = load_data_from_table(engine, SOURCE_TABLE, allowed_disp_nm1)\n",
    "    \n",
    "    if df_filtered is not None and len(df_filtered) > 0:\n",
    "        # 3. 데이터 파싱 (이전에 정의한 함수 사용)\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"🔄 데이터 파싱 중...\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        parsed_data = []\n",
    "        parsed_data_needs_check = []\n",
    "        unparsed_data = []\n",
    "        \n",
    "        for idx, row in df_filtered.iterrows():\n",
    "            parsed_rows, success, needs_check = parse_dimensions_advanced(row)\n",
    "            if success and parsed_rows:\n",
    "                if needs_check:\n",
    "                    parsed_data_needs_check.extend(parsed_rows)\n",
    "                else:\n",
    "                    parsed_data.extend(parsed_rows)\n",
    "            else:\n",
    "                unparsed_data.append(row)\n",
    "        \n",
    "        df_parsed = pd.DataFrame(parsed_data)\n",
    "        df_parsed_needs_check = pd.DataFrame(parsed_data_needs_check)\n",
    "        df_unparsed = pd.DataFrame(unparsed_data)\n",
    "        \n",
    "        # 파싱 통계 출력\n",
    "        total_parsed = len(df_parsed) + len(df_parsed_needs_check)\n",
    "        print(f\"✅ 파싱 성공 (확실): {len(df_parsed)}개 행\")\n",
    "        print(f\"⚠️  파싱 성공 (체크 필요): {len(df_parsed_needs_check)}개 행\")\n",
    "        print(f\"❌ 파싱 실패: {len(df_unparsed)}개 행\")\n",
    "        print(f\"📈 전체 대비 파싱률: {(total_parsed / len(df_filtered) * 100):.1f}%\")\n",
    "        \n",
    "        # 4. PostgreSQL 테이블에 저장\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"💾 데이터 저장 중...\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        success = save_parsed_data_to_table(\n",
    "            engine=engine,\n",
    "            df_parsed=df_parsed,\n",
    "            df_needs_check=df_parsed_needs_check,\n",
    "            source_table_name=SOURCE_TABLE,\n",
    "            target_table_name=TARGET_TABLE,\n",
    "            truncate_before_insert=TRUNCATE_BEFORE_INSERT\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"✅ 전체 작업 완료!\")\n",
    "            print(\"=\"*80)\n",
    "            print(f\"📊 요약:\")\n",
    "            print(f\"  - 소스 테이블: {SOURCE_TABLE}\")\n",
    "            print(f\"  - 타겟 테이블: {TARGET_TABLE}\")\n",
    "            print(f\"  - 저장된 데이터: {total_parsed}개 행\")\n",
    "            print(f\"  - 기존 데이터 삭제: {'예' if TRUNCATE_BEFORE_INSERT else '아니오'}\")\n",
    "            print(f\"  - 타겟 테이블 스키마: 소스 테이블 컬럼 + dimension_type, parsed_value, needs_check\")\n",
    "        else:\n",
    "            print(\"\\n❌ 데이터 저장 실패\")\n",
    "    else:\n",
    "        print(\"\\n❌ 데이터 로드 실패 또는 데이터 없음\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "oqk84734yc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL 데이터 처리 함수 정의 완료\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text, inspect\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "def get_sqlalchemy_engine():\n",
    "    \"\"\"SQLAlchemy 엔진 생성\"\"\"\n",
    "    try:\n",
    "        connection_string = f\"postgresql://{os.getenv('PG_USER')}:{os.getenv('PG_PASSWORD')}@{os.getenv('PG_HOST')}:{os.getenv('PG_PORT')}/{os.getenv('PG_DATABASE')}\"\n",
    "        engine = create_engine(connection_string)\n",
    "        print(f\"✅ SQLAlchemy 엔진 생성 성공\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        print(f\"❌ SQLAlchemy 엔진 생성 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_data_from_table(engine, table_name, allowed_disp_nm1):\n",
    "    \"\"\"\n",
    "    PostgreSQL 테이블에서 데이터 로드\n",
    "    \n",
    "    Parameters:\n",
    "    - engine: SQLAlchemy engine\n",
    "    - table_name: 소스 테이블명\n",
    "    - allowed_disp_nm1: 필터링할 disp_nm1 리스트\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 전체 데이터 로드\n",
    "        query = f\"SELECT * FROM {table_name}\"\n",
    "        df = pd.read_sql(query, engine)\n",
    "        print(f\"✅ 테이블 '{table_name}'에서 {len(df)}개 행 로드 완료\")\n",
    "        \n",
    "        # allowed_disp_nm1로 필터링\n",
    "        if allowed_disp_nm1 and len(allowed_disp_nm1) > 0:\n",
    "            df_filtered = df[df['disp_nm1'].isin(allowed_disp_nm1)]\n",
    "            print(f\"✅ allowed_disp_nm1로 필터링: {len(df_filtered)}개 행\")\n",
    "        else:\n",
    "            df_filtered = df\n",
    "            print(f\"⚠️  필터링 없이 전체 데이터 사용\")\n",
    "        \n",
    "        return df_filtered\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 데이터 로드 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_table_schema(engine, source_table_name):\n",
    "    \"\"\"\n",
    "    소스 테이블의 스키마를 읽어옴\n",
    "    \n",
    "    Parameters:\n",
    "    - engine: SQLAlchemy engine\n",
    "    - source_table_name: 소스 테이블명\n",
    "    \n",
    "    Returns:\n",
    "    - 컬럼 정보 딕셔너리 리스트\n",
    "    \"\"\"\n",
    "    try:\n",
    "        inspector = inspect(engine)\n",
    "        columns = inspector.get_columns(source_table_name)\n",
    "        print(f\"✅ 소스 테이블 '{source_table_name}' 스키마 읽기 완료 ({len(columns)}개 컬럼)\")\n",
    "        return columns\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 스키마 읽기 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def map_sqlalchemy_type_to_postgres(column_type):\n",
    "    \"\"\"\n",
    "    SQLAlchemy 타입을 PostgreSQL 타입으로 변환\n",
    "    \"\"\"\n",
    "    type_str = str(column_type)\n",
    "    \n",
    "    # 일반적인 타입 매핑\n",
    "    if 'INTEGER' in type_str or 'BIGINT' in type_str or 'SMALLINT' in type_str:\n",
    "        return 'INTEGER'\n",
    "    elif 'SERIAL' in type_str or 'BIGSERIAL' in type_str:\n",
    "        return 'SERIAL'\n",
    "    elif 'VARCHAR' in type_str:\n",
    "        # VARCHAR(길이) 추출\n",
    "        return type_str.replace('VARCHAR', 'VARCHAR')\n",
    "    elif 'TEXT' in type_str:\n",
    "        return 'TEXT'\n",
    "    elif 'BOOLEAN' in type_str or 'BOOL' in type_str:\n",
    "        return 'BOOLEAN'\n",
    "    elif 'TIMESTAMP' in type_str:\n",
    "        return 'TIMESTAMP'\n",
    "    elif 'DATE' in type_str:\n",
    "        return 'DATE'\n",
    "    elif 'NUMERIC' in type_str or 'DECIMAL' in type_str:\n",
    "        return type_str.replace('NUMERIC', 'NUMERIC')\n",
    "    elif 'FLOAT' in type_str or 'REAL' in type_str or 'DOUBLE' in type_str:\n",
    "        return 'DOUBLE PRECISION'\n",
    "    else:\n",
    "        # 기본값\n",
    "        return 'TEXT'\n",
    "\n",
    "def create_parsed_table_from_source(engine, source_table_name, target_table_name):\n",
    "    \"\"\"\n",
    "    소스 테이블의 스키마를 기반으로 파싱된 데이터를 저장할 테이블 생성\n",
    "    dimension_type과 parsed_value 컬럼 추가\n",
    "    \n",
    "    Parameters:\n",
    "    - engine: SQLAlchemy engine\n",
    "    - source_table_name: 소스 테이블명\n",
    "    - target_table_name: 생성할 테이블명\n",
    "    \"\"\"\n",
    "    # 소스 테이블 스키마 읽기\n",
    "    columns = get_table_schema(engine, source_table_name)\n",
    "    if columns is None:\n",
    "        return False\n",
    "    \n",
    "    # CREATE TABLE 쿼리 생성\n",
    "    column_definitions = []\n",
    "    \n",
    "    for col in columns:\n",
    "        col_name = col['name']\n",
    "        col_type = map_sqlalchemy_type_to_postgres(col['type'])\n",
    "        nullable = \"NULL\" if col['nullable'] else \"NOT NULL\"\n",
    "        \n",
    "        # PRIMARY KEY나 SERIAL 타입은 제거 (새 테이블에서는 id를 새로 만들 것)\n",
    "        if col.get('autoincrement') or 'primary_key' in str(col).lower():\n",
    "            continue\n",
    "            \n",
    "        column_definitions.append(f\"{col_name} {col_type}\")\n",
    "    \n",
    "    # dimension_type과 parsed_value 추가\n",
    "    column_definitions.append(\"dimension_type TEXT\")\n",
    "    column_definitions.append(\"parsed_value NUMERIC\")\n",
    "    column_definitions.append(\"needs_check BOOLEAN\")\n",
    "    \n",
    "    # CREATE TABLE 쿼리\n",
    "    create_table_query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {target_table_name} (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        {', '.join(column_definitions)},\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(create_table_query))\n",
    "            conn.commit()\n",
    "        print(f\"✅ 테이블 '{target_table_name}' 생성/확인 완료\")\n",
    "        print(f\"   추가된 컬럼: dimension_type, parsed_value, needs_check\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 테이블 생성 실패: {e}\")\n",
    "        return False\n",
    "\n",
    "def truncate_table(engine, table_name):\n",
    "    \"\"\"\n",
    "    테이블의 기존 데이터 삭제\n",
    "    \n",
    "    Parameters:\n",
    "    - engine: SQLAlchemy engine\n",
    "    - table_name: 테이블명\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(f\"TRUNCATE TABLE {table_name} RESTART IDENTITY CASCADE\"))\n",
    "            conn.commit()\n",
    "        print(f\"✅ 테이블 '{table_name}'의 기존 데이터 삭제 완료\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 데이터 삭제 실패: {e}\")\n",
    "        return False\n",
    "\n",
    "def save_parsed_data_to_table(engine, df_parsed, df_needs_check, source_table_name, target_table_name, truncate_before_insert=False):\n",
    "    \"\"\"\n",
    "    파싱된 데이터를 PostgreSQL 테이블에 저장\n",
    "    소스 테이블의 모든 컬럼 + dimension_type, parsed_value, needs_check 저장\n",
    "    \n",
    "    Parameters:\n",
    "    - engine: SQLAlchemy engine\n",
    "    - df_parsed: 파싱 성공한 확실한 데이터\n",
    "    - df_needs_check: 파싱 성공했지만 체크가 필요한 데이터\n",
    "    - source_table_name: 소스 테이블명\n",
    "    - target_table_name: 대상 테이블명\n",
    "    - truncate_before_insert: True이면 기존 데이터 삭제\n",
    "    \n",
    "    Returns:\n",
    "    - 성공 여부\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 테이블 생성\n",
    "        if not create_parsed_table_from_source(engine, source_table_name, target_table_name):\n",
    "            return False\n",
    "        \n",
    "        # 기존 데이터 삭제 옵션\n",
    "        if truncate_before_insert:\n",
    "            if not truncate_table(engine, target_table_name):\n",
    "                return False\n",
    "        \n",
    "        # 두 DataFrame 합치기\n",
    "        df_all = pd.DataFrame()\n",
    "        \n",
    "        if len(df_parsed) > 0:\n",
    "            df_parsed_copy = df_parsed.copy()\n",
    "            df_parsed_copy['needs_check'] = False\n",
    "            df_all = pd.concat([df_all, df_parsed_copy], ignore_index=True)\n",
    "        \n",
    "        if len(df_needs_check) > 0:\n",
    "            df_needs_check_copy = df_needs_check.copy()\n",
    "            df_needs_check_copy['needs_check'] = True\n",
    "            df_all = pd.concat([df_all, df_needs_check_copy], ignore_index=True)\n",
    "        \n",
    "        if len(df_all) == 0:\n",
    "            print(\"⚠️  저장할 데이터가 없습니다.\")\n",
    "            return True\n",
    "        \n",
    "        # dimension_type과 parsed_value가 있는지 확인\n",
    "        if 'dimension_type' not in df_all.columns or 'parsed_value' not in df_all.columns:\n",
    "            print(\"❌ dimension_type 또는 parsed_value 컬럼이 없습니다.\")\n",
    "            return False\n",
    "        \n",
    "        # 소스 테이블의 컬럼 정보 가져오기\n",
    "        source_columns = get_table_schema(engine, source_table_name)\n",
    "        if source_columns is None:\n",
    "            return False\n",
    "        \n",
    "        # 소스 컬럼명 리스트\n",
    "        source_column_names = [col['name'] for col in source_columns if not col.get('autoincrement')]\n",
    "        \n",
    "        # 저장할 DataFrame 구성: 소스의 모든 컬럼 + dimension_type, parsed_value, needs_check\n",
    "        df_to_save = pd.DataFrame()\n",
    "        \n",
    "        # 소스 테이블의 모든 컬럼 복사\n",
    "        for col_name in source_column_names:\n",
    "            if col_name in df_all.columns:\n",
    "                df_to_save[col_name] = df_all[col_name]\n",
    "        \n",
    "        # 새로운 컬럼 추가\n",
    "        df_to_save['dimension_type'] = df_all['dimension_type']\n",
    "        df_to_save['parsed_value'] = df_all['parsed_value']\n",
    "        df_to_save['needs_check'] = df_all['needs_check']\n",
    "        \n",
    "        # 데이터 저장\n",
    "        df_to_save.to_sql(target_table_name, engine, if_exists='append', index=False)\n",
    "        print(f\"✅ 테이블 '{target_table_name}'에 {len(df_to_save)}개 행 저장 완료\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 데이터 저장 실패: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "print(\"PostgreSQL 데이터 처리 함수 정의 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "krftk8luw9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터: 625개 행\n",
      "파싱 중...\n",
      "\n",
      "================================================================================\n",
      "📊 파싱 결과 통계\n",
      "================================================================================\n",
      "✅ 파싱 성공 (확실): 80개 행\n",
      "⚠️  파싱 성공 (체크 필요): 1769개 행\n",
      "❌ 파싱 실패: 5개 행\n",
      "📈 전체 대비 파싱률: 295.8%\n",
      "================================================================================\n",
      "\n",
      "✅ 파싱 성공 데이터 - 확실 (처음 20개):\n",
      "--------------------------------------------------------------------------------\n",
      "   disp_nm1         disp_nm2 dimension_type  parsed_value                                 value\n",
      "0        규격               크기          width         276.0           276(W) x 327(H) x 293(D) mm\n",
      "1        규격               크기         height         327.0           276(W) x 327(H) x 293(D) mm\n",
      "2        규격               크기          depth         293.0           276(W) x 327(H) x 293(D) mm\n",
      "3        규격               크기         height         216.0        216mm(H) x 790mm(W) x 287mm(D)\n",
      "4        규격               크기          width         790.0        216mm(H) x 790mm(W) x 287mm(D)\n",
      "5        규격               크기          depth         287.0        216mm(H) x 790mm(W) x 287mm(D)\n",
      "6        규격               크기          width         820.0          820 x 56 x103.5 mm(가로x높이x깊이)\n",
      "7        규격               크기         height          56.0          820 x 56 x103.5 mm(가로x높이x깊이)\n",
      "8        규격               크기          depth         103.5          820 x 56 x103.5 mm(가로x높이x깊이)\n",
      "9        외관  제품 크기(본체,WxHxD)          depth         165.0                           ∅165 ± 5 mm\n",
      "10       규격               크기          width         399.0        399mm(W) x 905mm(H) x 436mm(D)\n",
      "11       규격               크기         height         905.0        399mm(W) x 905mm(H) x 436mm(D)\n",
      "12       규격               크기          depth         436.0        399mm(W) x 905mm(H) x 436mm(D)\n",
      "13       규격               크기          width          89.0   89 x 210 x 176 mm (가로x높이x깊이, 좌우 각각)\n",
      "14       규격               크기         height         210.0   89 x 210 x 176 mm (가로x높이x깊이, 좌우 각각)\n",
      "15       규격               크기          depth         176.0   89 x 210 x 176 mm (가로x높이x깊이, 좌우 각각)\n",
      "16       규격               크기         height         460.4  460.4mm(H) x 596.9mm(W) x 288.9mm(D)\n",
      "17       규격               크기          width         596.9  460.4mm(H) x 596.9mm(W) x 288.9mm(D)\n",
      "18       규격               크기          depth         288.9  460.4mm(H) x 596.9mm(W) x 288.9mm(D)\n",
      "19       규격               크기         height         460.4  460.4mm(H) x 596.9mm(W) x 288.9mm(D)\n",
      "\n",
      "\n",
      "⚠️  파싱 성공 데이터 - 체크 필요 (단위 명시 없음, 처음 20개):\n",
      "--------------------------------------------------------------------------------\n",
      "   disp_nm1             disp_nm2 dimension_type  parsed_value                   value\n",
      "0        규격         크기(가로X깊이X높이)          width          78.0  78 x 115.7 x 12.8 (mm)\n",
      "1        규격         크기(가로X깊이X높이)         height         115.7  78 x 115.7 x 12.8 (mm)\n",
      "2        규격         크기(가로X깊이X높이)          depth          12.8  78 x 115.7 x 12.8 (mm)\n",
      "3        사양  제품 크기(가로 × 높이 × 깊이)          width         360.0      360 x 783 x 318 mm\n",
      "4        사양  제품 크기(가로 × 높이 × 깊이)         height         783.0      360 x 783 x 318 mm\n",
      "5        사양  제품 크기(가로 × 높이 × 깊이)          depth         318.0      360 x 783 x 318 mm\n",
      "6        사양  제품 크기(가로 × 높이 × 깊이)          width         360.0      360 x 783 x 318 mm\n",
      "7        사양  제품 크기(가로 × 높이 × 깊이)         height         783.0      360 x 783 x 318 mm\n",
      "8        사양  제품 크기(가로 × 높이 × 깊이)          depth         318.0      360 x 783 x 318 mm\n",
      "9        사양  제품 크기(가로 × 높이 × 깊이)          width         349.0      349 x 499 x 236 mm\n",
      "10       사양  제품 크기(가로 × 높이 × 깊이)         height         499.0      349 x 499 x 236 mm\n",
      "11       사양  제품 크기(가로 × 높이 × 깊이)          depth         236.0      349 x 499 x 236 mm\n",
      "12       사양  제품 크기(가로 × 높이 × 깊이)          width         320.0      320 × 545 × 320 mm\n",
      "13       사양  제품 크기(가로 × 높이 × 깊이)         height         545.0      320 × 545 × 320 mm\n",
      "14       사양  제품 크기(가로 × 높이 × 깊이)          depth         320.0      320 × 545 × 320 mm\n",
      "15       사양  제품 크기(가로 × 높이 × 깊이)          width         320.0     320 × 1070 × 320 mm\n",
      "16       사양  제품 크기(가로 × 높이 × 깊이)         height        1070.0     320 × 1070 × 320 mm\n",
      "17       사양  제품 크기(가로 × 높이 × 깊이)          depth         320.0     320 × 1070 × 320 mm\n",
      "18       사양  제품 크기(가로 × 높이 × 깊이)          width         320.0     320 × 1070 × 320 mm\n",
      "19       사양  제품 크기(가로 × 높이 × 깊이)         height        1070.0     320 × 1070 × 320 mm\n",
      "\n",
      "\n",
      "❌ 파싱 실패 데이터 (처음 20개):\n",
      "--------------------------------------------------------------------------------\n",
      "    disp_nm1      disp_nm2                                                               value\n",
      "296       규격            크기  타워 : 320mm x 1769mm(각 7.6kg), 서브우퍼 : 296mm x 400mm x 296mm(14.6kg)\n",
      "727    기본 사양  제품크기 (WxDxH)                                                                 NaN\n",
      "728    기본 사양  제품크기 (WxDxH)                                                                 NaN\n",
      "729    기본 사양  제품크기 (WxDxH)                                                                 NaN\n",
      "730    기본 사양  제품크기 (WxDxH)                                                                 NaN\n",
      "\n",
      "\n",
      "❌ 파싱 실패 패턴 분석 (disp_nm2별 개수):\n",
      "--------------------------------------------------------------------------------\n",
      "disp_nm2\n",
      "제품크기 (WxDxH)    4\n",
      "크기              1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def identify_dimension_type(text):\n",
    "    \"\"\"\n",
    "    텍스트에서 dimension 타입을 식별\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Depth 키워드\n",
    "    if any(keyword in text_lower for keyword in ['두께', '깊이', 'd']):\n",
    "        return 'depth'\n",
    "    # Width 키워드\n",
    "    elif any(keyword in text_lower for keyword in ['가로', '폭', 'w']):\n",
    "        return 'width'\n",
    "    # Height 키워드\n",
    "    elif any(keyword in text_lower for keyword in ['세로', '높이', 'h']):\n",
    "        return 'height'\n",
    "    \n",
    "    return None\n",
    "\n",
    "def parse_dimensions_advanced(row):\n",
    "    \"\"\"\n",
    "    disp_nm2에 따라 value를 파싱하는 함수 (확장 버전)\n",
    "    \"\"\"\n",
    "    parsed_rows = []\n",
    "    value = str(row['value'])\n",
    "    disp_nm2 = str(row['disp_nm2'])\n",
    "    needs_check = False  # 단위가 명확하지 않아 체크가 필요한 경우\n",
    "    \n",
    "    # 패턴 0: W숫자 x D숫자 x H숫자 형식 (예: \"W269 x D375 x H269 mm\")\n",
    "    wdh_pattern = r'([WwHhDd])\\s*(\\d+(?:\\.\\d+)?)'\n",
    "    wdh_matches = re.findall(wdh_pattern, value)\n",
    "    \n",
    "    if len(wdh_matches) >= 2:  # 최소 2개 이상의 dimension이 있는 경우\n",
    "        base_row = row.to_dict()\n",
    "        dimension_map = {'w': 'width', 'h': 'height', 'd': 'depth'}\n",
    "        \n",
    "        for dim_letter, num_val in wdh_matches:\n",
    "            dim_type = dimension_map.get(dim_letter.lower())\n",
    "            if dim_type:\n",
    "                new_row = base_row.copy()\n",
    "                new_row['dimension_type'] = dim_type\n",
    "                new_row['parsed_value'] = float(num_val)\n",
    "                new_row['needs_check'] = False\n",
    "                parsed_rows.append(new_row)\n",
    "        \n",
    "        if parsed_rows:\n",
    "            return parsed_rows, True, False\n",
    "    \n",
    "    # 패턴 1: value에 숫자(W), 숫자(H), 숫자(D)가 명시된 경우 (예: \"276(W) x 327(H) x 293(D) mm\", \"178(W) x 68(H) x 72(D) mm\")\n",
    "    # 다양한 형식 지원: 숫자(W), 숫자mm(W), W:숫자 등\n",
    "    whd_pattern = r'(\\d+(?:\\.\\d+)?)\\s*(?:mm)?\\s*\\(?\\s*([WwHhDd])\\s*\\)?'\n",
    "    whd_matches = re.findall(whd_pattern, value)\n",
    "    \n",
    "    if len(whd_matches) >= 2:  # 최소 2개 이상의 dimension이 있는 경우\n",
    "        base_row = row.to_dict()\n",
    "        dimension_map = {'w': 'width', 'h': 'height', 'd': 'depth'}\n",
    "        \n",
    "        for num_val, dim_letter in whd_matches:\n",
    "            dim_type = dimension_map.get(dim_letter.lower())\n",
    "            if dim_type:\n",
    "                new_row = base_row.copy()\n",
    "                new_row['dimension_type'] = dim_type\n",
    "                new_row['parsed_value'] = float(num_val)\n",
    "                new_row['needs_check'] = False\n",
    "                parsed_rows.append(new_row)\n",
    "        \n",
    "        if parsed_rows:\n",
    "            return parsed_rows, True, False\n",
    "    \n",
    "    # 패턴 2: \"가로x높이x깊이\" 텍스트가 있는 경우 (예: \"820 x 56 x103.5 mm(가로x높이x깊이)\")\n",
    "    if '가로' in value and '높이' in value and '깊이' in value:\n",
    "        nums = re.findall(r'(\\d+(?:\\.\\d+)?)', value)\n",
    "        if len(nums) >= 3:\n",
    "            base_row = row.to_dict()\n",
    "            \n",
    "            # 가로 (width)\n",
    "            row1 = base_row.copy()\n",
    "            row1['dimension_type'] = 'width'\n",
    "            row1['parsed_value'] = float(nums[0])\n",
    "            row1['needs_check'] = False\n",
    "            parsed_rows.append(row1)\n",
    "            \n",
    "            # 높이 (height)\n",
    "            row2 = base_row.copy()\n",
    "            row2['dimension_type'] = 'height'\n",
    "            row2['parsed_value'] = float(nums[1])\n",
    "            row2['needs_check'] = False\n",
    "            parsed_rows.append(row2)\n",
    "            \n",
    "            # 깊이 (depth)\n",
    "            row3 = base_row.copy()\n",
    "            row3['dimension_type'] = 'depth'\n",
    "            row3['parsed_value'] = float(nums[2])\n",
    "            row3['needs_check'] = False\n",
    "            parsed_rows.append(row3)\n",
    "            \n",
    "            return parsed_rows, True, False\n",
    "    \n",
    "    # 패턴 3: WxHxD 형식 (x로 구분, 단위 명시 없음) (예: \"180 x 70 x 72 mm\", \"223 x 96.5 x 94 mm\")\n",
    "    wxhxd_match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*[xX×]\\s*(\\d+(?:\\.\\d+)?)\\s*[xX×]\\s*(\\d+(?:\\.\\d+)?)', value)\n",
    "    if wxhxd_match:\n",
    "        val1, val2, val3 = wxhxd_match.groups()\n",
    "        base_row = row.to_dict()\n",
    "        \n",
    "        # 기본 가정: 가로 x 높이 x 깊이\n",
    "        dimensions = [\n",
    "            ('width', val1),\n",
    "            ('height', val2),\n",
    "            ('depth', val3)\n",
    "        ]\n",
    "        \n",
    "        for dim_type, val in dimensions:\n",
    "            new_row = base_row.copy()\n",
    "            new_row['dimension_type'] = dim_type\n",
    "            new_row['parsed_value'] = float(val)\n",
    "            new_row['needs_check'] = True  # 단위가 명확하지 않음\n",
    "            parsed_rows.append(new_row)\n",
    "        \n",
    "        return parsed_rows, True, True\n",
    "    \n",
    "    # 패턴 4: WxH 형식 (예: \"500x600 mm\")\n",
    "    wxh_match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*[xX×]\\s*(\\d+(?:\\.\\d+)?)', value)\n",
    "    if wxh_match:\n",
    "        val1, val2 = wxh_match.groups()\n",
    "        base_row = row.to_dict()\n",
    "        \n",
    "        # 기본 가정: 가로 x 높이\n",
    "        dimensions = [\n",
    "            ('width', val1),\n",
    "            ('height', val2)\n",
    "        ]\n",
    "        \n",
    "        for dim_type, val in dimensions:\n",
    "            new_row = base_row.copy()\n",
    "            new_row['dimension_type'] = dim_type\n",
    "            new_row['parsed_value'] = float(val)\n",
    "            new_row['needs_check'] = True  # 단위가 명확하지 않음\n",
    "            parsed_rows.append(new_row)\n",
    "        \n",
    "        return parsed_rows, True, True\n",
    "    \n",
    "    # 패턴 5: 단일 값 (disp_nm2에서 dimension 타입 식별)\n",
    "    single_match = re.search(r'(\\d+(?:\\.\\d+)?)', value)\n",
    "    if single_match:\n",
    "        dim_type = identify_dimension_type(disp_nm2)\n",
    "        if dim_type:\n",
    "            base_row = row.to_dict()\n",
    "            base_row['dimension_type'] = dim_type\n",
    "            base_row['parsed_value'] = float(single_match.group(1))\n",
    "            base_row['needs_check'] = False\n",
    "            parsed_rows.append(base_row)\n",
    "            return parsed_rows, True, False\n",
    "    \n",
    "    return parsed_rows, False, False\n",
    "\n",
    "# 필터링된 데이터의 모든 행에 대해 파싱 시도\n",
    "parsed_data = []\n",
    "parsed_data_needs_check = []\n",
    "unparsed_data = []\n",
    "\n",
    "print(f\"전체 데이터: {len(df_filtered)}개 행\")\n",
    "print(\"파싱 중...\\n\")\n",
    "\n",
    "for idx, row in df_filtered.iterrows():\n",
    "    parsed_rows, success, needs_check = parse_dimensions_advanced(row)\n",
    "    if success and parsed_rows:\n",
    "        if needs_check:\n",
    "            parsed_data_needs_check.extend(parsed_rows)\n",
    "        else:\n",
    "            parsed_data.extend(parsed_rows)\n",
    "    else:\n",
    "        unparsed_data.append(row)\n",
    "\n",
    "# 새로운 DataFrame 생성\n",
    "df_parsed = pd.DataFrame(parsed_data)\n",
    "df_parsed_needs_check = pd.DataFrame(parsed_data_needs_check)\n",
    "df_unparsed = pd.DataFrame(unparsed_data)\n",
    "\n",
    "# 결과 통계\n",
    "total_parsed = len(df_parsed) + len(df_parsed_needs_check)\n",
    "total_original_rows = len(df_filtered)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"📊 파싱 결과 통계\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"✅ 파싱 성공 (확실): {len(df_parsed)}개 행\")\n",
    "print(f\"⚠️  파싱 성공 (체크 필요): {len(df_parsed_needs_check)}개 행\")\n",
    "print(f\"❌ 파싱 실패: {len(df_unparsed)}개 행\")\n",
    "print(f\"📈 전체 대비 파싱률: {(total_parsed / total_original_rows * 100):.1f}%\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 파싱 성공한 데이터 출력 (확실한 것)\n",
    "if len(df_parsed) > 0:\n",
    "    print(\"\\n✅ 파싱 성공 데이터 - 확실 (처음 20개):\")\n",
    "    print(\"-\" * 80)\n",
    "    display_cols = ['disp_nm1', 'disp_nm2', 'disp_nm3', 'disp_nm4', 'dimension_type', 'parsed_value', 'value']\n",
    "    available_cols = [col for col in display_cols if col in df_parsed.columns]\n",
    "    print(df_parsed[available_cols].head(20).to_string())\n",
    "else:\n",
    "    print(\"\\n확실하게 파싱된 데이터가 없습니다.\")\n",
    "\n",
    "# 파싱 성공했지만 체크가 필요한 데이터 출력\n",
    "if len(df_parsed_needs_check) > 0:\n",
    "    print(\"\\n\\n⚠️  파싱 성공 데이터 - 체크 필요 (단위 명시 없음, 처음 20개):\")\n",
    "    print(\"-\" * 80)\n",
    "    display_cols = ['disp_nm1', 'disp_nm2', 'disp_nm3', 'disp_nm4', 'dimension_type', 'parsed_value', 'value']\n",
    "    available_cols = [col for col in display_cols if col in df_parsed_needs_check.columns]\n",
    "    print(df_parsed_needs_check[available_cols].head(20).to_string())\n",
    "else:\n",
    "    print(\"\\n체크가 필요한 데이터가 없습니다.\")\n",
    "\n",
    "# 파싱 실패한 데이터 출력\n",
    "if len(df_unparsed) > 0:\n",
    "    print(\"\\n\\n❌ 파싱 실패 데이터 (처음 20개):\")\n",
    "    print(\"-\" * 80)\n",
    "    display_cols = ['disp_nm1', 'disp_nm2', 'disp_nm3', 'disp_nm4', 'value']\n",
    "    available_cols = [col for col in display_cols if col in df_unparsed.columns]\n",
    "    print(df_unparsed[available_cols].head(20).to_string())\n",
    "    \n",
    "    # 파싱 실패 패턴 분석\n",
    "    print(\"\\n\\n❌ 파싱 실패 패턴 분석 (disp_nm2별 개수):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(df_unparsed['disp_nm2'].value_counts().head(10))\n",
    "else:\n",
    "    print(\"\\n\\n모든 데이터가 성공적으로 파싱되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace15ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mdl_code</th>\n",
       "      <th>goods_nm</th>\n",
       "      <th>disp_lv1</th>\n",
       "      <th>disp_lv2</th>\n",
       "      <th>disp_lv3</th>\n",
       "      <th>category_lv1</th>\n",
       "      <th>category_lv2</th>\n",
       "      <th>category_lv3</th>\n",
       "      <th>disp_nm1</th>\n",
       "      <th>disp_nm2</th>\n",
       "      <th>value</th>\n",
       "      <th>is_numeric</th>\n",
       "      <th>symbols</th>\n",
       "      <th>dimension_type</th>\n",
       "      <th>parsed_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VS90F40CRG</td>\n",
       "      <td>Bespoke AI 제트 400W 침구브러시 패키지</td>\n",
       "      <td>리빙가전</td>\n",
       "      <td>청소기</td>\n",
       "      <td>BESPOKE AI 제트</td>\n",
       "      <td>진공청소기</td>\n",
       "      <td>VC STICK</td>\n",
       "      <td>Bespoke Jet AI</td>\n",
       "      <td>외관</td>\n",
       "      <td>제품 크기(본체,WxHxD)</td>\n",
       "      <td>250x970x243 mm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mm</td>\n",
       "      <td>width</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VS90F40CRG</td>\n",
       "      <td>Bespoke AI 제트 400W 침구브러시 패키지</td>\n",
       "      <td>리빙가전</td>\n",
       "      <td>청소기</td>\n",
       "      <td>BESPOKE AI 제트</td>\n",
       "      <td>진공청소기</td>\n",
       "      <td>VC STICK</td>\n",
       "      <td>Bespoke Jet AI</td>\n",
       "      <td>외관</td>\n",
       "      <td>제품 크기(본체,WxHxD)</td>\n",
       "      <td>250x970x243 mm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mm</td>\n",
       "      <td>height</td>\n",
       "      <td>970.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VS90F40CRG</td>\n",
       "      <td>Bespoke AI 제트 400W 침구브러시 패키지</td>\n",
       "      <td>리빙가전</td>\n",
       "      <td>청소기</td>\n",
       "      <td>BESPOKE AI 제트</td>\n",
       "      <td>진공청소기</td>\n",
       "      <td>VC STICK</td>\n",
       "      <td>Bespoke Jet AI</td>\n",
       "      <td>외관</td>\n",
       "      <td>제품 크기(본체,WxHxD)</td>\n",
       "      <td>250x970x243 mm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mm</td>\n",
       "      <td>depth</td>\n",
       "      <td>243.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VS90F40CNG</td>\n",
       "      <td>Bespoke AI 제트 400W 펫브러시 패키지</td>\n",
       "      <td>리빙가전</td>\n",
       "      <td>청소기</td>\n",
       "      <td>BESPOKE AI 제트</td>\n",
       "      <td>진공청소기</td>\n",
       "      <td>VC STICK</td>\n",
       "      <td>Bespoke Jet AI</td>\n",
       "      <td>외관</td>\n",
       "      <td>제품 크기(본체,WxHxD)</td>\n",
       "      <td>250x970x243 mm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mm</td>\n",
       "      <td>width</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VS90F40CNG</td>\n",
       "      <td>Bespoke AI 제트 400W 펫브러시 패키지</td>\n",
       "      <td>리빙가전</td>\n",
       "      <td>청소기</td>\n",
       "      <td>BESPOKE AI 제트</td>\n",
       "      <td>진공청소기</td>\n",
       "      <td>VC STICK</td>\n",
       "      <td>Bespoke Jet AI</td>\n",
       "      <td>외관</td>\n",
       "      <td>제품 크기(본체,WxHxD)</td>\n",
       "      <td>250x970x243 mm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mm</td>\n",
       "      <td>height</td>\n",
       "      <td>970.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mdl_code                      goods_nm disp_lv1 disp_lv2       disp_lv3  \\\n",
       "0  VS90F40CRG  Bespoke AI 제트 400W 침구브러시 패키지     리빙가전      청소기  BESPOKE AI 제트   \n",
       "1  VS90F40CRG  Bespoke AI 제트 400W 침구브러시 패키지     리빙가전      청소기  BESPOKE AI 제트   \n",
       "2  VS90F40CRG  Bespoke AI 제트 400W 침구브러시 패키지     리빙가전      청소기  BESPOKE AI 제트   \n",
       "3  VS90F40CNG   Bespoke AI 제트 400W 펫브러시 패키지     리빙가전      청소기  BESPOKE AI 제트   \n",
       "4  VS90F40CNG   Bespoke AI 제트 400W 펫브러시 패키지     리빙가전      청소기  BESPOKE AI 제트   \n",
       "\n",
       "  category_lv1 category_lv2    category_lv3 disp_nm1         disp_nm2  \\\n",
       "0        진공청소기     VC STICK  Bespoke Jet AI       외관  제품 크기(본체,WxHxD)   \n",
       "1        진공청소기     VC STICK  Bespoke Jet AI       외관  제품 크기(본체,WxHxD)   \n",
       "2        진공청소기     VC STICK  Bespoke Jet AI       외관  제품 크기(본체,WxHxD)   \n",
       "3        진공청소기     VC STICK  Bespoke Jet AI       외관  제품 크기(본체,WxHxD)   \n",
       "4        진공청소기     VC STICK  Bespoke Jet AI       외관  제품 크기(본체,WxHxD)   \n",
       "\n",
       "            value  is_numeric symbols dimension_type  parsed_value  \n",
       "0  250x970x243 mm         NaN      mm          width         250.0  \n",
       "1  250x970x243 mm         NaN      mm         height         970.0  \n",
       "2  250x970x243 mm         NaN      mm          depth         243.0  \n",
       "3  250x970x243 mm         NaN      mm          width         250.0  \n",
       "4  250x970x243 mm         NaN      mm         height         970.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 필요한 라이브러리 임포트\n",
    "import os\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "print(\"라이브러리 임포트 완료\")\n",
    "\n",
    "# 2. PostgreSQL 연결 설정\n",
    "def get_db_connection():\n",
    "    \"\"\"PostgreSQL 데이터베이스 연결\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=os.getenv('PG_HOST'),\n",
    "            port=os.getenv('PG_PORT'),\n",
    "            database=os.getenv('PG_DATABASE'),\n",
    "            user=os.getenv('PG_USER'),\n",
    "            password=os.getenv('PG_PASSWORD')\n",
    "        )\n",
    "        print(f\"✅ PostgreSQL 연결 성공: {os.getenv('PG_HOST')}\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"❌ PostgreSQL 연결 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "# 연결 테스트\n",
    "conn = get_db_connection()\n",
    "if conn:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8h0wlsefcsi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disp_nm2의 unique 값들:\n",
      "['크기' '크기(가로X깊이X높이)' '제품 크기(가로 × 높이 × 깊이)' '크기(세로x가로x두께, mm)'\n",
      " '크기(폭 × 높이 × 깊이)' '제품 크기(본체,WxHxD)' '제품크기 (가로x높이x깊이)' '제품크기'\n",
      " '제품크기 (WxDxH)' '제품크기 (가로x세로x높이)' '크기 (가로x높이x깊이)' '크기(가로x높이x깊이)'\n",
      " '크기(세로x가로x두께)' '크기(W×H×D)' '제품크기 (가로x높이x 깊이)' '크기(가로x깊이x높이)']\n",
      "\n",
      "총 16개의 unique 값\n"
     ]
    }
   ],
   "source": [
    "# allowed_disp_nm1에 있는 값들만 필터링\n",
    "df_filtered = df[df['disp_nm1'].isin(allowed_disp_nm1)]\n",
    "\n",
    "# disp_nm2의 unique 값 출력\n",
    "print(\"disp_nm2의 unique 값들:\")\n",
    "print(df_filtered['disp_nm2'].unique())\n",
    "print(f\"\\n총 {df_filtered['disp_nm2'].nunique()}개의 unique 값\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bzf1cyvjtvb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disp_nm1의 unique 값들:\n",
      "['규격' '사양' '외관 사양' '기본 사양' '외관' '팔레트' '기본사양' '기타제원' '마스터 박스' '유니트 박스'\n",
      " '본체치수' '주요사양' '하단패널' '일체형 청정스테이션' '일반사양']\n",
      "\n",
      "총 15개의 unique 값\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TSV 파일 읽기\n",
    "df = pd.read_csv('/Users/toby/prog/kt/rubicon/data/kt_spec_validation_table_20251021.tsv', sep='\\t')\n",
    "\n",
    "# disp_nm1의 unique 리스트 출력\n",
    "print(\"disp_nm1의 unique 값들:\")\n",
    "print(df['disp_nm1'].unique())\n",
    "print(f\"\\n총 {df['disp_nm1'].nunique()}개의 unique 값\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "062f4abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_disp_nm1 = ['규격','사양','외관 사양','기본 사양','외관','기본사양','본체치수','주요사양','일반사양']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b692a825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6502af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61ebaaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d52612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b6b667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 필요한 라이브러리 임포트\n",
    "import os\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from typing import Dict, List, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "print(\"라이브러리 임포트 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932ff9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. PostgreSQL 연결 설정\n",
    "def get_db_connection():\n",
    "    \"\"\"PostgreSQL 데이터베이스 연결\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=os.getenv('PG_HOST'),\n",
    "            port=os.getenv('PG_PORT'),\n",
    "            database=os.getenv('PG_DATABASE'),\n",
    "            user=os.getenv('PG_USER'),\n",
    "            password=os.getenv('PG_PASSWORD')\n",
    "        )\n",
    "        print(f\"✅ PostgreSQL 연결 성공: {os.getenv('PG_HOST')}\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"❌ PostgreSQL 연결 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "# 연결 테스트\n",
    "conn = get_db_connection()\n",
    "if conn:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Azure OpenAI 클라이언트 설정\n",
    "def get_openai_client():\n",
    "    \"\"\"Azure OpenAI 클라이언트 생성\"\"\"\n",
    "    try:\n",
    "        # 환경 변수 확인\n",
    "        endpoint = os.getenv('ENDPOINT_URL')\n",
    "        api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "        api_version = os.getenv('AZURE_API_VERSION', '2024-02-01')  # 기본값 제공\n",
    "        \n",
    "        if not endpoint:\n",
    "            print(\"❌ ENDPOINT_URL 환경 변수가 설정되지 않았습니다.\")\n",
    "            return None\n",
    "        \n",
    "        if not api_key:\n",
    "            print(\"❌ AZURE_OPENAI_API_KEY 환경 변수가 설정되지 않았습니다.\")\n",
    "            return None\n",
    "        \n",
    "        # API 버전이 없으면 기본값 사용\n",
    "        if not api_version:\n",
    "            api_version = '2024-02-01'\n",
    "            print(f\"⚠️ AZURE_API_VERSION이 설정되지 않아 기본값 사용: {api_version}\")\n",
    "        \n",
    "        print(f\"📋 Azure OpenAI 설정:\")\n",
    "        print(f\"  - Endpoint: {endpoint[:50]}...\")\n",
    "        print(f\"  - API Version: {api_version}\")\n",
    "        print(f\"  - Deployment: {os.getenv('DEPLOYMENT_NAME')}\")\n",
    "        \n",
    "        client = AzureOpenAI(\n",
    "            azure_endpoint=endpoint,\n",
    "            api_key=api_key,\n",
    "            api_version=api_version,\n",
    "        )\n",
    "        print(f\"✅ Azure OpenAI 클라이언트 생성 성공\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Azure OpenAI 클라이언트 생성 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "# 클라이언트 테스트\n",
    "openai_client = get_openai_client()\n",
    "\n",
    "# 간단한 테스트 호출\n",
    "if openai_client:\n",
    "    try:\n",
    "        print(\"\\n🧪 API 연결 테스트...\")\n",
    "        test_response = openai_client.chat.completions.create(\n",
    "            model=os.getenv('DEPLOYMENT_NAME'),\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Say 'Hello' in one word.\"}\n",
    "            ],\n",
    "            max_tokens=10\n",
    "        )\n",
    "        if test_response and test_response.choices:\n",
    "            print(f\"✅ API 테스트 성공: '{test_response.choices[0].message.content}'\")\n",
    "        else:\n",
    "            print(\"⚠️ API 응답이 비어있습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ API 테스트 실패: {e}\")\n",
    "        print(f\"   상세 에러: {type(e).__name__}\")\n",
    "        if hasattr(e, '__dict__'):\n",
    "            print(f\"   에러 속성: {e.__dict__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3c1ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 테이블 스키마 정보 조회\n",
    "def get_table_schema(table_name='test'):\n",
    "    \"\"\"테이블 스키마 정보 조회 (코멘트 포함)\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    if not conn:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # PostgreSQL에서 컬럼 정보와 코멘트를 함께 조회\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            c.column_name,\n",
    "            c.data_type,\n",
    "            c.character_maximum_length,\n",
    "            c.numeric_precision,\n",
    "            c.numeric_scale,\n",
    "            c.is_nullable,\n",
    "            c.column_default,\n",
    "            pgd.description as column_comment\n",
    "        FROM information_schema.columns c\n",
    "        LEFT JOIN pg_catalog.pg_statio_all_tables as st\n",
    "            ON c.table_schema = st.schemaname \n",
    "            AND c.table_name = st.relname\n",
    "        LEFT JOIN pg_catalog.pg_description pgd \n",
    "            ON pgd.objoid = st.relid \n",
    "            AND pgd.objsubid = c.ordinal_position\n",
    "        WHERE c.table_schema = 'public' \n",
    "        AND c.table_name = %s\n",
    "        ORDER BY c.ordinal_position;\n",
    "        \"\"\"\n",
    "        \n",
    "        df_schema = pd.read_sql_query(query, conn, params=(table_name,))\n",
    "        print(f\"✅ 테이블 '{table_name}' 스키마 조회 성공\")\n",
    "        print(f\"   - 컬럼 수: {len(df_schema)}\")\n",
    "        \n",
    "        # 코멘트가 있는 컬럼 수 확인\n",
    "        comment_count = df_schema['column_comment'].notna().sum()\n",
    "        print(f\"   - 코멘트가 있는 컬럼: {comment_count}개\")\n",
    "        \n",
    "        return df_schema\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 테이블 스키마 조회 실패: {e}\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# 스키마 조회\n",
    "df_schema = get_table_schema(table_name=\"kt_merged_product_20250929\")\n",
    "if df_schema is not None:\n",
    "    print(\"\\n테이블 스키마 정보:\")\n",
    "    display(df_schema.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd4856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vc8lj1ptyxi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. kt_merged_product_20250929 테이블 데이터 조회\n",
    "def get_product_data():\n",
    "    \"\"\"kt_merged_product_20250929 테이블에서 특정 컬럼만 조회\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    if not conn:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            product_id,\n",
    "            display_category_major,\n",
    "            display_category_middle,\n",
    "            display_category_minor,\n",
    "            model_code,\n",
    "            model_name,\n",
    "            product_name,\n",
    "            product_specification\n",
    "        FROM kt_merged_product_20250929\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        print(f\"✅ 데이터 조회 성공: {len(df)}개 행\")\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 데이터 조회 실패: {e}\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# 데이터 조회 및 표시\n",
    "df_products = get_product_data()\n",
    "if df_products is not None:\n",
    "    print(\"\\nkt_merged_product_20250929 테이블 데이터 (product_id, model_code, model_name, product_specification):\")\n",
    "    display(df_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53aha0crak",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. product_specification JSON 분석\n",
    "def analyze_product_specifications():\n",
    "    \"\"\"product_specification JSON 필드를 상세 분석\"\"\"\n",
    "    \n",
    "    conn = get_db_connection()\n",
    "    if not conn:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # display_category_middle과 product_specification을 함께 조회\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            display_category_middle,\n",
    "            product_specification\n",
    "        FROM kt_merged_product_20250929\n",
    "        WHERE product_specification IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        print(f\"✅ 분석용 데이터 조회 성공: {len(df)}개 행\\n\")\n",
    "        \n",
    "        # 1. display_category_middle별 key-value 분석\n",
    "        category_key_stats = {}\n",
    "        all_keys = set()\n",
    "        key_value_examples = {}\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            category = row['display_category_middle']\n",
    "            spec = row['product_specification']\n",
    "            \n",
    "            if category not in category_key_stats:\n",
    "                category_key_stats[category] = {\n",
    "                    'keys': set(),\n",
    "                    'key_counts': {},\n",
    "                    'value_examples': {}\n",
    "                }\n",
    "            \n",
    "            try:\n",
    "                if isinstance(spec, str):\n",
    "                    spec_dict = json.loads(spec)\n",
    "                elif isinstance(spec, dict):\n",
    "                    spec_dict = spec\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                for key, value in spec_dict.items():\n",
    "                    all_keys.add(key)\n",
    "                    category_key_stats[category]['keys'].add(key)\n",
    "                    \n",
    "                    # key별 카운트\n",
    "                    if key not in category_key_stats[category]['key_counts']:\n",
    "                        category_key_stats[category]['key_counts'][key] = 0\n",
    "                    category_key_stats[category]['key_counts'][key] += 1\n",
    "                    \n",
    "                    # value 예시 수집 (최대 3개)\n",
    "                    if key not in category_key_stats[category]['value_examples']:\n",
    "                        category_key_stats[category]['value_examples'][key] = set()\n",
    "                    if len(category_key_stats[category]['value_examples'][key]) < 20:\n",
    "                        if value and str(value).strip():\n",
    "                            category_key_stats[category]['value_examples'][key].add(str(value)[:50])\n",
    "                    \n",
    "                    # 전체 key-value 예시\n",
    "                    if key not in key_value_examples:\n",
    "                        key_value_examples[key] = set()\n",
    "                    # if len(key_value_examples[key]) < 5:\n",
    "                    #     if value and str(value).strip():\n",
    "                    #         key_value_examples[key].add(str(value)[:50])\n",
    "                    if value and str(value).strip():\n",
    "                        key_value_examples[key].add(str(value)[:50])\n",
    "                            \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # 2. display_category_middle별 주요 key 출력\n",
    "        print(\"=\" * 80)\n",
    "        print(\"1. display_category_middle별 주요 Key와 Value 예시\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for category in sorted(category_key_stats.keys())[:]:  \n",
    "            stats = category_key_stats[category]\n",
    "            total_products = sum(stats['key_counts'].values()) / len(stats['keys']) if stats['keys'] else 0\n",
    "            \n",
    "            print(f\"\\n📁 {category}\")\n",
    "            print(f\"   총 제품 수: ~{int(total_products)}\")\n",
    "            print(f\"   고유 key 수: {len(stats['keys'])}\")\n",
    "            \n",
    "            top_keys = sorted(stats['key_counts'].items(), key=lambda x: x[1], reverse=True)[:]\n",
    "            for key, count in top_keys:\n",
    "                examples = list(stats['value_examples'].get(key, []))[:2]\n",
    "                examples_str = \", \".join([f\"'{ex}'\" for ex in examples])\n",
    "                print(f\"   • {key}: {count}회 (예: {examples_str})\")\n",
    "        \n",
    "        # 3. 특정 key가 어떤 display_category_middle에 속하는지 분석\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"2. 주요 Key별 소속 Category 분석\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        key_to_categories = {}\n",
    "        for category, stats in category_key_stats.items():\n",
    "            for key in stats['keys']:\n",
    "                if key not in key_to_categories:\n",
    "                    key_to_categories[key] = set()\n",
    "                key_to_categories[key].add(category)\n",
    "        \n",
    "        # 여러 카테고리에 걸쳐 있는 key들\n",
    "        cross_category_keys = {k: v for k, v in key_to_categories.items() if len(v) > 1}\n",
    "        sorted_keys = sorted(cross_category_keys.items(), key=lambda x: len(x[1]), reverse=True)[:]\n",
    "        \n",
    "        print(\"\\n📊 여러 카테고리에 걸쳐 있는 Key (상위 10개):\")\n",
    "        for key, categories in sorted_keys:\n",
    "            print(f\"   • {key}: {len(categories)}개 카테고리\")\n",
    "            print(f\"     → {', '.join(list(categories)[:])}\")\n",
    "        \n",
    "        # 단일 카테고리에만 있는 특화된 key들\n",
    "        single_category_keys = {k: v for k, v in key_to_categories.items() if len(v) == 1}\n",
    "        \n",
    "        print(\"\\n📌 특정 카테고리 전용 Key 예시:\")\n",
    "        category_specific = {}\n",
    "        for key, categories in single_category_keys.items():\n",
    "            cat = list(categories)[0]\n",
    "            if cat not in category_specific:\n",
    "                category_specific[cat] = []\n",
    "            category_specific[cat].append(key)\n",
    "        \n",
    "        for cat, keys in list(category_specific.items())[:5]:\n",
    "            print(f\"   • {cat}: {', '.join(keys[:3])}\")\n",
    "        \n",
    "        # 4. 전체 통계\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"3. 전체 JSON Key-Value 통계\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # key별 전체 사용 횟수\n",
    "        total_key_counts = {}\n",
    "        for stats in category_key_stats.values():\n",
    "            for key, count in stats['key_counts'].items():\n",
    "                if key not in total_key_counts:\n",
    "                    total_key_counts[key] = 0\n",
    "                total_key_counts[key] += count\n",
    "        \n",
    "        print(f\"\\n📈 전체 통계:\")\n",
    "        print(f\"   • 총 고유 key 수: {len(all_keys)}\")\n",
    "        print(f\"   • 총 카테고리 수: {len(category_key_stats)}\")\n",
    "        print(f\"   • 분석된 제품 수: {len(df)}\")\n",
    "        \n",
    "        print(f\"\\n📊 가장 많이 사용되는 Key :\")\n",
    "        top_overall_keys = sorted(total_key_counts.items(), key=lambda x: x[1], reverse=True)[:]\n",
    "        for key, count in top_overall_keys:\n",
    "            percentage = (count / len(df)) * 100\n",
    "            examples = list(key_value_examples.get(key, []))[:2]\n",
    "            examples_str = \", \".join([f\"'{ex}'\" for ex in examples])\n",
    "            print(f\"   {key:30s} : {count:5d}회 ({percentage:5.1f}%) | 예: {examples_str}\")\n",
    "        \n",
    "        # 5. 데이터 타입 분석\n",
    "        print(f\"\\n📝 Value 데이터 타입 패턴:\")\n",
    "        type_patterns = {\n",
    "            '숫자형': [],\n",
    "            '불린형': [],\n",
    "            '텍스트형': [],\n",
    "            '단위포함': [],\n",
    "            '리스트형': []\n",
    "        }\n",
    "        \n",
    "        for key, examples in key_value_examples.items():\n",
    "            sample = list(examples)[0] if examples else \"\"\n",
    "            if sample:\n",
    "                if sample.lower() in ['true', 'false', '예', '아니오']:\n",
    "                    type_patterns['불린형'].append(key)\n",
    "                elif any(unit in sample for unit in ['GB', 'MB', 'mm', 'cm', 'kg', 'W', 'Hz', 'mAh']):\n",
    "                    type_patterns['단위포함'].append(key)\n",
    "                elif sample.replace('.', '').replace('-', '').isdigit():\n",
    "                    type_patterns['숫자형'].append(key)\n",
    "                elif ',' in sample or '|' in sample:\n",
    "                    type_patterns['리스트형'].append(key)\n",
    "                else:\n",
    "                    type_patterns['텍스트형'].append(key)\n",
    "        \n",
    "        for pattern_type, keys in type_patterns.items():\n",
    "            if keys:\n",
    "                print(f\"   • {pattern_type}: {', '.join(keys[:5])}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 분석 실패: {e}\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# 분석 실행\n",
    "df_spec_analysis = analyze_product_specifications()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zl7hztfer9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. product_specification 고유 key 추출 및 유사 key 분석\n",
    "def analyze_similar_keys():\n",
    "    \"\"\"product_specification의 모든 고유 key를 추출하고 유사한 key 쌍을 찾기\"\"\"\n",
    "    \n",
    "    conn = get_db_connection()\n",
    "    if not conn:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # 모든 product_specification 데이터 가져오기\n",
    "        query = \"\"\"\n",
    "        SELECT product_specification\n",
    "        FROM kt_merged_product_20250929\n",
    "        WHERE product_specification IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        print(f\"✅ 데이터 조회 성공: {len(df)}개 행\\n\")\n",
    "        \n",
    "        # 모든 고유 key 추출\n",
    "        all_keys = set()\n",
    "        for spec in df['product_specification']:\n",
    "            try:\n",
    "                if isinstance(spec, str):\n",
    "                    spec_dict = json.loads(spec)\n",
    "                elif isinstance(spec, dict):\n",
    "                    spec_dict = spec\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                all_keys.update(spec_dict.keys())\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # 정렬된 key 리스트\n",
    "        sorted_keys = sorted(list(all_keys))\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(f\"1. 전체 고유 Key 목록 (총 {len(sorted_keys)}개)\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # 알파벳/한글별로 그룹화\n",
    "        english_keys = [k for k in sorted_keys if k and k[0].isascii()]\n",
    "        korean_keys = [k for k in sorted_keys if k and not k[0].isascii()]\n",
    "        \n",
    "        print(\"\\n📌 영문 Key (한 줄씩):\")\n",
    "        print(\"-\" * 40)\n",
    "        for key in english_keys:\n",
    "            print(key)\n",
    "        \n",
    "        print(\"\\n📌 한글 Key (한 줄씩):\")\n",
    "        print(\"-\" * 40)\n",
    "        for key in korean_keys:\n",
    "            print(key)\n",
    "        \n",
    "        # OpenAI를 사용한 유사 key 분석 (여러 번 호출)\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"2. OpenAI를 활용한 유사 Key 상세 분석\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # OpenAI 클라이언트 가져오기\n",
    "        openai_client = get_openai_client()\n",
    "        \n",
    "        all_similar_pairs = []\n",
    "        \n",
    "        if openai_client:\n",
    "            try:\n",
    "                # key를 배치로 나누어 분석 (한 번에 너무 많으면 API가 제대로 분석 못함)\n",
    "                batch_size = 50\n",
    "                for i in range(0, len(sorted_keys), batch_size):\n",
    "                    batch_keys = sorted_keys[i:i+batch_size]\n",
    "                    keys_str = ', '.join(batch_keys)\n",
    "                    \n",
    "                    print(f\"\\n🤖 배치 {i//batch_size + 1}/{(len(sorted_keys)-1)//batch_size + 1} 분석 중...\")\n",
    "                    \n",
    "                    prompt = f\"\"\"다음은 제품 사양을 나타내는 JSON key 목록입니다:\n",
    "{keys_str}\n",
    "\n",
    "위 key들 중에서 의미가 유사하거나 중복되는 key 쌍들을 모두 찾아주세요.\n",
    "예를 들어:\n",
    "- 같은 속성을 다른 이름으로 표현한 경우\n",
    "- 한글과 영문으로 중복된 경우\n",
    "- 약어와 전체 이름이 함께 있는 경우\n",
    "- 대소문자만 다른 경우\n",
    "- 띄어쓰기나 언더스코어 차이만 있는 경우\n",
    "\n",
    "가능한 모든 유사 쌍을 찾아서 JSON 형식으로 응답해주세요:\n",
    "{{\n",
    "  \"similar_pairs\": [\n",
    "    {{\"key1\": \"첫번째key\", \"key2\": \"두번째key\", \"reason\": \"유사한 이유\"}},\n",
    "    ...\n",
    "  ]\n",
    "}}\n",
    "\"\"\"\n",
    "                    \n",
    "                    response = openai_client.chat.completions.create(\n",
    "                        model=os.getenv('DEPLOYMENT_NAME'),\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"You are a data analyst expert in finding similar or duplicate fields in datasets. Find ALL possible similar pairs. Respond in Korean.\"},\n",
    "                            {\"role\": \"user\", \"content\": prompt}\n",
    "                        ],\n",
    "                        max_tokens=3000,\n",
    "                        temperature=0.2\n",
    "                    )\n",
    "                    \n",
    "                    if response and response.choices:\n",
    "                        result_text = response.choices[0].message.content\n",
    "                        \n",
    "                        # JSON 파싱 시도\n",
    "                        try:\n",
    "                            if '```json' in result_text:\n",
    "                                json_str = result_text.split('```json')[1].split('```')[0]\n",
    "                            elif '{' in result_text:\n",
    "                                start = result_text.index('{')\n",
    "                                end = result_text.rindex('}') + 1\n",
    "                                json_str = result_text[start:end]\n",
    "                            else:\n",
    "                                json_str = result_text\n",
    "                                \n",
    "                            similar_data = json.loads(json_str)\n",
    "                            \n",
    "                            if 'similar_pairs' in similar_data:\n",
    "                                all_similar_pairs.extend(similar_data['similar_pairs'])\n",
    "                                \n",
    "                        except json.JSONDecodeError:\n",
    "                            pass\n",
    "                \n",
    "                # 중복 제거 및 출력\n",
    "                print(\"\\n📊 발견된 모든 유사 Key 쌍 (상세):\")\n",
    "                print(\"-\" * 60)\n",
    "                \n",
    "                seen_pairs = set()\n",
    "                for idx, pair in enumerate(all_similar_pairs, 1):\n",
    "                    key1, key2 = pair.get('key1', ''), pair.get('key2', '')\n",
    "                    pair_tuple = tuple(sorted([key1, key2]))\n",
    "                    \n",
    "                    if pair_tuple not in seen_pairs:\n",
    "                        seen_pairs.add(pair_tuple)\n",
    "                        print(f\"\\n{idx}. 유사 쌍:\")\n",
    "                        print(f\"   Key 1: {key1}\")\n",
    "                        print(f\"   Key 2: {key2}\")\n",
    "                        print(f\"   이유: {pair.get('reason', '')}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"❌ OpenAI API 호출 실패: {e}\")\n",
    "        else:\n",
    "            print(\"⚠️ OpenAI 클라이언트를 사용할 수 없습니다.\")\n",
    "        \n",
    "        # 상세한 규칙 기반 유사성 분석\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"3. 규칙 기반 유사 Key 상세 탐지\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        similar_groups = {\n",
    "            '색상 관련': [],\n",
    "            '크기/용량 관련': [],\n",
    "            '무게 관련': [],\n",
    "            '화면 관련': [],\n",
    "            '배터리 관련': [],\n",
    "            '메모리 관련': [],\n",
    "            '카메라 관련': [],\n",
    "            '네트워크/통신 관련': [],\n",
    "            '프로세서/성능 관련': [],\n",
    "            '오디오/사운드 관련': [],\n",
    "            '연결/포트 관련': [],\n",
    "            '전원 관련': [],\n",
    "            '센서 관련': [],\n",
    "            '보안 관련': [],\n",
    "            '기타 기능': []\n",
    "        }\n",
    "        \n",
    "        for key in sorted_keys:\n",
    "            key_lower = key.lower()\n",
    "            \n",
    "            # 색상 관련\n",
    "            if any(word in key_lower for word in ['color', '색상', '컬러', 'colour']):\n",
    "                similar_groups['색상 관련'].append(key)\n",
    "            \n",
    "            # 크기/용량 관련\n",
    "            if any(word in key_lower for word in ['size', '크기', '사이즈', '용량', 'capacity', 'dimension', '치수', 'volume']):\n",
    "                similar_groups['크기/용량 관련'].append(key)\n",
    "            \n",
    "            # 무게 관련\n",
    "            if any(word in key_lower for word in ['weight', '무게', '중량', 'mass']):\n",
    "                similar_groups['무게 관련'].append(key)\n",
    "            \n",
    "            # 화면 관련\n",
    "            if any(word in key_lower for word in ['display', '화면', '디스플레이', 'screen', 'panel', '패널', 'lcd', 'oled', 'resolution', '해상도']):\n",
    "                similar_groups['화면 관련'].append(key)\n",
    "            \n",
    "            # 배터리 관련\n",
    "            if any(word in key_lower for word in ['battery', '배터리', '전지', 'charge', '충전', 'power bank']):\n",
    "                similar_groups['배터리 관련'].append(key)\n",
    "            \n",
    "            # 메모리 관련\n",
    "            if any(word in key_lower for word in ['memory', '메모리', 'ram', 'storage', '저장', 'rom', 'ssd', 'hdd']):\n",
    "                similar_groups['메모리 관련'].append(key)\n",
    "            \n",
    "            # 카메라 관련\n",
    "            if any(word in key_lower for word in ['camera', '카메라', '촬영', '렌즈', 'lens', 'photo', '사진', 'video', '동영상']):\n",
    "                similar_groups['카메라 관련'].append(key)\n",
    "            \n",
    "            # 네트워크/통신 관련\n",
    "            if any(word in key_lower for word in ['network', '네트워크', 'wifi', 'wi-fi', 'bluetooth', '블루투스', 'lte', '5g', '4g', 'cellular']):\n",
    "                similar_groups['네트워크/통신 관련'].append(key)\n",
    "            \n",
    "            # 프로세서/성능 관련\n",
    "            if any(word in key_lower for word in ['processor', '프로세서', 'cpu', 'gpu', 'chip', '칩', 'performance', '성능']):\n",
    "                similar_groups['프로세서/성능 관련'].append(key)\n",
    "            \n",
    "            # 오디오/사운드 관련\n",
    "            if any(word in key_lower for word in ['audio', '오디오', 'sound', '사운드', 'speaker', '스피커', 'microphone', '마이크']):\n",
    "                similar_groups['오디오/사운드 관련'].append(key)\n",
    "            \n",
    "            # 연결/포트 관련\n",
    "            if any(word in key_lower for word in ['port', '포트', 'usb', 'hdmi', 'connector', '연결', 'jack', '잭']):\n",
    "                similar_groups['연결/포트 관련'].append(key)\n",
    "            \n",
    "            # 전원 관련\n",
    "            if any(word in key_lower for word in ['power', '전원', 'voltage', '전압', 'adapter', '어댑터', 'charger']):\n",
    "                similar_groups['전원 관련'].append(key)\n",
    "            \n",
    "            # 센서 관련\n",
    "            if any(word in key_lower for word in ['sensor', '센서', 'gyro', '자이로', 'accelerometer', '가속도계']):\n",
    "                similar_groups['센서 관련'].append(key)\n",
    "            \n",
    "            # 보안 관련\n",
    "            if any(word in key_lower for word in ['security', '보안', 'fingerprint', '지문', 'face', '얼굴', 'lock', '잠금']):\n",
    "                similar_groups['보안 관련'].append(key)\n",
    "        \n",
    "        print(\"\\n📌 주제별 유사 Key 그룹 (전체 목록):\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for group_name, keys in similar_groups.items():\n",
    "            if keys:\n",
    "                print(f\"\\n【{group_name}】 총 {len(keys)}개\")\n",
    "                print(\"-\" * 40)\n",
    "                for key in sorted(keys):\n",
    "                    print(key)\n",
    "        \n",
    "        # 대소문자/언더스코어 차이만 있는 key 찾기\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"4. 형식만 다른 중복 Key 탐지\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        normalized_keys = {}\n",
    "        for key in sorted_keys:\n",
    "            # 정규화: 소문자로 변환, 언더스코어/하이픈/공백 제거\n",
    "            normalized = key.lower().replace('_', '').replace('-', '').replace(' ', '')\n",
    "            if normalized not in normalized_keys:\n",
    "                normalized_keys[normalized] = []\n",
    "            normalized_keys[normalized].append(key)\n",
    "        \n",
    "        print(\"\\n📌 형식만 다른 중복 가능 Key:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        duplicate_count = 0\n",
    "        for normalized, original_keys in normalized_keys.items():\n",
    "            if len(original_keys) > 1:\n",
    "                duplicate_count += 1\n",
    "                print(f\"\\n중복 그룹 {duplicate_count}:\")\n",
    "                for key in original_keys:\n",
    "                    print(f\"  - {key}\")\n",
    "        \n",
    "        if duplicate_count == 0:\n",
    "            print(\"형식만 다른 중복 key가 발견되지 않았습니다.\")\n",
    "        \n",
    "        return sorted_keys\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 분석 실패: {e}\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# 분석 실행\n",
    "unique_keys = analyze_similar_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c92a9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec125c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. product_specification 계층 구조 분석\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def get_json_depth(obj, current_depth=0):\n",
    "    \"\"\"JSON 객체의 최대 깊이를 재귀적으로 계산\"\"\"\n",
    "    if not isinstance(obj, (dict, list)):\n",
    "        return current_depth\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        if not obj:\n",
    "            return current_depth\n",
    "        return max(get_json_depth(v, current_depth + 1) for v in obj.values())\n",
    "    \n",
    "    if isinstance(obj, list):\n",
    "        if not obj:\n",
    "            return current_depth\n",
    "        return max(get_json_depth(item, current_depth) for item in obj)\n",
    "\n",
    "def flatten_json_hierarchy(json_obj, max_depth=5):\n",
    "    \"\"\"JSON을 계층별 key-value 쌍으로 변환\"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    # 각 계층 초기화\n",
    "    for i in range(1, max_depth + 1):\n",
    "        result[f'level_{i}_key'] = None\n",
    "        result[f'level_{i}_value'] = None\n",
    "    \n",
    "    def traverse(obj, path=[], depth=1):\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "            \n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                if depth == 1:\n",
    "                    result[f'level_{depth}_key'] = key\n",
    "                    \n",
    "                    # value가 단순 타입인 경우\n",
    "                    if not isinstance(value, (dict, list)):\n",
    "                        result[f'level_{depth}_value'] = str(value) if value is not None else None\n",
    "                    # value가 dict인 경우 하위 계층 탐색\n",
    "                    elif isinstance(value, dict):\n",
    "                        result[f'level_{depth}_value'] = 'object'\n",
    "                        traverse(value, path + [key], depth + 1)\n",
    "                    # value가 list인 경우\n",
    "                    elif isinstance(value, list):\n",
    "                        result[f'level_{depth}_value'] = f'array[{len(value)}]'\n",
    "                        # 첫 번째 요소만 탐색\n",
    "                        if value and isinstance(value[0], dict):\n",
    "                            traverse(value[0], path + [key], depth + 1)\n",
    "    \n",
    "    if isinstance(json_obj, dict):\n",
    "        traverse(json_obj)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# DB에서 데이터 로드\n",
    "conn = get_db_connection()\n",
    "if conn:\n",
    "    try:\n",
    "        # product_specification이 not null인 데이터만 조회\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            product_id,\n",
    "            display_category_major,\n",
    "            display_category_middle,\n",
    "            display_category_minor,\n",
    "            product_specification\n",
    "        FROM kt_merged_product_20250929\n",
    "        WHERE product_specification IS NOT NULL\n",
    "        LIMIT 10000\n",
    "        \"\"\"\n",
    "        \n",
    "        df_products = pd.read_sql_query(query, conn)\n",
    "        print(f\"📊 데이터 로드 완료: {len(df_products)}개 행\")\n",
    "        \n",
    "        # JSON 파싱 및 최대 깊이 계산\n",
    "        max_depth = 0\n",
    "        valid_specs = []\n",
    "        \n",
    "        for idx, row in df_products.iterrows():\n",
    "            try:\n",
    "                spec_str = row['product_specification']\n",
    "                if spec_str:\n",
    "                    # JSON 파싱\n",
    "                    if isinstance(spec_str, str):\n",
    "                        spec_json = json.loads(spec_str)\n",
    "                    else:\n",
    "                        spec_json = spec_str\n",
    "                    \n",
    "                    # 깊이 계산\n",
    "                    depth = get_json_depth(spec_json)\n",
    "                    max_depth = max(max_depth, depth)\n",
    "                    \n",
    "                    valid_specs.append((idx, spec_json))\n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ 행 {idx} JSON 파싱 실패: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n📏 최대 JSON 계층 깊이: {max_depth}\")\n",
    "        print(f\"✅ 유효한 JSON 데이터: {len(valid_specs)}개\")\n",
    "        \n",
    "        # 계층별로 데이터프레임 생성\n",
    "        rows_data = []\n",
    "        \n",
    "        for idx, spec_json in valid_specs[:1000]:  # 시범적으로 1000개만 처리\n",
    "            row = df_products.iloc[idx]\n",
    "            \n",
    "            # 기본 정보\n",
    "            row_dict = {\n",
    "                'product_id': row['product_id'],\n",
    "                'display_category_major': row['display_category_major'],\n",
    "                'display_category_middle': row['display_category_middle'],\n",
    "                'display_category_minor': row['display_category_minor']\n",
    "            }\n",
    "            \n",
    "            # JSON 계층 정보 추가\n",
    "            hierarchy = flatten_json_hierarchy(spec_json, max_depth)\n",
    "            row_dict.update(hierarchy)\n",
    "            \n",
    "            rows_data.append(row_dict)\n",
    "        \n",
    "        # 최종 데이터프레임 생성\n",
    "        df_hierarchy = pd.DataFrame(rows_data)\n",
    "        \n",
    "        print(f\"\\n📋 계층 구조 데이터프레임 생성 완료\")\n",
    "        print(f\"   - 행 수: {len(df_hierarchy)}\")\n",
    "        print(f\"   - 컬럼 수: {len(df_hierarchy.columns)}\")\n",
    "        print(f\"\\n컬럼 목록:\")\n",
    "        for col in df_hierarchy.columns:\n",
    "            print(f\"   - {col}\")\n",
    "        \n",
    "        # 첫 5개 행 출력\n",
    "        print(\"\\n🔍 데이터 샘플 (첫 5개 행):\")\n",
    "        display(df_hierarchy.tail(10))\n",
    "        \n",
    "        # 각 계층별 고유 key 분석\n",
    "        print(\"\\n📊 각 계층별 고유 key 분석:\")\n",
    "        for i in range(1, max_depth + 1):\n",
    "            key_col = f'level_{i}_key'\n",
    "            if key_col in df_hierarchy.columns:\n",
    "                unique_keys = df_hierarchy[key_col].dropna().unique()\n",
    "                print(f\"\\n   Level {i}:\")\n",
    "                print(f\"   - 고유 key 개수: {len(unique_keys)}\")\n",
    "                if len(unique_keys) <= 20:\n",
    "                    print(f\"   - Keys: {sorted(unique_keys)}\")\n",
    "                else:\n",
    "                    print(f\"   - Top 10 Keys: {sorted(unique_keys)[:10]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류 발생: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        conn.close()\n",
    "else:\n",
    "    print(\"❌ DB 연결 실패\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7f18cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. product_specification 계층 구조 상세 분석 (개선 버전)\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "\n",
    "def extract_nested_values(obj, parent_keys=None, results=None):\n",
    "    \"\"\"\n",
    "    중첩된 JSON 객체에서 모든 leaf 값들을 추출\n",
    "    parent_keys: 상위 key들의 리스트\n",
    "    results: 결과를 저장할 리스트\n",
    "    \"\"\"\n",
    "    if parent_keys is None:\n",
    "        parent_keys = []\n",
    "    if results is None:\n",
    "        results = []\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        for key, value in obj.items():\n",
    "            current_path = parent_keys + [key]\n",
    "            \n",
    "            if isinstance(value, dict):\n",
    "                # 딕셔너리인 경우 재귀 호출\n",
    "                extract_nested_values(value, current_path, results)\n",
    "            elif isinstance(value, list):\n",
    "                # 리스트인 경우\n",
    "                if len(value) > 0:\n",
    "                    # 리스트의 첫 번째 요소만 처리\n",
    "                    first_item = value[0]\n",
    "                    if isinstance(first_item, dict):\n",
    "                        # 딕셔너리가 포함된 리스트인 경우\n",
    "                        extract_nested_values(first_item, current_path, results)\n",
    "                    else:\n",
    "                        # 단순 값의 리스트인 경우\n",
    "                        results.append({\n",
    "                            'level': len(current_path),\n",
    "                            'key': '; '.join(current_path),\n",
    "                            'value': str(first_item)\n",
    "                        })\n",
    "                else:\n",
    "                    # 빈 리스트\n",
    "                    results.append({\n",
    "                        'level': len(current_path),\n",
    "                        'key': '; '.join(current_path),\n",
    "                        'value': None\n",
    "                    })\n",
    "            else:\n",
    "                # 단순 값인 경우 (leaf node)\n",
    "                results.append({\n",
    "                    'level': len(current_path),\n",
    "                    'key': '; '.join(current_path),\n",
    "                    'value': str(value) if value is not None else None\n",
    "                })\n",
    "    elif isinstance(obj, list):\n",
    "        # 최상위가 리스트인 경우\n",
    "        if len(obj) > 0 and isinstance(obj[0], dict):\n",
    "            extract_nested_values(obj[0], parent_keys, results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_hierarchical_dataframe(df_products, max_rows=None):\n",
    "    \"\"\"제품별로 계층 구조화된 데이터프레임 생성\"\"\"\n",
    "    all_rows = []\n",
    "    \n",
    "    # 처리할 행 수 제한\n",
    "    rows_to_process = df_products if max_rows is None else df_products.head(max_rows)\n",
    "    \n",
    "    for idx, row in rows_to_process.iterrows():\n",
    "        try:\n",
    "            spec_str = row['product_specification']\n",
    "            if pd.notna(spec_str):\n",
    "                # JSON 파싱\n",
    "                if isinstance(spec_str, str):\n",
    "                    spec_json = json.loads(spec_str)\n",
    "                else:\n",
    "                    spec_json = spec_str\n",
    "                \n",
    "                # 중첩된 모든 값 추출\n",
    "                nested_values = extract_nested_values(spec_json)\n",
    "                \n",
    "                # 각 추출된 값에 대해 행 생성\n",
    "                for item in nested_values:\n",
    "                    row_data = {\n",
    "                        'product_id': row['product_id'],\n",
    "                        'display_category_major': row['display_category_major'],\n",
    "                        'display_category_middle': row['display_category_middle'],\n",
    "                        'display_category_minor': row['display_category_minor'],\n",
    "                        'level': item['level'],\n",
    "                        'key': item['key'],\n",
    "                        'value': item['value']\n",
    "                    }\n",
    "                    all_rows.append(row_data)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ 행 {idx} 처리 실패: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # 데이터프레임 생성 및 정렬\n",
    "    df_result = pd.DataFrame(all_rows)\n",
    "    if not df_result.empty:\n",
    "        df_result = df_result.sort_values(['product_id', 'key'])\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "def get_max_depth(obj, current_depth=0):\n",
    "    \"\"\"JSON 객체의 최대 깊이 계산\"\"\"\n",
    "    if not isinstance(obj, (dict, list)):\n",
    "        return current_depth\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        if not obj:\n",
    "            return current_depth\n",
    "        return max(get_max_depth(v, current_depth + 1) for v in obj.values())\n",
    "    \n",
    "    if isinstance(obj, list):\n",
    "        if not obj:\n",
    "            return current_depth\n",
    "        return max(get_max_depth(item, current_depth) for item in obj)\n",
    "\n",
    "# DB에서 데이터 로드 및 분석 실행\n",
    "conn = get_db_connection()\n",
    "if conn:\n",
    "    try:\n",
    "        # product_specification이 not null인 데이터만 조회\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            product_id,\n",
    "            display_category_major,\n",
    "            display_category_middle,\n",
    "            display_category_minor,\n",
    "            product_specification\n",
    "        FROM kt_merged_product_20250929\n",
    "        WHERE product_specification IS NOT NULL\n",
    "        LIMIT 5000\n",
    "        \"\"\"\n",
    "        \n",
    "        df_products = pd.read_sql_query(query, conn)\n",
    "        print(f\"📊 데이터 로드 완료: {len(df_products)}개 행\")\n",
    "        \n",
    "        # 최대 계층 깊이 분석\n",
    "        print(\"\\n⏳ JSON 계층 깊이 분석 중...\")\n",
    "        max_depth = 0\n",
    "        depth_distribution = {}\n",
    "        \n",
    "        for spec in df_products['product_specification']:\n",
    "            try:\n",
    "                if isinstance(spec, str):\n",
    "                    spec_json = json.loads(spec)\n",
    "                else:\n",
    "                    spec_json = spec\n",
    "                \n",
    "                depth = get_max_depth(spec_json)\n",
    "                max_depth = max(max_depth, depth)\n",
    "                \n",
    "                if depth not in depth_distribution:\n",
    "                    depth_distribution[depth] = 0\n",
    "                depth_distribution[depth] += 1\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n📏 계층 깊이 분석 결과:\")\n",
    "        print(f\"   - 최대 계층 깊이: {max_depth}\")\n",
    "        for depth, count in sorted(depth_distribution.items()):\n",
    "            print(f\"   - 깊이 {depth}: {count}개 제품\")\n",
    "        \n",
    "        # 계층 구조 데이터프레임 생성\n",
    "        print(\"\\n⏳ 계층 구조 데이터프레임 생성 중...\")\n",
    "        df_hierarchical = create_hierarchical_dataframe(df_products, max_rows=1000)\n",
    "        \n",
    "        print(f\"\\n✅ 분석 완료!\")\n",
    "        print(f\"   - 총 레코드 수: {len(df_hierarchical):,}\")\n",
    "        print(f\"   - 고유 제품 수: {df_hierarchical['product_id'].nunique():,}\")\n",
    "        print(f\"   - 고유 key 수: {df_hierarchical['key'].nunique():,}\")\n",
    "        \n",
    "        # 레벨별 통계\n",
    "        print(f\"\\n📊 레벨별 데이터 분포:\")\n",
    "        level_stats = df_hierarchical['level'].value_counts().sort_index()\n",
    "        for level, count in level_stats.items():\n",
    "            print(f\"   - Level {level}: {count:,}개 레코드\")\n",
    "        \n",
    "        # 샘플 데이터 표시\n",
    "        print(\"\\n📋 샘플 데이터 (처음 30개 행):\")\n",
    "        display(df_hierarchical.head(30))\n",
    "        \n",
    "        # 다양한 레벨의 예시 보여주기\n",
    "        print(\"\\n🔍 레벨별 데이터 예시:\")\n",
    "        for level in sorted(df_hierarchical['level'].unique()):\n",
    "            level_sample = df_hierarchical[df_hierarchical['level'] == level].head(3)\n",
    "            if not level_sample.empty:\n",
    "                print(f\"\\n   Level {level} 예시:\")\n",
    "                for _, row in level_sample.iterrows():\n",
    "                    print(f\"      • key: '{row['key']}' → value: '{row['value']}'\")\n",
    "        \n",
    "        # 특정 제품의 전체 구조 보기\n",
    "        sample_product = df_hierarchical['product_id'].iloc[0]\n",
    "        print(f\"\\n🔍 제품 {sample_product}의 전체 specification 구조:\")\n",
    "        sample_spec = df_hierarchical[df_hierarchical['product_id'] == sample_product][['level', 'key', 'value']]\n",
    "        display(sample_spec)\n",
    "        \n",
    "        # 중첩된 key 분석 (';'를 포함하는 key들)\n",
    "        nested_keys = df_hierarchical[df_hierarchical['key'].str.contains(';', na=False)]\n",
    "        if not nested_keys.empty:\n",
    "            print(f\"\\n🔗 중첩된 구조를 가진 key 통계:\")\n",
    "            print(f\"   - 총 중첩 key 수: {len(nested_keys):,}\")\n",
    "            print(f\"   - 고유 중첩 key 패턴: {nested_keys['key'].nunique():,}\")\n",
    "            \n",
    "            # 중첩 구조 예시\n",
    "            print(\"\\n   중첩 구조 예시 (상위 10개):\")\n",
    "            nested_examples = nested_keys.groupby('key').size().sort_values(ascending=False).head(10)\n",
    "            for key, count in nested_examples.items():\n",
    "                print(f\"      • {key}: {count}개\")\n",
    "        \n",
    "        # 카테고리별 주요 key 분석\n",
    "        print(\"\\n📊 카테고리별 주요 key 통계:\")\n",
    "        category_key_stats = df_hierarchical.groupby(['display_category_major', 'key']).size().reset_index(name='count')\n",
    "        \n",
    "        major_categories = df_hierarchical['display_category_major'].value_counts().head(3).index\n",
    "        for category in major_categories:\n",
    "            cat_data = category_key_stats[category_key_stats['display_category_major'] == category]\n",
    "            top_keys = cat_data.nlargest(5, 'count')\n",
    "            \n",
    "            print(f\"\\n   [{category}] - 상위 5개 key:\")\n",
    "            for _, row in top_keys.iterrows():\n",
    "                level_indicator = \"  \" * (row['key'].count(';'))  # 들여쓰기로 레벨 표시\n",
    "                print(f\"      {level_indicator}• {row['key']}: {row['count']}개\")\n",
    "        \n",
    "        # 가장 깊은 중첩 구조 찾기\n",
    "        if not nested_keys.empty:\n",
    "            max_nesting = nested_keys['key'].str.count(';').max() + 1\n",
    "            deepest_keys = nested_keys[nested_keys['key'].str.count(';') == (max_nesting - 1)]\n",
    "            \n",
    "            print(f\"\\n🏔️ 가장 깊은 중첩 구조 (Level {max_nesting}):\")\n",
    "            for _, row in deepest_keys.head(5).iterrows():\n",
    "                print(f\"   • {row['key']}\")\n",
    "                print(f\"     → value: {row['value']}\")\n",
    "                print(f\"     → category: {row['display_category_major']}\")\n",
    "        \n",
    "        # 결과를 변수에 저장\n",
    "        df_hierarchical_final = df_hierarchical\n",
    "        \n",
    "        print(\"\\n💾 데이터프레임이 다음 변수에 저장되었습니다:\")\n",
    "        print(\"   - df_hierarchical_final: 계층 구조 데이터프레임\")\n",
    "        print(\"     • level: 깊이 (1, 2, 3...)\")\n",
    "        print(\"     • key: 계층 경로 (';'로 구분)\")\n",
    "        print(\"     • value: 실제 값\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류 발생: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        conn.close()\n",
    "else:\n",
    "    print(\"❌ DB 연결 실패\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cfb0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9g7ralqjsna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. product_specification 깊이가 2 이상인 제품 찾기\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def get_json_depth(obj, current_depth=0):\n",
    "    \"\"\"JSON 객체의 최대 깊이를 재귀적으로 계산\"\"\"\n",
    "    if not isinstance(obj, (dict, list)):\n",
    "        return current_depth\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        if not obj:\n",
    "            return current_depth\n",
    "        return max(get_json_depth(v, current_depth + 1) for v in obj.values())\n",
    "    \n",
    "    if isinstance(obj, list):\n",
    "        if not obj:\n",
    "            return current_depth\n",
    "        return max(get_json_depth(item, current_depth) for item in obj)\n",
    "\n",
    "# DB에서 데이터 로드\n",
    "conn = get_db_connection()\n",
    "if conn:\n",
    "    try:\n",
    "        # product_specification이 not null인 모든 데이터 조회\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            product_id,\n",
    "            display_category_major,\n",
    "            display_category_middle,\n",
    "            display_category_minor,\n",
    "            model_name,\n",
    "            product_name,\n",
    "            product_specification\n",
    "        FROM kt_merged_product_20250929\n",
    "        WHERE product_specification IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        df_products = pd.read_sql_query(query, conn)\n",
    "        print(f\"📊 전체 데이터 로드 완료: {len(df_products)}개 행\\n\")\n",
    "        \n",
    "        # 각 제품의 JSON 깊이 계산\n",
    "        products_with_depth = []\n",
    "        \n",
    "        for idx, row in df_products.iterrows():\n",
    "            try:\n",
    "                spec_str = row['product_specification']\n",
    "                if pd.notna(spec_str):\n",
    "                    # JSON 파싱\n",
    "                    if isinstance(spec_str, str):\n",
    "                        spec_json = json.loads(spec_str)\n",
    "                    else:\n",
    "                        spec_json = spec_str\n",
    "                    \n",
    "                    # 깊이 계산\n",
    "                    depth = get_json_depth(spec_json)\n",
    "                    \n",
    "                    products_with_depth.append({\n",
    "                        'product_id': row['product_id'],\n",
    "                        'display_category_major': row['display_category_major'],\n",
    "                        'display_category_middle': row['display_category_middle'],\n",
    "                        'display_category_minor': row['display_category_minor'],\n",
    "                        'model_name': row['model_name'],\n",
    "                        'product_name': row['product_name'],\n",
    "                        'depth': depth,\n",
    "                        'spec_sample': str(spec_json)[:200] + '...' if len(str(spec_json)) > 200 else str(spec_json)\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ 행 {idx} (product_id: {row['product_id']}) 처리 실패: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # 데이터프레임 생성\n",
    "        df_depth = pd.DataFrame(products_with_depth)\n",
    "        \n",
    "        # 깊이별 통계\n",
    "        print(\"📊 JSON 깊이별 제품 분포:\")\n",
    "        depth_stats = df_depth['depth'].value_counts().sort_index()\n",
    "        for depth, count in depth_stats.items():\n",
    "            percentage = (count / len(df_depth)) * 100\n",
    "            print(f\"   - 깊이 {depth}: {count:4d}개 ({percentage:5.1f}%)\")\n",
    "        \n",
    "        # 깊이가 2 이상인 제품 필터링\n",
    "        df_deep = df_depth[df_depth['depth'] >= 2].copy()\n",
    "        \n",
    "        print(f\"\\n🔍 깊이가 2 이상인 제품: {len(df_deep)}개\")\n",
    "        \n",
    "        if len(df_deep) > 0:\n",
    "            # 카테고리별 분포\n",
    "            print(\"\\n📦 카테고리별 분포 (깊이 >= 2):\")\n",
    "            category_dist = df_deep['display_category_major'].value_counts()\n",
    "            for category, count in category_dist.head(10).items():\n",
    "                print(f\"   • {category}: {count}개\")\n",
    "            \n",
    "            # 깊이별 상세 분석\n",
    "            print(\"\\n📋 깊이별 제품 목록:\")\n",
    "            \n",
    "            for depth in sorted(df_deep['depth'].unique(), reverse=True):\n",
    "                depth_products = df_deep[df_deep['depth'] == depth]\n",
    "                print(f\"\\n━━━ 깊이 {depth} ({len(depth_products)}개 제품) ━━━\")\n",
    "                \n",
    "                # 최대 10개만 표시\n",
    "                for idx, (_, row) in enumerate(depth_products.head(10).iterrows(), 1):\n",
    "                    print(f\"\\n{idx}. Product ID: {row['product_id']}\")\n",
    "                    print(f\"   - 제품명: {row['product_name']}\")\n",
    "                    print(f\"   - 모델명: {row['model_name']}\")\n",
    "                    print(f\"   - 카테고리: {row['display_category_major']} > {row['display_category_middle']} > {row['display_category_minor']}\")\n",
    "                    print(f\"   - JSON 샘플: {row['spec_sample']}\")\n",
    "                \n",
    "                if len(depth_products) > 10:\n",
    "                    print(f\"\\n   ... 외 {len(depth_products) - 10}개 더 있음\")\n",
    "            \n",
    "            # 깊이가 가장 깊은 제품 상세 분석\n",
    "            max_depth = df_deep['depth'].max()\n",
    "            deepest_products = df_deep[df_deep['depth'] == max_depth]\n",
    "            \n",
    "            print(f\"\\n🏔️ 가장 깊은 구조를 가진 제품 (깊이 {max_depth}):\")\n",
    "            for idx, (_, row) in enumerate(deepest_products.iterrows(), 1):\n",
    "                print(f\"\\n{idx}. {row['product_id']} - {row['product_name']}\")\n",
    "                \n",
    "                # 실제 JSON 구조 파싱하여 보여주기\n",
    "                spec_data = df_products[df_products['product_id'] == row['product_id']]['product_specification'].iloc[0]\n",
    "                if isinstance(spec_data, str):\n",
    "                    spec_json = json.loads(spec_data)\n",
    "                else:\n",
    "                    spec_json = spec_data\n",
    "                \n",
    "                # JSON 구조를 들여쓰기로 보기 좋게 출력\n",
    "                print(\"   JSON 구조:\")\n",
    "                print(json.dumps(spec_json, indent=2, ensure_ascii=False)[:1000])\n",
    "                if len(json.dumps(spec_json)) > 1000:\n",
    "                    print(\"   ... (이하 생략)\")\n",
    "            \n",
    "            # 결과 데이터프레임 표시\n",
    "            print(\"\\n📊 깊이 2 이상 제품 데이터프레임 (상위 20개):\")\n",
    "            display(df_deep[['product_id', 'product_name', 'display_category_major', 'display_category_middle', 'depth']].head(20))\n",
    "            \n",
    "            # CSV로 저장할 수 있도록 준비\n",
    "            df_deep_export = df_deep[['product_id', 'product_name', 'model_name', \n",
    "                                      'display_category_major', 'display_category_middle', \n",
    "                                      'display_category_minor', 'depth']]\n",
    "            \n",
    "            print(f\"\\n💾 변수에 저장됨:\")\n",
    "            print(f\"   - df_deep_products: 깊이 2 이상인 제품 전체 데이터\")\n",
    "            print(f\"   - df_deep_export: 내보내기용 데이터 (spec_sample 제외)\")\n",
    "            \n",
    "            # 변수에 저장\n",
    "            df_deep_products = df_deep\n",
    "            df_deep_export = df_deep_export\n",
    "            \n",
    "        else:\n",
    "            print(\"\\n✅ 모든 제품의 JSON 깊이가 1입니다.\")\n",
    "            df_deep_products = pd.DataFrame()\n",
    "            df_deep_export = pd.DataFrame()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류 발생: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        conn.close()\n",
    "else:\n",
    "    print(\"❌ DB 연결 실패\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uywec00nvf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. 카테고리별 상위 10개 key의 value 분포 및 outlier 분석\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def detect_outliers_in_values(values_list):\n",
    "    \"\"\"값 리스트에서 outlier를 감지\"\"\"\n",
    "    if len(values_list) < 3:\n",
    "        return []\n",
    "    \n",
    "    # 숫자형 값 시도\n",
    "    numeric_values = []\n",
    "    for v in values_list:\n",
    "        try:\n",
    "            # 단위 제거하고 숫자만 추출 시도\n",
    "            import re\n",
    "            num_match = re.findall(r'[-+]?\\d*\\.?\\d+', str(v))\n",
    "            if num_match:\n",
    "                numeric_values.append(float(num_match[0]))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # 숫자형 outlier 감지 (IQR 방법)\n",
    "    if len(numeric_values) > 3:\n",
    "        q1 = np.percentile(numeric_values, 25)\n",
    "        q3 = np.percentile(numeric_values, 75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        \n",
    "        outliers = []\n",
    "        for i, val in enumerate(numeric_values):\n",
    "            if val < lower_bound or val > upper_bound:\n",
    "                outliers.append((values_list[i], val, 'numeric'))\n",
    "        return outliers\n",
    "    \n",
    "    # 문자형 값의 빈도 기반 outlier (매우 드문 값)\n",
    "    value_counts = Counter(values_list)\n",
    "    total = len(values_list)\n",
    "    outliers = []\n",
    "    \n",
    "    for value, count in value_counts.items():\n",
    "        if count == 1 and total > 10:  # 전체 10개 이상 중 1번만 나타난 값\n",
    "            outliers.append((value, count/total, 'frequency'))\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# DB에서 데이터 로드\n",
    "conn = get_db_connection()\n",
    "if conn:\n",
    "    try:\n",
    "        # product_specification이 있는 모든 데이터 조회\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            product_id,\n",
    "            display_category_major,\n",
    "            display_category_middle,\n",
    "            display_category_minor,\n",
    "            product_name,\n",
    "            product_specification\n",
    "        FROM kt_merged_product_20250929\n",
    "        WHERE product_specification IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        df_products = pd.read_sql_query(query, conn)\n",
    "        print(f\"📊 전체 데이터 로드 완료: {len(df_products)}개 행\\n\")\n",
    "        \n",
    "        # 카테고리별로 데이터 정리\n",
    "        category_key_values = {}\n",
    "        \n",
    "        for idx, row in df_products.iterrows():\n",
    "            try:\n",
    "                category = row['display_category_major']\n",
    "                spec_str = row['product_specification']\n",
    "                \n",
    "                if pd.notna(spec_str):\n",
    "                    # JSON 파싱\n",
    "                    if isinstance(spec_str, str):\n",
    "                        spec_json = json.loads(spec_str)\n",
    "                    else:\n",
    "                        spec_json = spec_str\n",
    "                    \n",
    "                    if category not in category_key_values:\n",
    "                        category_key_values[category] = {}\n",
    "                    \n",
    "                    # 각 key-value 저장\n",
    "                    for key, value in spec_json.items():\n",
    "                        if key not in category_key_values[category]:\n",
    "                            category_key_values[category][key] = []\n",
    "                        \n",
    "                        # value 처리\n",
    "                        if isinstance(value, list) and len(value) > 0:\n",
    "                            value_str = str(value[0])\n",
    "                        elif isinstance(value, (dict, list)):\n",
    "                            value_str = json.dumps(value, ensure_ascii=False)\n",
    "                        else:\n",
    "                            value_str = str(value) if value is not None else None\n",
    "                        \n",
    "                        if value_str:\n",
    "                            category_key_values[category][key].append({\n",
    "                                'product_id': row['product_id'],\n",
    "                                'product_name': row['product_name'],\n",
    "                                'value': value_str\n",
    "                            })\n",
    "                            \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # 각 카테고리별 분석\n",
    "        print(\"=\" * 80)\n",
    "        print(\"📊 카테고리별 상위 10개 Key의 Value 분포 및 Outlier 분석\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        all_outliers = []\n",
    "        \n",
    "        for category in sorted(category_key_values.keys()):\n",
    "            print(f\"\\n\\n{'='*60}\")\n",
    "            print(f\"📦 카테고리: {category}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # 해당 카테고리의 key를 빈도순으로 정렬\n",
    "            key_frequencies = [(key, len(values)) for key, values in category_key_values[category].items()]\n",
    "            top_keys = sorted(key_frequencies, key=lambda x: x[1], reverse=True)[:10]\n",
    "            \n",
    "            print(f\"총 {len(key_frequencies)}개 key 중 상위 10개 분석\\n\")\n",
    "            \n",
    "            category_outliers = []\n",
    "            \n",
    "            for i, (key, freq) in enumerate(top_keys, 1):\n",
    "                values_data = category_key_values[category][key]\n",
    "                values_list = [v['value'] for v in values_data]\n",
    "                unique_values = list(set(values_list))\n",
    "                \n",
    "                print(f\"\\n{i}. Key: '{key}'\")\n",
    "                print(f\"   - 총 제품 수: {len(values_list)}\")\n",
    "                print(f\"   - 고유 값 개수: {len(unique_values)}\")\n",
    "                \n",
    "                # value 분포 분석\n",
    "                value_counter = Counter(values_list)\n",
    "                \n",
    "                # 상위 5개 빈도 값\n",
    "                top_values = value_counter.most_common(5)\n",
    "                print(f\"   - 최빈 값 Top 5:\")\n",
    "                for value, count in top_values:\n",
    "                    percentage = (count / len(values_list)) * 100\n",
    "                    display_value = value[:50] + '...' if len(value) > 50 else value\n",
    "                    print(f\"      • '{display_value}': {count}개 ({percentage:.1f}%)\")\n",
    "                \n",
    "                # Outlier 감지\n",
    "                outliers = detect_outliers_in_values(values_list)\n",
    "                \n",
    "                if outliers:\n",
    "                    print(f\"   \\n   🔴 Outlier 발견 ({len(outliers)}개):\")\n",
    "                    \n",
    "                    # outlier 타입별로 정리\n",
    "                    numeric_outliers = [o for o in outliers if o[2] == 'numeric']\n",
    "                    frequency_outliers = [o for o in outliers if o[2] == 'frequency']\n",
    "                    \n",
    "                    if numeric_outliers:\n",
    "                        print(f\"      [수치형 Outlier]\")\n",
    "                        for out_value, numeric_val, _ in numeric_outliers[:5]:\n",
    "                            # 해당 값을 가진 제품 찾기\n",
    "                            products = [v for v in values_data if v['value'] == out_value]\n",
    "                            if products:\n",
    "                                product = products[0]\n",
    "                                print(f\"         - 값: '{out_value}' (수치: {numeric_val:.2f})\")\n",
    "                                print(f\"           제품: {product['product_id']} - {product['product_name'][:50]}\")\n",
    "                            \n",
    "                            # 전체 outlier 리스트에 추가\n",
    "                            category_outliers.append({\n",
    "                                'category': category,\n",
    "                                'key': key,\n",
    "                                'value': out_value,\n",
    "                                'outlier_type': 'numeric',\n",
    "                                'product_id': product['product_id'] if products else None,\n",
    "                                'product_name': product['product_name'] if products else None\n",
    "                            })\n",
    "                    \n",
    "                    if frequency_outliers and len(unique_values) > 10:\n",
    "                        print(f\"      [빈도 기반 Outlier (희귀 값)]\")\n",
    "                        for out_value, freq_ratio, _ in frequency_outliers[:5]:\n",
    "                            # 해당 값을 가진 제품 찾기\n",
    "                            products = [v for v in values_data if v['value'] == out_value]\n",
    "                            if products:\n",
    "                                product = products[0]\n",
    "                                display_value = out_value[:50] + '...' if len(out_value) > 50 else out_value\n",
    "                                print(f\"         - 값: '{display_value}'\")\n",
    "                                print(f\"           제품: {product['product_id']} - {product['product_name'][:50]}\")\n",
    "                            \n",
    "                            # 전체 outlier 리스트에 추가\n",
    "                            category_outliers.append({\n",
    "                                'category': category,\n",
    "                                'key': key,\n",
    "                                'value': out_value,\n",
    "                                'outlier_type': 'frequency',\n",
    "                                'product_id': product['product_id'] if products else None,\n",
    "                                'product_name': product['product_name'] if products else None\n",
    "                            })\n",
    "                \n",
    "                # 값의 패턴 분석\n",
    "                if len(unique_values) <= 10:\n",
    "                    print(f\"   - 모든 고유 값:\")\n",
    "                    for value in sorted(unique_values)[:10]:\n",
    "                        display_value = value[:50] + '...' if len(value) > 50 else value\n",
    "                        count = value_counter[value]\n",
    "                        print(f\"      • '{display_value}': {count}개\")\n",
    "            \n",
    "            all_outliers.extend(category_outliers)\n",
    "            \n",
    "            # 카테고리별 outlier 요약\n",
    "            if category_outliers:\n",
    "                print(f\"\\n   📊 {category} 카테고리 Outlier 요약:\")\n",
    "                print(f\"      - 총 {len(category_outliers)}개 outlier 발견\")\n",
    "                outlier_keys = Counter([o['key'] for o in category_outliers])\n",
    "                print(f\"      - Outlier가 있는 key: {', '.join(outlier_keys.keys())}\")\n",
    "        \n",
    "        # 전체 Outlier 데이터프레임 생성\n",
    "        if all_outliers:\n",
    "            df_outliers = pd.DataFrame(all_outliers)\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"📊 전체 Outlier 요약\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            print(f\"\\n총 {len(df_outliers)}개 outlier 발견\")\n",
    "            \n",
    "            print(\"\\n카테고리별 outlier 수:\")\n",
    "            category_counts = df_outliers['category'].value_counts()\n",
    "            for cat, count in category_counts.items():\n",
    "                print(f\"   • {cat}: {count}개\")\n",
    "            \n",
    "            print(\"\\nOutlier 타입별 분포:\")\n",
    "            type_counts = df_outliers['outlier_type'].value_counts()\n",
    "            for type_name, count in type_counts.items():\n",
    "                print(f\"   • {type_name}: {count}개\")\n",
    "            \n",
    "            print(\"\\n상위 10개 Outlier 상세:\")\n",
    "            display(df_outliers[['category', 'key', 'value', 'outlier_type', 'product_id', 'product_name']].head(10))\n",
    "            \n",
    "            # 특정 key에서 자주 outlier가 발생하는지 확인\n",
    "            key_outlier_counts = df_outliers.groupby(['category', 'key']).size().sort_values(ascending=False).head(10)\n",
    "            \n",
    "            print(\"\\nOutlier가 많이 발생한 Key Top 10:\")\n",
    "            for (cat, key), count in key_outlier_counts.items():\n",
    "                print(f\"   • [{cat}] {key}: {count}개 outlier\")\n",
    "            \n",
    "            print(\"\\n💾 변수에 저장됨:\")\n",
    "            print(\"   - df_outliers: 전체 outlier 데이터프레임\")\n",
    "            print(\"   - category_key_values: 카테고리별 key-value 전체 데이터\")\n",
    "        else:\n",
    "            print(\"\\n✅ 명확한 outlier가 발견되지 않았습니다.\")\n",
    "            df_outliers = pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류 발생: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        conn.close()\n",
    "else:\n",
    "    print(\"❌ DB 연결 실패\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wagr185p7o",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. display_category_major, display_category_middle, key별 분포 및 outlier 분석\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def detect_distribution_outliers(groups_data):\n",
    "    \"\"\"그룹별 count 분포에서 outlier 감지\"\"\"\n",
    "    counts = [g['count'] for g in groups_data]\n",
    "    \n",
    "    if len(counts) < 4:\n",
    "        return []\n",
    "    \n",
    "    # IQR 방법으로 outlier 감지\n",
    "    q1 = np.percentile(counts, 25)\n",
    "    q3 = np.percentile(counts, 75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    outliers = []\n",
    "    for group in groups_data:\n",
    "        if group['count'] < lower_bound or group['count'] > upper_bound:\n",
    "            group['outlier_type'] = 'count_outlier'\n",
    "            group['lower_bound'] = lower_bound\n",
    "            group['upper_bound'] = upper_bound\n",
    "            outliers.append(group)\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# DB에서 데이터 로드\n",
    "conn = get_db_connection()\n",
    "if conn:\n",
    "    try:\n",
    "        # product_specification이 있는 모든 데이터 조회\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            product_id,\n",
    "            display_category_major,\n",
    "            display_category_middle,\n",
    "            display_category_minor,\n",
    "            product_name,\n",
    "            product_specification\n",
    "        FROM kt_merged_product_20250929\n",
    "        WHERE product_specification IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        df_products = pd.read_sql_query(query, conn)\n",
    "        print(f\"📊 전체 데이터 로드 완료: {len(df_products)}개 행\\n\")\n",
    "        \n",
    "        # 카테고리(major, middle)와 key별로 데이터 정리\n",
    "        category_key_data = []\n",
    "        \n",
    "        for idx, row in df_products.iterrows():\n",
    "            try:\n",
    "                spec_str = row['product_specification']\n",
    "                \n",
    "                if pd.notna(spec_str):\n",
    "                    # JSON 파싱\n",
    "                    if isinstance(spec_str, str):\n",
    "                        spec_json = json.loads(spec_str)\n",
    "                    else:\n",
    "                        spec_json = spec_str\n",
    "                    \n",
    "                    # 각 key에 대해 데이터 수집\n",
    "                    for key, value in spec_json.items():\n",
    "                        # value 처리\n",
    "                        if isinstance(value, list) and len(value) > 0:\n",
    "                            value_str = str(value[0])\n",
    "                        elif isinstance(value, (dict, list)):\n",
    "                            value_str = json.dumps(value, ensure_ascii=False)\n",
    "                        else:\n",
    "                            value_str = str(value) if value is not None else None\n",
    "                        \n",
    "                        if value_str:\n",
    "                            category_key_data.append({\n",
    "                                'display_category_major': row['display_category_major'],\n",
    "                                'display_category_middle': row['display_category_middle'],\n",
    "                                'display_category_minor': row['display_category_minor'],\n",
    "                                'key': key,\n",
    "                                'value': value_str,\n",
    "                                'product_id': row['product_id'],\n",
    "                                'product_name': row['product_name']\n",
    "                            })\n",
    "                            \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # 데이터프레임 생성\n",
    "        df_category_key = pd.DataFrame(category_key_data)\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"📊 Category Major, Middle, Key별 분포 분석\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Group by major, middle, key\n",
    "        grouped = df_category_key.groupby(['display_category_major', 'display_category_middle', 'key']).agg({\n",
    "            'product_id': 'count',\n",
    "            'value': lambda x: len(set(x))  # 고유 value 개수\n",
    "        }).rename(columns={'product_id': 'count', 'value': 'unique_values'}).reset_index()\n",
    "        \n",
    "        print(f\"\\n총 {len(grouped)}개의 (major, middle, key) 조합\\n\")\n",
    "        \n",
    "        # 기본 통계\n",
    "        print(\"📈 Count 분포 통계:\")\n",
    "        print(f\"   - 평균: {grouped['count'].mean():.2f}\")\n",
    "        print(f\"   - 중앙값: {grouped['count'].median():.2f}\")\n",
    "        print(f\"   - 최소값: {grouped['count'].min()}\")\n",
    "        print(f\"   - 최대값: {grouped['count'].max()}\")\n",
    "        print(f\"   - 표준편차: {grouped['count'].std():.2f}\")\n",
    "        \n",
    "        # Count 기준 상위/하위 그룹\n",
    "        print(\"\\n📊 Count 기준 Top 10 조합:\")\n",
    "        top_combinations = grouped.nlargest(10, 'count')\n",
    "        for idx, row in top_combinations.iterrows():\n",
    "            print(f\"   {row['count']:3d}개: [{row['display_category_major']}] > [{row['display_category_middle']}] > '{row['key']}'\")\n",
    "        \n",
    "        print(\"\\n📊 Count 기준 Bottom 10 조합 (가장 적은):\")\n",
    "        bottom_combinations = grouped.nsmallest(10, 'count')\n",
    "        for idx, row in bottom_combinations.iterrows():\n",
    "            print(f\"   {row['count']:3d}개: [{row['display_category_major']}] > [{row['display_category_middle']}] > '{row['key']}'\")\n",
    "        \n",
    "        # 각 major 카테고리별 outlier 분석\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"🔍 Major 카테고리별 Outlier 분석\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        all_outliers = []\n",
    "        \n",
    "        for major_cat in grouped['display_category_major'].unique():\n",
    "            major_data = grouped[grouped['display_category_major'] == major_cat]\n",
    "            \n",
    "            if len(major_data) < 4:\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\n📦 {major_cat}\")\n",
    "            print(f\"   - 총 {len(major_data)}개 (middle, key) 조합\")\n",
    "            \n",
    "            # Count 기반 outlier 찾기\n",
    "            groups_list = major_data.to_dict('records')\n",
    "            outliers = detect_distribution_outliers(groups_list)\n",
    "            \n",
    "            if outliers:\n",
    "                print(f\"   - 🔴 {len(outliers)}개 outlier 발견\")\n",
    "                \n",
    "                # 높은 outlier (비정상적으로 많은)\n",
    "                high_outliers = [o for o in outliers if o['count'] > o['upper_bound']]\n",
    "                if high_outliers:\n",
    "                    print(f\"\\n   [비정상적으로 많은 count]\")\n",
    "                    for out in sorted(high_outliers, key=lambda x: x['count'], reverse=True)[:5]:\n",
    "                        print(f\"      • {out['count']}개: [{out['display_category_middle']}] > '{out['key']}'\")\n",
    "                        print(f\"        (정상 범위: {out['lower_bound']:.1f} ~ {out['upper_bound']:.1f})\")\n",
    "                \n",
    "                # 낮은 outlier (비정상적으로 적은)\n",
    "                low_outliers = [o for o in outliers if o['count'] < o['lower_bound']]\n",
    "                if low_outliers:\n",
    "                    print(f\"\\n   [비정상적으로 적은 count]\")\n",
    "                    for out in sorted(low_outliers, key=lambda x: x['count'])[:5]:\n",
    "                        print(f\"      • {out['count']}개: [{out['display_category_middle']}] > '{out['key']}'\")\n",
    "                \n",
    "                all_outliers.extend(outliers)\n",
    "        \n",
    "        # Middle 카테고리별 분석\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"🔍 Middle 카테고리별 Key 분포 분석\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        middle_key_stats = grouped.groupby('display_category_middle').agg({\n",
    "            'key': 'count',\n",
    "            'count': ['sum', 'mean', 'std']\n",
    "        }).round(2)\n",
    "        \n",
    "        middle_key_stats.columns = ['unique_keys', 'total_products', 'avg_products_per_key', 'std_products']\n",
    "        middle_key_stats = middle_key_stats.sort_values('total_products', ascending=False)\n",
    "        \n",
    "        print(\"\\nMiddle 카테고리별 통계 (상위 15개):\")\n",
    "        display(middle_key_stats.head(15))\n",
    "        \n",
    "        # Key별 카테고리 분포 분석\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"🔑 Key별 카테고리 분포 분석\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        key_category_stats = grouped.groupby('key').agg({\n",
    "            'display_category_major': 'nunique',\n",
    "            'display_category_middle': 'nunique',\n",
    "            'count': ['sum', 'mean', 'std']\n",
    "        }).round(2)\n",
    "        \n",
    "        key_category_stats.columns = ['major_categories', 'middle_categories', 'total_products', 'avg_products', 'std_products']\n",
    "        key_category_stats = key_category_stats.sort_values('total_products', ascending=False)\n",
    "        \n",
    "        print(\"\\nKey별 카테고리 분포 (상위 20개):\")\n",
    "        display(key_category_stats.head(20))\n",
    "        \n",
    "        # 특이 패턴 찾기\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"🎯 특이 패턴 분석\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # 1. 한 middle 카테고리에만 있는 unique key\n",
    "        unique_keys_per_middle = grouped.groupby('key')['display_category_middle'].nunique()\n",
    "        single_middle_keys = unique_keys_per_middle[unique_keys_per_middle == 1].index\n",
    "        \n",
    "        if len(single_middle_keys) > 0:\n",
    "            print(f\"\\n1. 단일 Middle 카테고리 전용 Key ({len(single_middle_keys)}개 중 10개):\")\n",
    "            for key in list(single_middle_keys)[:10]:\n",
    "                middle_cat = grouped[grouped['key'] == key]['display_category_middle'].iloc[0]\n",
    "                count = grouped[grouped['key'] == key]['count'].iloc[0]\n",
    "                print(f\"   • '{key}' → [{middle_cat}] ({count}개 제품)\")\n",
    "        \n",
    "        # 2. 여러 카테고리에 걸쳐있는 key\n",
    "        multi_category_keys = key_category_stats[key_category_stats['middle_categories'] >= 5].head(10)\n",
    "        \n",
    "        if len(multi_category_keys) > 0:\n",
    "            print(f\"\\n2. 5개 이상 Middle 카테고리에 걸친 범용 Key:\")\n",
    "            for key, row in multi_category_keys.iterrows():\n",
    "                print(f\"   • '{key}': {int(row['middle_categories'])}개 middle 카테고리, {int(row['total_products'])}개 제품\")\n",
    "        \n",
    "        # 3. Value 다양성이 높은 조합\n",
    "        high_diversity = grouped.nlargest(10, 'unique_values')\n",
    "        \n",
    "        print(f\"\\n3. Value 다양성이 높은 조합 (고유값이 많은):\")\n",
    "        for idx, row in high_diversity.iterrows():\n",
    "            print(f\"   • {row['unique_values']}개 고유값: [{row['display_category_major']}] > [{row['display_category_middle']}] > '{row['key']}'\")\n",
    "        \n",
    "        # Outlier 데이터프레임 생성\n",
    "        if all_outliers:\n",
    "            df_group_outliers = pd.DataFrame(all_outliers)\n",
    "            print(f\"\\n\\n💾 결과 저장:\")\n",
    "            print(f\"   - df_grouped: 전체 그룹별 통계 데이터프레임\")\n",
    "            print(f\"   - df_group_outliers: 그룹 outlier 데이터프레임\")\n",
    "            print(f\"   - middle_key_stats: Middle 카테고리별 통계\")\n",
    "            print(f\"   - key_category_stats: Key별 카테고리 분포 통계\")\n",
    "        else:\n",
    "            df_group_outliers = pd.DataFrame()\n",
    "            print(f\"\\n\\n💾 결과 저장:\")\n",
    "            print(f\"   - df_grouped: 전체 그룹별 통계 데이터프레임\")\n",
    "            print(f\"   - middle_key_stats: Middle 카테고리별 통계\")\n",
    "            print(f\"   - key_category_stats: Key별 카테고리 분포 통계\")\n",
    "        \n",
    "        # 변수 저장\n",
    "        df_grouped = grouped\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류 발생: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        conn.close()\n",
    "else:\n",
    "    print(\"❌ DB 연결 실패\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mund3u8cv5j",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. 모든 카테고리의 unique values를 분석하여 파일로 출력 (비율 포함)\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def analyze_all_categories_to_file():\n",
    "    \"\"\"모든 middle 카테고리의 각 key별 unique values를 분석하여 파일로 저장 (비율 포함)\"\"\"\n",
    "    \n",
    "    conn = get_db_connection()\n",
    "    if not conn:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # 모든 데이터 조회\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            display_category_major,\n",
    "            display_category_middle,\n",
    "            display_category_minor,\n",
    "            product_id,\n",
    "            product_specification\n",
    "        FROM kt_merged_product_20250929\n",
    "        WHERE product_specification IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        df_products = pd.read_sql_query(query, conn)\n",
    "        print(f\"📊 전체 데이터 로드 완료: {len(df_products)}개 제품\")\n",
    "        \n",
    "        # 카테고리별로 제품 수 계산 (비율 계산용)\n",
    "        category_product_counts = {}\n",
    "        \n",
    "        # 카테고리별로 unique values 수집\n",
    "        category_key_values = {}\n",
    "        category_key_products = {}  # 각 key가 나타난 제품 ID 추적\n",
    "        \n",
    "        for idx, row in df_products.iterrows():\n",
    "            try:\n",
    "                major = row['display_category_major']\n",
    "                middle = row['display_category_middle']\n",
    "                product_id = row['product_id']\n",
    "                spec_str = row['product_specification']\n",
    "                \n",
    "                if pd.notna(spec_str):\n",
    "                    # JSON 파싱\n",
    "                    if isinstance(spec_str, str):\n",
    "                        spec_json = json.loads(spec_str)\n",
    "                    else:\n",
    "                        spec_json = spec_str\n",
    "                    \n",
    "                    # 카테고리별 제품 수 카운트\n",
    "                    if major not in category_product_counts:\n",
    "                        category_product_counts[major] = {'total': set(), 'middle': {}}\n",
    "                    if middle not in category_product_counts[major]['middle']:\n",
    "                        category_product_counts[major]['middle'][middle] = set()\n",
    "                    \n",
    "                    category_product_counts[major]['total'].add(product_id)\n",
    "                    category_product_counts[major]['middle'][middle].add(product_id)\n",
    "                    \n",
    "                    # 카테고리 조합 키 생성\n",
    "                    category_key = (major, middle)\n",
    "                    \n",
    "                    if category_key not in category_key_values:\n",
    "                        category_key_values[category_key] = {}\n",
    "                        category_key_products[category_key] = {}\n",
    "                    \n",
    "                    # 각 key-value 수집\n",
    "                    for key, value in spec_json.items():\n",
    "                        if key not in category_key_values[category_key]:\n",
    "                            category_key_values[category_key][key] = set()\n",
    "                            category_key_products[category_key][key] = set()\n",
    "                        \n",
    "                        # value 처리\n",
    "                        if isinstance(value, list) and len(value) > 0:\n",
    "                            value_str = str(value[0])\n",
    "                        elif isinstance(value, (dict, list)):\n",
    "                            value_str = json.dumps(value, ensure_ascii=False)\n",
    "                        else:\n",
    "                            value_str = str(value) if value is not None else None\n",
    "                        \n",
    "                        if value_str:\n",
    "                            category_key_values[category_key][key].add(value_str)\n",
    "                            category_key_products[category_key][key].add(product_id)\n",
    "                            \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        print(f\"✅ 데이터 수집 완료: {len(category_key_values)}개 카테고리 조합\")\n",
    "        \n",
    "        # 카테고리별 총 제품 수 계산\n",
    "        category_totals = {}\n",
    "        for major, data in category_product_counts.items():\n",
    "            category_totals[major] = {\n",
    "                'total': len(data['total']),\n",
    "                'middle': {middle: len(products) for middle, products in data['middle'].items()}\n",
    "            }\n",
    "        \n",
    "        # 결과를 정리하여 리스트로 변환\n",
    "        result_data = []\n",
    "        \n",
    "        for (major, middle), keys_dict in category_key_values.items():\n",
    "            major_total = category_totals[major]['total']\n",
    "            middle_total = category_totals[major]['middle'][middle]\n",
    "            \n",
    "            for key, values in keys_dict.items():\n",
    "                sorted_values = sorted(list(values))\n",
    "                \n",
    "                # 이 key를 가진 제품 수\n",
    "                products_with_key = len(category_key_products[(major, middle)][key])\n",
    "                \n",
    "                # 비율 계산\n",
    "                ratio_in_middle = (products_with_key / middle_total * 100) if middle_total > 0 else 0\n",
    "                ratio_in_major = (products_with_key / major_total * 100) if major_total > 0 else 0\n",
    "                \n",
    "                # 탭으로 구분된 값 리스트 생성\n",
    "                # 값들 사이는 세미콜론(;)으로 구분\n",
    "                values_str = '; '.join(sorted_values)\n",
    "                \n",
    "                result_data.append({\n",
    "                    'display_category_major': major,\n",
    "                    'display_category_middle': middle,\n",
    "                    'key': key,\n",
    "                    'product_count': products_with_key,\n",
    "                    'middle_total': middle_total,\n",
    "                    'ratio_in_middle': round(ratio_in_middle, 2),\n",
    "                    'major_total': major_total,\n",
    "                    'ratio_in_major': round(ratio_in_major, 2),\n",
    "                    'unique_values_count': len(sorted_values),\n",
    "                    'unique_values': values_str\n",
    "                })\n",
    "        \n",
    "        # 데이터프레임 생성 및 정렬\n",
    "        df_result = pd.DataFrame(result_data)\n",
    "        df_result = df_result.sort_values(['display_category_major', 'display_category_middle', 'key'])\n",
    "        \n",
    "        # 통계 출력\n",
    "        print(f\"\\n📊 분석 통계:\")\n",
    "        print(f\"   - 총 레코드 수: {len(df_result):,}\")\n",
    "        print(f\"   - Major 카테고리 수: {df_result['display_category_major'].nunique()}\")\n",
    "        print(f\"   - Middle 카테고리 수: {df_result['display_category_middle'].nunique()}\")\n",
    "        print(f\"   - 고유 Key 수: {df_result['key'].nunique()}\")\n",
    "        \n",
    "        # 카테고리별 통계\n",
    "        print(f\"\\n📋 카테고리별 통계:\")\n",
    "        category_stats = df_result.groupby(['display_category_major', 'display_category_middle']).agg({\n",
    "            'key': 'count',\n",
    "            'middle_total': 'first'\n",
    "        }).rename(columns={'key': 'key_count', 'middle_total': 'product_count'}).reset_index()\n",
    "        category_stats = category_stats.sort_values('product_count', ascending=False)\n",
    "        \n",
    "        print(\"\\n상위 10개 카테고리:\")\n",
    "        for idx, row in category_stats.head(10).iterrows():\n",
    "            print(f\"   • [{row['display_category_major']}] {row['display_category_middle']}: \"\n",
    "                  f\"{row['product_count']}개 제품, {row['key_count']}개 key\")\n",
    "        \n",
    "        # 파일로 저장 (탭 구분)\n",
    "        output_file = 'category_middle_spec_analysis.txt'\n",
    "        \n",
    "        # 저장할 컬럼 순서 정의\n",
    "        columns_order = [\n",
    "            'display_category_major', \n",
    "            'display_category_middle', \n",
    "            'key', \n",
    "            'product_count',\n",
    "            'middle_total',\n",
    "            'ratio_in_middle',\n",
    "            'major_total', \n",
    "            'ratio_in_major',\n",
    "            'unique_values_count',\n",
    "            'unique_values'\n",
    "        ]\n",
    "        \n",
    "        # 탭으로 구분된 텍스트 파일로 저장\n",
    "        df_result[columns_order].to_csv(output_file, sep='\\t', index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"\\n💾 파일 저장 완료: {output_file}\")\n",
    "        print(f\"   - 파일 크기: {os.path.getsize(output_file) / 1024:.2f} KB\")\n",
    "        print(f\"   - 총 {len(df_result):,}개 행\")\n",
    "        print(f\"   - 엑셀에서 열 때: 텍스트 가져오기 → 탭으로 구분\")\n",
    "        \n",
    "        # 샘플 데이터 표시 (비율 포함)\n",
    "        print(f\"\\n📋 데이터 샘플 (처음 10개 행):\")\n",
    "        display_cols = ['display_category_major', 'display_category_middle', 'key', \n",
    "                       'product_count', 'ratio_in_middle', 'ratio_in_major', 'unique_values_count']\n",
    "        display(df_result[display_cols].head(10))\n",
    "        \n",
    "        # 비율이 높은 key들 분석\n",
    "        print(f\"\\n📈 Middle 카테고리 내 비율이 높은 Key Top 10 (90% 이상):\")\n",
    "        high_ratio = df_result[df_result['ratio_in_middle'] >= 90].sort_values('ratio_in_middle', ascending=False)\n",
    "        for idx, row in high_ratio.head(10).iterrows():\n",
    "            print(f\"   • {row['ratio_in_middle']:.1f}%: [{row['display_category_major']}] \"\n",
    "                  f\"{row['display_category_middle']} > '{row['key']}' ({row['product_count']}/{row['middle_total']})\")\n",
    "        \n",
    "        # 비율이 낮은 key들 분석\n",
    "        print(f\"\\n📉 Middle 카테고리 내 비율이 낮은 Key (10% 미만, 최소 5개 제품):\")\n",
    "        low_ratio = df_result[(df_result['ratio_in_middle'] < 10) & (df_result['product_count'] >= 5)]\n",
    "        low_ratio = low_ratio.sort_values('ratio_in_middle')\n",
    "        for idx, row in low_ratio.head(10).iterrows():\n",
    "            print(f\"   • {row['ratio_in_middle']:.1f}%: [{row['display_category_major']}] \"\n",
    "                  f\"{row['display_category_middle']} > '{row['key']}' ({row['product_count']}/{row['middle_total']})\")\n",
    "        \n",
    "        # 특정 카테고리 상세 보기\n",
    "        print(f\"\\n🔍 특정 카테고리 예시 (갤럭시 스마트폰):\")\n",
    "        galaxy_data = df_result[df_result['display_category_middle'] == '갤럭시 스마트폰']\n",
    "        if not galaxy_data.empty:\n",
    "            galaxy_data = galaxy_data.sort_values('ratio_in_middle', ascending=False)\n",
    "            for idx, row in galaxy_data.head(5).iterrows():\n",
    "                print(f\"\\n   Key: '{row['key']}'\")\n",
    "                print(f\"   제품 수: {row['product_count']}/{row['middle_total']} ({row['ratio_in_middle']:.1f}%)\")\n",
    "                print(f\"   Major 내 비율: {row['ratio_in_major']:.1f}%\")\n",
    "                print(f\"   고유값 개수: {row['unique_values_count']}개\")\n",
    "                # 값이 너무 길면 일부만 표시\n",
    "                values_preview = row['unique_values']\n",
    "                if len(values_preview) > 150:\n",
    "                    values_preview = values_preview[:150] + '...'\n",
    "                print(f\"   Values: {values_preview}\")\n",
    "        \n",
    "        # 추가로 Excel 형식으로도 저장 (선택사항)\n",
    "        excel_file = 'category_middle_spec_analysis.xlsx'\n",
    "        try:\n",
    "            # unique_values가 너무 길면 Excel 셀 제한(32,767자)에 걸릴 수 있으므로 제한\n",
    "            df_excel = df_result.copy()\n",
    "            df_excel['unique_values'] = df_excel['unique_values'].apply(\n",
    "                lambda x: x[:32000] + '...(truncated)' if len(x) > 32000 else x\n",
    "            )\n",
    "            df_excel[columns_order].to_excel(excel_file, index=False, engine='openpyxl')\n",
    "            print(f\"\\n💾 Excel 파일도 저장됨: {excel_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠️ Excel 저장 실패 (파일이 너무 큼): {e}\")\n",
    "            print(f\"   → 텍스트 파일({output_file})을 사용하세요\")\n",
    "        \n",
    "        print(f\"\\n✅ 분석 완료!\")\n",
    "        print(f\"   - df_all_categories_analysis: 전체 분석 결과 데이터프레임\")\n",
    "        \n",
    "        return df_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류 발생: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# 모든 카테고리 분석 실행\n",
    "df_all_categories_analysis = analyze_all_categories_to_file()\n",
    "\n",
    "# 파일이 제대로 생성되었는지 확인\n",
    "if os.path.exists('category_middle_spec_analysis.txt'):\n",
    "    print(f\"\\n✅ 파일 확인: category_middle_spec_analysis.txt\")\n",
    "    print(f\"   파일 경로: {os.path.abspath('category_middle_spec_analysis.txt')}\")\n",
    "    \n",
    "    # 파일 처음 몇 줄 미리보기\n",
    "    with open('category_middle_spec_analysis.txt', 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()[:5]\n",
    "        print(f\"\\n📄 파일 미리보기 (처음 5줄):\")\n",
    "        for i, line in enumerate(lines, 1):\n",
    "            # 탭으로 구분된 필드 확인\n",
    "            fields = line.strip().split('\\t')\n",
    "            if i == 1:  # 헤더\n",
    "                print(f\"   헤더: {len(fields)}개 컬럼\")\n",
    "                print(f\"   컬럼: {fields[:5]}... (처음 5개)\")\n",
    "            else:\n",
    "                print(f\"   {i}: {fields[2] if len(fields) > 2 else ''} - \"\n",
    "                      f\"{fields[5] if len(fields) > 5 else ''}% in middle, \"\n",
    "                      f\"{fields[7] if len(fields) > 7 else ''}% in major\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vknoybp5b5o",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. display_category_minor까지 포함한 상세 분석\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def analyze_all_categories_with_minor():\n",
    "    \"\"\"major, middle, minor 카테고리의 각 key별 unique values를 분석하여 파일로 저장 (비율 포함)\"\"\"\n",
    "    \n",
    "    conn = get_db_connection()\n",
    "    if not conn:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # 모든 데이터 조회\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            display_category_major,\n",
    "            display_category_middle,\n",
    "            display_category_minor,\n",
    "            product_id,\n",
    "            product_specification\n",
    "        FROM kt_merged_product_20250929\n",
    "        WHERE product_specification IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        df_products = pd.read_sql_query(query, conn)\n",
    "        print(f\"📊 전체 데이터 로드 완료: {len(df_products)}개 제품\")\n",
    "        \n",
    "        # 카테고리별로 제품 수 계산 (비율 계산용)\n",
    "        category_product_counts = {}\n",
    "        \n",
    "        # 카테고리별로 unique values 수집\n",
    "        category_key_values = {}\n",
    "        category_key_products = {}  # 각 key가 나타난 제품 ID 추적\n",
    "        \n",
    "        for idx, row in df_products.iterrows():\n",
    "            try:\n",
    "                major = row['display_category_major']\n",
    "                middle = row['display_category_middle']\n",
    "                minor = row['display_category_minor']\n",
    "                product_id = row['product_id']\n",
    "                spec_str = row['product_specification']\n",
    "                \n",
    "                if pd.notna(spec_str):\n",
    "                    # JSON 파싱\n",
    "                    if isinstance(spec_str, str):\n",
    "                        spec_json = json.loads(spec_str)\n",
    "                    else:\n",
    "                        spec_json = spec_str\n",
    "                    \n",
    "                    # 카테고리별 제품 수 카운트\n",
    "                    if major not in category_product_counts:\n",
    "                        category_product_counts[major] = {'total': set(), 'middle': {}}\n",
    "                    if middle not in category_product_counts[major]['middle']:\n",
    "                        category_product_counts[major]['middle'][middle] = {'total': set(), 'minor': {}}\n",
    "                    if minor not in category_product_counts[major]['middle'][middle]['minor']:\n",
    "                        category_product_counts[major]['middle'][middle]['minor'][minor] = set()\n",
    "                    \n",
    "                    category_product_counts[major]['total'].add(product_id)\n",
    "                    category_product_counts[major]['middle'][middle]['total'].add(product_id)\n",
    "                    category_product_counts[major]['middle'][middle]['minor'][minor].add(product_id)\n",
    "                    \n",
    "                    # 카테고리 조합 키 생성 (minor까지 포함)\n",
    "                    category_key = (major, middle, minor)\n",
    "                    \n",
    "                    if category_key not in category_key_values:\n",
    "                        category_key_values[category_key] = {}\n",
    "                        category_key_products[category_key] = {}\n",
    "                    \n",
    "                    # 각 key-value 수집\n",
    "                    for key, value in spec_json.items():\n",
    "                        if key not in category_key_values[category_key]:\n",
    "                            category_key_values[category_key][key] = set()\n",
    "                            category_key_products[category_key][key] = set()\n",
    "                        \n",
    "                        # value 처리\n",
    "                        if isinstance(value, list) and len(value) > 0:\n",
    "                            value_str = str(value[0])\n",
    "                        elif isinstance(value, (dict, list)):\n",
    "                            value_str = json.dumps(value, ensure_ascii=False)\n",
    "                        else:\n",
    "                            value_str = str(value) if value is not None else None\n",
    "                        \n",
    "                        if value_str:\n",
    "                            category_key_values[category_key][key].add(value_str)\n",
    "                            category_key_products[category_key][key].add(product_id)\n",
    "                            \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        print(f\"✅ 데이터 수집 완료: {len(category_key_values)}개 카테고리 조합 (minor 포함)\")\n",
    "        \n",
    "        # 카테고리별 총 제품 수 계산\n",
    "        category_totals = {}\n",
    "        for major, major_data in category_product_counts.items():\n",
    "            category_totals[major] = {\n",
    "                'total': len(major_data['total']),\n",
    "                'middle': {}\n",
    "            }\n",
    "            for middle, middle_data in major_data['middle'].items():\n",
    "                category_totals[major]['middle'][middle] = {\n",
    "                    'total': len(middle_data['total']),\n",
    "                    'minor': {minor: len(products) for minor, products in middle_data['minor'].items()}\n",
    "                }\n",
    "        \n",
    "        # 결과를 정리하여 리스트로 변환\n",
    "        result_data = []\n",
    "        \n",
    "        for (major, middle, minor), keys_dict in category_key_values.items():\n",
    "            major_total = category_totals[major]['total']\n",
    "            middle_total = category_totals[major]['middle'][middle]['total']\n",
    "            minor_total = category_totals[major]['middle'][middle]['minor'].get(minor, 0)\n",
    "            \n",
    "            for key, values in keys_dict.items():\n",
    "                sorted_values = sorted(list(values))\n",
    "                \n",
    "                # 이 key를 가진 제품 수\n",
    "                products_with_key = len(category_key_products[(major, middle, minor)][key])\n",
    "                \n",
    "                # 비율 계산\n",
    "                ratio_in_minor = (products_with_key / minor_total * 100) if minor_total > 0 else 0\n",
    "                ratio_in_middle = (products_with_key / middle_total * 100) if middle_total > 0 else 0\n",
    "                ratio_in_major = (products_with_key / major_total * 100) if major_total > 0 else 0\n",
    "                \n",
    "                # 탭으로 구분된 값 리스트 생성\n",
    "                # 값들 사이는 세미콜론(;)으로 구분\n",
    "                values_str = '; '.join(sorted_values)\n",
    "                \n",
    "                result_data.append({\n",
    "                    'display_category_major': major,\n",
    "                    'display_category_middle': middle,\n",
    "                    'display_category_minor': minor,\n",
    "                    'key': key,\n",
    "                    'product_count': products_with_key,\n",
    "                    'minor_total': minor_total,\n",
    "                    'ratio_in_minor': round(ratio_in_minor, 2),\n",
    "                    'middle_total': middle_total,\n",
    "                    'ratio_in_middle': round(ratio_in_middle, 2),\n",
    "                    'major_total': major_total,\n",
    "                    'ratio_in_major': round(ratio_in_major, 2),\n",
    "                    'unique_values_count': len(sorted_values),\n",
    "                    'unique_values': values_str\n",
    "                })\n",
    "        \n",
    "        # 데이터프레임 생성 및 정렬\n",
    "        df_result = pd.DataFrame(result_data)\n",
    "        df_result = df_result.sort_values(['display_category_major', 'display_category_middle', 'display_category_minor', 'key'])\n",
    "        \n",
    "        # 통계 출력\n",
    "        print(f\"\\n📊 분석 통계 (Minor 포함):\")\n",
    "        print(f\"   - 총 레코드 수: {len(df_result):,}\")\n",
    "        print(f\"   - Major 카테고리 수: {df_result['display_category_major'].nunique()}\")\n",
    "        print(f\"   - Middle 카테고리 수: {df_result['display_category_middle'].nunique()}\")\n",
    "        print(f\"   - Minor 카테고리 수: {df_result['display_category_minor'].nunique()}\")\n",
    "        print(f\"   - 고유 Key 수: {df_result['key'].nunique()}\")\n",
    "        \n",
    "        # Minor 카테고리별 통계\n",
    "        print(f\"\\n📋 Minor 카테고리별 통계:\")\n",
    "        category_stats = df_result.groupby(['display_category_major', 'display_category_middle', 'display_category_minor']).agg({\n",
    "            'key': 'count',\n",
    "            'minor_total': 'first'\n",
    "        }).rename(columns={'key': 'key_count', 'minor_total': 'product_count'}).reset_index()\n",
    "        category_stats = category_stats.sort_values('product_count', ascending=False)\n",
    "        \n",
    "        print(\"\\n상위 10개 Minor 카테고리:\")\n",
    "        for idx, row in category_stats.head(10).iterrows():\n",
    "            print(f\"   • [{row['display_category_major']}] > [{row['display_category_middle']}] > [{row['display_category_minor']}]: \"\n",
    "                  f\"{row['product_count']}개 제품, {row['key_count']}개 key\")\n",
    "        \n",
    "        # 파일로 저장 (탭 구분)\n",
    "        output_file = 'category_minor_spec_analysis.txt'\n",
    "        \n",
    "        # 저장할 컬럼 순서 정의\n",
    "        columns_order = [\n",
    "            'display_category_major', \n",
    "            'display_category_middle',\n",
    "            'display_category_minor',\n",
    "            'key', \n",
    "            'product_count',\n",
    "            'minor_total',\n",
    "            'ratio_in_minor',\n",
    "            'middle_total',\n",
    "            'ratio_in_middle',\n",
    "            'major_total', \n",
    "            'ratio_in_major',\n",
    "            'unique_values_count',\n",
    "            'unique_values'\n",
    "        ]\n",
    "        \n",
    "        # 탭으로 구분된 텍스트 파일로 저장\n",
    "        df_result[columns_order].to_csv(output_file, sep='\\t', index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"\\n💾 파일 저장 완료: {output_file}\")\n",
    "        print(f\"   - 파일 크기: {os.path.getsize(output_file) / 1024:.2f} KB\")\n",
    "        print(f\"   - 총 {len(df_result):,}개 행\")\n",
    "        print(f\"   - 엑셀에서 열 때: 텍스트 가져오기 → 탭으로 구분\")\n",
    "        \n",
    "        # 샘플 데이터 표시 (비율 포함)\n",
    "        print(f\"\\n📋 데이터 샘플 (처음 10개 행):\")\n",
    "        display_cols = ['display_category_major', 'display_category_middle', 'display_category_minor', 'key', \n",
    "                       'product_count', 'ratio_in_minor', 'ratio_in_middle', 'ratio_in_major', 'unique_values_count']\n",
    "        display(df_result[display_cols].head(10))\n",
    "        \n",
    "        # Minor 카테고리 내 비율이 높은 key들 분석\n",
    "        print(f\"\\n📈 Minor 카테고리 내 비율이 100%인 Key (모든 제품이 가진 key):\")\n",
    "        perfect_ratio = df_result[df_result['ratio_in_minor'] == 100].sort_values(['minor_total', 'display_category_minor'], ascending=[False, True])\n",
    "        for idx, row in perfect_ratio.head(15).iterrows():\n",
    "            print(f\"   • [{row['display_category_major']}] > [{row['display_category_middle']}] > [{row['display_category_minor']}] > '{row['key']}' \"\n",
    "                  f\"({row['product_count']}/{row['minor_total']})\")\n",
    "        \n",
    "        # Minor별 key 다양성 분석\n",
    "        minor_diversity = df_result.groupby(['display_category_minor']).agg({\n",
    "            'key': 'nunique',\n",
    "            'minor_total': 'first'\n",
    "        }).rename(columns={'key': 'unique_keys'}).reset_index()\n",
    "        minor_diversity = minor_diversity.sort_values('unique_keys', ascending=False)\n",
    "        \n",
    "        print(f\"\\n🎯 Key 다양성이 높은 Minor 카테고리 Top 10:\")\n",
    "        for idx, row in minor_diversity.head(10).iterrows():\n",
    "            print(f\"   • {row['display_category_minor']}: {row['unique_keys']}개 고유 key ({row['minor_total']}개 제품)\")\n",
    "        \n",
    "        # 특정 Minor 카테고리 상세 보기\n",
    "        print(f\"\\n🔍 특정 Minor 카테고리 예시 (갤럭시 S):\")\n",
    "        galaxy_s_data = df_result[df_result['display_category_minor'] == '갤럭시 S']\n",
    "        if not galaxy_s_data.empty:\n",
    "            galaxy_s_data = galaxy_s_data.sort_values('ratio_in_minor', ascending=False)\n",
    "            for idx, row in galaxy_s_data.head(5).iterrows():\n",
    "                print(f\"\\n   Key: '{row['key']}'\")\n",
    "                print(f\"   제품 수: {row['product_count']}/{row['minor_total']} (Minor: {row['ratio_in_minor']:.1f}%)\")\n",
    "                print(f\"   Middle 내 비율: {row['ratio_in_middle']:.1f}%\")\n",
    "                print(f\"   Major 내 비율: {row['ratio_in_major']:.1f}%\")\n",
    "                print(f\"   고유값 개수: {row['unique_values_count']}개\")\n",
    "                # 값이 너무 길면 일부만 표시\n",
    "                values_preview = row['unique_values']\n",
    "                if len(values_preview) > 100:\n",
    "                    values_preview = values_preview[:100] + '...'\n",
    "                print(f\"   Values: {values_preview}\")\n",
    "        \n",
    "        # Minor 카테고리가 없는 경우 분석\n",
    "        null_minor = df_result[df_result['display_category_minor'].isna() | (df_result['display_category_minor'] == '')]\n",
    "        if not null_minor.empty:\n",
    "            print(f\"\\n⚠️ Minor 카테고리가 없는 레코드: {len(null_minor)}개\")\n",
    "        \n",
    "        # 추가로 Excel 형식으로도 저장 (선택사항)\n",
    "        excel_file = 'category_minor_spec_analysis.xlsx'\n",
    "        try:\n",
    "            # unique_values가 너무 길면 Excel 셀 제한(32,767자)에 걸릴 수 있으므로 제한\n",
    "            df_excel = df_result.copy()\n",
    "            df_excel['unique_values'] = df_excel['unique_values'].apply(\n",
    "                lambda x: x[:32000] + '...(truncated)' if len(x) > 32000 else x\n",
    "            )\n",
    "            df_excel[columns_order].to_excel(excel_file, index=False, engine='openpyxl')\n",
    "            print(f\"\\n💾 Excel 파일도 저장됨: {excel_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠️ Excel 저장 실패 (파일이 너무 큼): {e}\")\n",
    "            print(f\"   → 텍스트 파일({output_file})을 사용하세요\")\n",
    "        \n",
    "        print(f\"\\n✅ 분석 완료!\")\n",
    "        print(f\"   - df_all_categories_minor_analysis: Minor까지 포함한 전체 분석 결과\")\n",
    "        \n",
    "        return df_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류 발생: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Minor 카테고리까지 포함한 분석 실행\n",
    "df_all_categories_minor_analysis = analyze_all_categories_with_minor()\n",
    "\n",
    "# 파일이 제대로 생성되었는지 확인\n",
    "if os.path.exists('category_minor_spec_analysis.txt'):\n",
    "    print(f\"\\n✅ 파일 확인: category_minor_spec_analysis.txt\")\n",
    "    print(f\"   파일 경로: {os.path.abspath('category_minor_spec_analysis.txt')}\")\n",
    "    \n",
    "    # 파일 처음 몇 줄 미리보기\n",
    "    with open('category_minor_spec_analysis.txt', 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()[:5]\n",
    "        print(f\"\\n📄 파일 미리보기 (처음 5줄):\")\n",
    "        for i, line in enumerate(lines, 1):\n",
    "            # 탭으로 구분된 필드 확인\n",
    "            fields = line.strip().split('\\t')\n",
    "            if i == 1:  # 헤더\n",
    "                print(f\"   헤더: {len(fields)}개 컬럼\")\n",
    "                print(f\"   컬럼 목록:\")\n",
    "                for j, col in enumerate(fields[:10]):  # 처음 10개 컬럼만\n",
    "                    print(f\"      {j+1}. {col}\")\n",
    "            else:\n",
    "                if len(fields) >= 7:\n",
    "                    print(f\"   {i}: [{fields[2]}] '{fields[3]}' - \"\n",
    "                          f\"{fields[6]}% in minor, {fields[8]}% in middle, {fields[10]}% in major\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381110fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993f387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_galaxy_smartphone_values.sort_values(['display_category_major','display_category_middle','key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0ebdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monitor_values.sort_values(['display_category_major','display_category_middle','key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a0df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tv_values.sort_values(['display_category_major','display_category_middle','key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e21140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PC/주변기기\t모니터\t215\n",
    "# 모바일\t갤럭시 스마트폰\t94\n",
    "# TV\tTV\t175\n",
    "\n",
    "result = df_grouped.groupby(['display_category_major', 'display_category_middle']).size().reset_index().rename(columns={0:'count'})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4671a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.sort_values(by=['count'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d877b156",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_grouped' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m pd.set_option(\u001b[33m'\u001b[39m\u001b[33mdisplay.max_columns\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;66;03m## 모든 열을 출력한다.\u001b[39;00m\n\u001b[32m      6\u001b[39m pd.set_option(\u001b[33m'\u001b[39m\u001b[33mdisplay.max_rows\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;66;03m## 모든 열을 출력한다.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mdf_grouped\u001b[49m[df_grouped[\u001b[33m'\u001b[39m\u001b[33mdisplay_category_middle\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33m갤럭시 스마트폰\u001b[39m\u001b[33m'\u001b[39m].head(\u001b[32m100\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# for idx, token in enumerate(df_grouped[df_grouped['display_category_middle'] == '갤럭시 스마트폰'].iterrows()):\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m#     print(f\"{idx} : {token[1]}\")\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df_grouped' is not defined"
     ]
    }
   ],
   "source": [
    "# PC/주변기기\t모니터\t215\n",
    "# 모바일\t갤럭시 스마트폰\t94\n",
    "# TV\tTV\t175\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None) ## 모든 열을 출력한다.\n",
    "pd.set_option('display.max_rows', None) ## 모든 열을 출력한다.\n",
    " \n",
    "df_grouped[df_grouped['display_category_middle'] == '갤럭시 스마트폰'].head(100)\n",
    "# for idx, token in enumerate(df_grouped[df_grouped['display_category_middle'] == '갤럭시 스마트폰'].iterrows()):\n",
    "#     print(f\"{idx} : {token[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab9833",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f9cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "/Users/toby/prog/kt/rubicon/data/kt_spec_validation_table_20251021.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6694ccc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc435c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
