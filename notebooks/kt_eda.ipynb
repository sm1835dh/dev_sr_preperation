{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1otlvs5st1k",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SQLAlchemy ì—”ì§„ ìƒì„± ì„±ê³µ\n",
      "\n",
      "================================================================================\n",
      "ğŸ“¥ ë°ì´í„° ë¡œë“œ ì¤‘...\n",
      "================================================================================\n",
      "âœ… í…Œì´ë¸” 'test_spec_01'ì—ì„œ 955ê°œ í–‰ ë¡œë“œ ì™„ë£Œ\n",
      "âœ… allowed_disp_nm1ë¡œ í•„í„°ë§: 625ê°œ í–‰\n",
      "\n",
      "================================================================================\n",
      "ğŸ”„ ë°ì´í„° íŒŒì‹± ì¤‘...\n",
      "================================================================================\n",
      "âœ… íŒŒì‹± ì„±ê³µ (í™•ì‹¤): 80ê°œ í–‰\n",
      "âš ï¸  íŒŒì‹± ì„±ê³µ (ì²´í¬ í•„ìš”): 1769ê°œ í–‰\n",
      "âŒ íŒŒì‹± ì‹¤íŒ¨: 5ê°œ í–‰\n",
      "ğŸ“ˆ ì „ì²´ ëŒ€ë¹„ íŒŒì‹±ë¥ : 295.8%\n",
      "\n",
      "================================================================================\n",
      "ğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘...\n",
      "================================================================================\n",
      "âœ… ì†ŒìŠ¤ í…Œì´ë¸” 'test_spec_01' ìŠ¤í‚¤ë§ˆ ì½ê¸° ì™„ë£Œ (13ê°œ ì»¬ëŸ¼)\n",
      "âœ… í…Œì´ë¸” 'test_spec_02' ìƒì„±/í™•ì¸ ì™„ë£Œ\n",
      "   ì¶”ê°€ëœ ì»¬ëŸ¼: dimension_type, parsed_value, needs_check\n",
      "âœ… í…Œì´ë¸” 'test_spec_02'ì˜ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ\n",
      "âœ… ì†ŒìŠ¤ í…Œì´ë¸” 'test_spec_01' ìŠ¤í‚¤ë§ˆ ì½ê¸° ì™„ë£Œ (13ê°œ ì»¬ëŸ¼)\n",
      "âœ… í…Œì´ë¸” 'test_spec_02'ì— 1849ê°œ í–‰ ì €ì¥ ì™„ë£Œ\n",
      "\n",
      "================================================================================\n",
      "âœ… ì „ì²´ ì‘ì—… ì™„ë£Œ!\n",
      "================================================================================\n",
      "ğŸ“Š ìš”ì•½:\n",
      "  - ì†ŒìŠ¤ í…Œì´ë¸”: test_spec_01\n",
      "  - íƒ€ê²Ÿ í…Œì´ë¸”: test_spec_02\n",
      "  - ì €ì¥ëœ ë°ì´í„°: 1849ê°œ í–‰\n",
      "  - ê¸°ì¡´ ë°ì´í„° ì‚­ì œ: ì˜ˆ\n",
      "  - íƒ€ê²Ÿ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ: ì†ŒìŠ¤ í…Œì´ë¸” ì»¬ëŸ¼ + dimension_type, parsed_value, needs_check\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ: PostgreSQLì—ì„œ ë°ì´í„° ë¡œë“œ ë° íŒŒì‹± í›„ ì €ì¥\n",
    "# ============================================\n",
    "\n",
    "# ì„¤ì •\n",
    "SOURCE_TABLE = \"test_spec_01\"  # ì†ŒìŠ¤ í…Œì´ë¸”ëª… (ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”)\n",
    "TARGET_TABLE = \"test_spec_02\"  # íƒ€ê²Ÿ í…Œì´ë¸”ëª… (ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”)\n",
    "TRUNCATE_BEFORE_INSERT = True  # True: ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ì‚½ì…, False: ê¸°ì¡´ ë°ì´í„° ìœ ì§€í•˜ê³  ì¶”ê°€\n",
    "\n",
    "# 1. SQLAlchemy ì—”ì§„ ìƒì„±\n",
    "engine = get_sqlalchemy_engine()\n",
    "\n",
    "if engine is None:\n",
    "    print(\"âŒ ì—”ì§„ ìƒì„± ì‹¤íŒ¨. ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.\")\n",
    "else:\n",
    "    # 2. ë°ì´í„° ë¡œë“œ\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ“¥ ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "    print(\"=\"*80)\n",
    "    df_filtered = load_data_from_table(engine, SOURCE_TABLE, allowed_disp_nm1)\n",
    "    \n",
    "    if df_filtered is not None and len(df_filtered) > 0:\n",
    "        # 3. ë°ì´í„° íŒŒì‹± (ì´ì „ì— ì •ì˜í•œ í•¨ìˆ˜ ì‚¬ìš©)\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ğŸ”„ ë°ì´í„° íŒŒì‹± ì¤‘...\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        parsed_data = []\n",
    "        parsed_data_needs_check = []\n",
    "        unparsed_data = []\n",
    "        \n",
    "        for idx, row in df_filtered.iterrows():\n",
    "            parsed_rows, success, needs_check = parse_dimensions_advanced(row)\n",
    "            if success and parsed_rows:\n",
    "                if needs_check:\n",
    "                    parsed_data_needs_check.extend(parsed_rows)\n",
    "                else:\n",
    "                    parsed_data.extend(parsed_rows)\n",
    "            else:\n",
    "                unparsed_data.append(row)\n",
    "        \n",
    "        df_parsed = pd.DataFrame(parsed_data)\n",
    "        df_parsed_needs_check = pd.DataFrame(parsed_data_needs_check)\n",
    "        df_unparsed = pd.DataFrame(unparsed_data)\n",
    "        \n",
    "        # íŒŒì‹± í†µê³„ ì¶œë ¥\n",
    "        total_parsed = len(df_parsed) + len(df_parsed_needs_check)\n",
    "        print(f\"âœ… íŒŒì‹± ì„±ê³µ (í™•ì‹¤): {len(df_parsed)}ê°œ í–‰\")\n",
    "        print(f\"âš ï¸  íŒŒì‹± ì„±ê³µ (ì²´í¬ í•„ìš”): {len(df_parsed_needs_check)}ê°œ í–‰\")\n",
    "        print(f\"âŒ íŒŒì‹± ì‹¤íŒ¨: {len(df_unparsed)}ê°œ í–‰\")\n",
    "        print(f\"ğŸ“ˆ ì „ì²´ ëŒ€ë¹„ íŒŒì‹±ë¥ : {(total_parsed / len(df_filtered) * 100):.1f}%\")\n",
    "        \n",
    "        # 4. PostgreSQL í…Œì´ë¸”ì— ì €ì¥\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘...\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        success = save_parsed_data_to_table(\n",
    "            engine=engine,\n",
    "            df_parsed=df_parsed,\n",
    "            df_needs_check=df_parsed_needs_check,\n",
    "            source_table_name=SOURCE_TABLE,\n",
    "            target_table_name=TARGET_TABLE,\n",
    "            truncate_before_insert=TRUNCATE_BEFORE_INSERT\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"âœ… ì „ì²´ ì‘ì—… ì™„ë£Œ!\")\n",
    "            print(\"=\"*80)\n",
    "            print(f\"ğŸ“Š ìš”ì•½:\")\n",
    "            print(f\"  - ì†ŒìŠ¤ í…Œì´ë¸”: {SOURCE_TABLE}\")\n",
    "            print(f\"  - íƒ€ê²Ÿ í…Œì´ë¸”: {TARGET_TABLE}\")\n",
    "            print(f\"  - ì €ì¥ëœ ë°ì´í„°: {total_parsed}ê°œ í–‰\")\n",
    "            print(f\"  - ê¸°ì¡´ ë°ì´í„° ì‚­ì œ: {'ì˜ˆ' if TRUNCATE_BEFORE_INSERT else 'ì•„ë‹ˆì˜¤'}\")\n",
    "            print(f\"  - íƒ€ê²Ÿ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ: ì†ŒìŠ¤ í…Œì´ë¸” ì»¬ëŸ¼ + dimension_type, parsed_value, needs_check\")\n",
    "        else:\n",
    "            print(\"\\nâŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨\")\n",
    "    else:\n",
    "        print(\"\\nâŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ë˜ëŠ” ë°ì´í„° ì—†ìŒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "oqk84734yc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text, inspect\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env íŒŒì¼ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "def get_sqlalchemy_engine():\n",
    "    \"\"\"SQLAlchemy ì—”ì§„ ìƒì„±\"\"\"\n",
    "    try:\n",
    "        connection_string = f\"postgresql://{os.getenv('PG_USER')}:{os.getenv('PG_PASSWORD')}@{os.getenv('PG_HOST')}:{os.getenv('PG_PORT')}/{os.getenv('PG_DATABASE')}\"\n",
    "        engine = create_engine(connection_string)\n",
    "        print(f\"âœ… SQLAlchemy ì—”ì§„ ìƒì„± ì„±ê³µ\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ SQLAlchemy ì—”ì§„ ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_data_from_table(engine, table_name, allowed_disp_nm1):\n",
    "    \"\"\"\n",
    "    PostgreSQL í…Œì´ë¸”ì—ì„œ ë°ì´í„° ë¡œë“œ\n",
    "    \n",
    "    Parameters:\n",
    "    - engine: SQLAlchemy engine\n",
    "    - table_name: ì†ŒìŠ¤ í…Œì´ë¸”ëª…\n",
    "    - allowed_disp_nm1: í•„í„°ë§í•  disp_nm1 ë¦¬ìŠ¤íŠ¸\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ì „ì²´ ë°ì´í„° ë¡œë“œ\n",
    "        query = f\"SELECT * FROM {table_name}\"\n",
    "        df = pd.read_sql(query, engine)\n",
    "        print(f\"âœ… í…Œì´ë¸” '{table_name}'ì—ì„œ {len(df)}ê°œ í–‰ ë¡œë“œ ì™„ë£Œ\")\n",
    "        \n",
    "        # allowed_disp_nm1ë¡œ í•„í„°ë§\n",
    "        if allowed_disp_nm1 and len(allowed_disp_nm1) > 0:\n",
    "            df_filtered = df[df['disp_nm1'].isin(allowed_disp_nm1)]\n",
    "            print(f\"âœ… allowed_disp_nm1ë¡œ í•„í„°ë§: {len(df_filtered)}ê°œ í–‰\")\n",
    "        else:\n",
    "            df_filtered = df\n",
    "            print(f\"âš ï¸  í•„í„°ë§ ì—†ì´ ì „ì²´ ë°ì´í„° ì‚¬ìš©\")\n",
    "        \n",
    "        return df_filtered\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_table_schema(engine, source_table_name):\n",
    "    \"\"\"\n",
    "    ì†ŒìŠ¤ í…Œì´ë¸”ì˜ ìŠ¤í‚¤ë§ˆë¥¼ ì½ì–´ì˜´\n",
    "    \n",
    "    Parameters:\n",
    "    - engine: SQLAlchemy engine\n",
    "    - source_table_name: ì†ŒìŠ¤ í…Œì´ë¸”ëª…\n",
    "    \n",
    "    Returns:\n",
    "    - ì»¬ëŸ¼ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    try:\n",
    "        inspector = inspect(engine)\n",
    "        columns = inspector.get_columns(source_table_name)\n",
    "        print(f\"âœ… ì†ŒìŠ¤ í…Œì´ë¸” '{source_table_name}' ìŠ¤í‚¤ë§ˆ ì½ê¸° ì™„ë£Œ ({len(columns)}ê°œ ì»¬ëŸ¼)\")\n",
    "        return columns\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ìŠ¤í‚¤ë§ˆ ì½ê¸° ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "def map_sqlalchemy_type_to_postgres(column_type):\n",
    "    \"\"\"\n",
    "    SQLAlchemy íƒ€ì…ì„ PostgreSQL íƒ€ì…ìœ¼ë¡œ ë³€í™˜\n",
    "    \"\"\"\n",
    "    type_str = str(column_type)\n",
    "    \n",
    "    # ì¼ë°˜ì ì¸ íƒ€ì… ë§¤í•‘\n",
    "    if 'INTEGER' in type_str or 'BIGINT' in type_str or 'SMALLINT' in type_str:\n",
    "        return 'INTEGER'\n",
    "    elif 'SERIAL' in type_str or 'BIGSERIAL' in type_str:\n",
    "        return 'SERIAL'\n",
    "    elif 'VARCHAR' in type_str:\n",
    "        # VARCHAR(ê¸¸ì´) ì¶”ì¶œ\n",
    "        return type_str.replace('VARCHAR', 'VARCHAR')\n",
    "    elif 'TEXT' in type_str:\n",
    "        return 'TEXT'\n",
    "    elif 'BOOLEAN' in type_str or 'BOOL' in type_str:\n",
    "        return 'BOOLEAN'\n",
    "    elif 'TIMESTAMP' in type_str:\n",
    "        return 'TIMESTAMP'\n",
    "    elif 'DATE' in type_str:\n",
    "        return 'DATE'\n",
    "    elif 'NUMERIC' in type_str or 'DECIMAL' in type_str:\n",
    "        return type_str.replace('NUMERIC', 'NUMERIC')\n",
    "    elif 'FLOAT' in type_str or 'REAL' in type_str or 'DOUBLE' in type_str:\n",
    "        return 'DOUBLE PRECISION'\n",
    "    else:\n",
    "        # ê¸°ë³¸ê°’\n",
    "        return 'TEXT'\n",
    "\n",
    "def create_parsed_table_from_source(engine, source_table_name, target_table_name):\n",
    "    \"\"\"\n",
    "    ì†ŒìŠ¤ í…Œì´ë¸”ì˜ ìŠ¤í‚¤ë§ˆë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŒŒì‹±ëœ ë°ì´í„°ë¥¼ ì €ì¥í•  í…Œì´ë¸” ìƒì„±\n",
    "    dimension_typeê³¼ parsed_value ì»¬ëŸ¼ ì¶”ê°€\n",
    "    \n",
    "    Parameters:\n",
    "    - engine: SQLAlchemy engine\n",
    "    - source_table_name: ì†ŒìŠ¤ í…Œì´ë¸”ëª…\n",
    "    - target_table_name: ìƒì„±í•  í…Œì´ë¸”ëª…\n",
    "    \"\"\"\n",
    "    # ì†ŒìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì½ê¸°\n",
    "    columns = get_table_schema(engine, source_table_name)\n",
    "    if columns is None:\n",
    "        return False\n",
    "    \n",
    "    # CREATE TABLE ì¿¼ë¦¬ ìƒì„±\n",
    "    column_definitions = []\n",
    "    \n",
    "    for col in columns:\n",
    "        col_name = col['name']\n",
    "        col_type = map_sqlalchemy_type_to_postgres(col['type'])\n",
    "        nullable = \"NULL\" if col['nullable'] else \"NOT NULL\"\n",
    "        \n",
    "        # PRIMARY KEYë‚˜ SERIAL íƒ€ì…ì€ ì œê±° (ìƒˆ í…Œì´ë¸”ì—ì„œëŠ” idë¥¼ ìƒˆë¡œ ë§Œë“¤ ê²ƒ)\n",
    "        if col.get('autoincrement') or 'primary_key' in str(col).lower():\n",
    "            continue\n",
    "            \n",
    "        column_definitions.append(f\"{col_name} {col_type}\")\n",
    "    \n",
    "    # dimension_typeê³¼ parsed_value ì¶”ê°€\n",
    "    column_definitions.append(\"dimension_type TEXT\")\n",
    "    column_definitions.append(\"parsed_value NUMERIC\")\n",
    "    column_definitions.append(\"needs_check BOOLEAN\")\n",
    "    \n",
    "    # CREATE TABLE ì¿¼ë¦¬\n",
    "    create_table_query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {target_table_name} (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        {', '.join(column_definitions)},\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(create_table_query))\n",
    "            conn.commit()\n",
    "        print(f\"âœ… í…Œì´ë¸” '{target_table_name}' ìƒì„±/í™•ì¸ ì™„ë£Œ\")\n",
    "        print(f\"   ì¶”ê°€ëœ ì»¬ëŸ¼: dimension_type, parsed_value, needs_check\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "        return False\n",
    "\n",
    "def truncate_table(engine, table_name):\n",
    "    \"\"\"\n",
    "    í…Œì´ë¸”ì˜ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ\n",
    "    \n",
    "    Parameters:\n",
    "    - engine: SQLAlchemy engine\n",
    "    - table_name: í…Œì´ë¸”ëª…\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(f\"TRUNCATE TABLE {table_name} RESTART IDENTITY CASCADE\"))\n",
    "            conn.commit()\n",
    "        print(f\"âœ… í…Œì´ë¸” '{table_name}'ì˜ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨: {e}\")\n",
    "        return False\n",
    "\n",
    "def save_parsed_data_to_table(engine, df_parsed, df_needs_check, source_table_name, target_table_name, truncate_before_insert=False):\n",
    "    \"\"\"\n",
    "    íŒŒì‹±ëœ ë°ì´í„°ë¥¼ PostgreSQL í…Œì´ë¸”ì— ì €ì¥\n",
    "    ì†ŒìŠ¤ í…Œì´ë¸”ì˜ ëª¨ë“  ì»¬ëŸ¼ + dimension_type, parsed_value, needs_check ì €ì¥\n",
    "    \n",
    "    Parameters:\n",
    "    - engine: SQLAlchemy engine\n",
    "    - df_parsed: íŒŒì‹± ì„±ê³µí•œ í™•ì‹¤í•œ ë°ì´í„°\n",
    "    - df_needs_check: íŒŒì‹± ì„±ê³µí–ˆì§€ë§Œ ì²´í¬ê°€ í•„ìš”í•œ ë°ì´í„°\n",
    "    - source_table_name: ì†ŒìŠ¤ í…Œì´ë¸”ëª…\n",
    "    - target_table_name: ëŒ€ìƒ í…Œì´ë¸”ëª…\n",
    "    - truncate_before_insert: Trueì´ë©´ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ\n",
    "    \n",
    "    Returns:\n",
    "    - ì„±ê³µ ì—¬ë¶€\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # í…Œì´ë¸” ìƒì„±\n",
    "        if not create_parsed_table_from_source(engine, source_table_name, target_table_name):\n",
    "            return False\n",
    "        \n",
    "        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì˜µì…˜\n",
    "        if truncate_before_insert:\n",
    "            if not truncate_table(engine, target_table_name):\n",
    "                return False\n",
    "        \n",
    "        # ë‘ DataFrame í•©ì¹˜ê¸°\n",
    "        df_all = pd.DataFrame()\n",
    "        \n",
    "        if len(df_parsed) > 0:\n",
    "            df_parsed_copy = df_parsed.copy()\n",
    "            df_parsed_copy['needs_check'] = False\n",
    "            df_all = pd.concat([df_all, df_parsed_copy], ignore_index=True)\n",
    "        \n",
    "        if len(df_needs_check) > 0:\n",
    "            df_needs_check_copy = df_needs_check.copy()\n",
    "            df_needs_check_copy['needs_check'] = True\n",
    "            df_all = pd.concat([df_all, df_needs_check_copy], ignore_index=True)\n",
    "        \n",
    "        if len(df_all) == 0:\n",
    "            print(\"âš ï¸  ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return True\n",
    "        \n",
    "        # dimension_typeê³¼ parsed_valueê°€ ìˆëŠ”ì§€ í™•ì¸\n",
    "        if 'dimension_type' not in df_all.columns or 'parsed_value' not in df_all.columns:\n",
    "            print(\"âŒ dimension_type ë˜ëŠ” parsed_value ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return False\n",
    "        \n",
    "        # ì†ŒìŠ¤ í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "        source_columns = get_table_schema(engine, source_table_name)\n",
    "        if source_columns is None:\n",
    "            return False\n",
    "        \n",
    "        # ì†ŒìŠ¤ ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸\n",
    "        source_column_names = [col['name'] for col in source_columns if not col.get('autoincrement')]\n",
    "        \n",
    "        # ì €ì¥í•  DataFrame êµ¬ì„±: ì†ŒìŠ¤ì˜ ëª¨ë“  ì»¬ëŸ¼ + dimension_type, parsed_value, needs_check\n",
    "        df_to_save = pd.DataFrame()\n",
    "        \n",
    "        # ì†ŒìŠ¤ í…Œì´ë¸”ì˜ ëª¨ë“  ì»¬ëŸ¼ ë³µì‚¬\n",
    "        for col_name in source_column_names:\n",
    "            if col_name in df_all.columns:\n",
    "                df_to_save[col_name] = df_all[col_name]\n",
    "        \n",
    "        # ìƒˆë¡œìš´ ì»¬ëŸ¼ ì¶”ê°€\n",
    "        df_to_save['dimension_type'] = df_all['dimension_type']\n",
    "        df_to_save['parsed_value'] = df_all['parsed_value']\n",
    "        df_to_save['needs_check'] = df_all['needs_check']\n",
    "        \n",
    "        # ë°ì´í„° ì €ì¥\n",
    "        df_to_save.to_sql(target_table_name, engine, if_exists='append', index=False)\n",
    "        print(f\"âœ… í…Œì´ë¸” '{target_table_name}'ì— {len(df_to_save)}ê°œ í–‰ ì €ì¥ ì™„ë£Œ\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "print(\"PostgreSQL ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "krftk8luw9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²´ ë°ì´í„°: 625ê°œ í–‰\n",
      "íŒŒì‹± ì¤‘...\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š íŒŒì‹± ê²°ê³¼ í†µê³„\n",
      "================================================================================\n",
      "âœ… íŒŒì‹± ì„±ê³µ (í™•ì‹¤): 80ê°œ í–‰\n",
      "âš ï¸  íŒŒì‹± ì„±ê³µ (ì²´í¬ í•„ìš”): 1769ê°œ í–‰\n",
      "âŒ íŒŒì‹± ì‹¤íŒ¨: 5ê°œ í–‰\n",
      "ğŸ“ˆ ì „ì²´ ëŒ€ë¹„ íŒŒì‹±ë¥ : 295.8%\n",
      "================================================================================\n",
      "\n",
      "âœ… íŒŒì‹± ì„±ê³µ ë°ì´í„° - í™•ì‹¤ (ì²˜ìŒ 20ê°œ):\n",
      "--------------------------------------------------------------------------------\n",
      "   disp_nm1         disp_nm2 dimension_type  parsed_value                                 value\n",
      "0        ê·œê²©               í¬ê¸°          width         276.0           276(W) x 327(H) x 293(D) mm\n",
      "1        ê·œê²©               í¬ê¸°         height         327.0           276(W) x 327(H) x 293(D) mm\n",
      "2        ê·œê²©               í¬ê¸°          depth         293.0           276(W) x 327(H) x 293(D) mm\n",
      "3        ê·œê²©               í¬ê¸°         height         216.0        216mm(H) x 790mm(W) x 287mm(D)\n",
      "4        ê·œê²©               í¬ê¸°          width         790.0        216mm(H) x 790mm(W) x 287mm(D)\n",
      "5        ê·œê²©               í¬ê¸°          depth         287.0        216mm(H) x 790mm(W) x 287mm(D)\n",
      "6        ê·œê²©               í¬ê¸°          width         820.0          820 x 56 x103.5 mm(ê°€ë¡œxë†’ì´xê¹Šì´)\n",
      "7        ê·œê²©               í¬ê¸°         height          56.0          820 x 56 x103.5 mm(ê°€ë¡œxë†’ì´xê¹Šì´)\n",
      "8        ê·œê²©               í¬ê¸°          depth         103.5          820 x 56 x103.5 mm(ê°€ë¡œxë†’ì´xê¹Šì´)\n",
      "9        ì™¸ê´€  ì œí’ˆ í¬ê¸°(ë³¸ì²´,WxHxD)          depth         165.0                           âˆ…165 Â± 5 mm\n",
      "10       ê·œê²©               í¬ê¸°          width         399.0        399mm(W) x 905mm(H) x 436mm(D)\n",
      "11       ê·œê²©               í¬ê¸°         height         905.0        399mm(W) x 905mm(H) x 436mm(D)\n",
      "12       ê·œê²©               í¬ê¸°          depth         436.0        399mm(W) x 905mm(H) x 436mm(D)\n",
      "13       ê·œê²©               í¬ê¸°          width          89.0   89 x 210 x 176 mm (ê°€ë¡œxë†’ì´xê¹Šì´, ì¢Œìš° ê°ê°)\n",
      "14       ê·œê²©               í¬ê¸°         height         210.0   89 x 210 x 176 mm (ê°€ë¡œxë†’ì´xê¹Šì´, ì¢Œìš° ê°ê°)\n",
      "15       ê·œê²©               í¬ê¸°          depth         176.0   89 x 210 x 176 mm (ê°€ë¡œxë†’ì´xê¹Šì´, ì¢Œìš° ê°ê°)\n",
      "16       ê·œê²©               í¬ê¸°         height         460.4  460.4mm(H) x 596.9mm(W) x 288.9mm(D)\n",
      "17       ê·œê²©               í¬ê¸°          width         596.9  460.4mm(H) x 596.9mm(W) x 288.9mm(D)\n",
      "18       ê·œê²©               í¬ê¸°          depth         288.9  460.4mm(H) x 596.9mm(W) x 288.9mm(D)\n",
      "19       ê·œê²©               í¬ê¸°         height         460.4  460.4mm(H) x 596.9mm(W) x 288.9mm(D)\n",
      "\n",
      "\n",
      "âš ï¸  íŒŒì‹± ì„±ê³µ ë°ì´í„° - ì²´í¬ í•„ìš” (ë‹¨ìœ„ ëª…ì‹œ ì—†ìŒ, ì²˜ìŒ 20ê°œ):\n",
      "--------------------------------------------------------------------------------\n",
      "   disp_nm1             disp_nm2 dimension_type  parsed_value                   value\n",
      "0        ê·œê²©         í¬ê¸°(ê°€ë¡œXê¹Šì´Xë†’ì´)          width          78.0  78 x 115.7 x 12.8 (mm)\n",
      "1        ê·œê²©         í¬ê¸°(ê°€ë¡œXê¹Šì´Xë†’ì´)         height         115.7  78 x 115.7 x 12.8 (mm)\n",
      "2        ê·œê²©         í¬ê¸°(ê°€ë¡œXê¹Šì´Xë†’ì´)          depth          12.8  78 x 115.7 x 12.8 (mm)\n",
      "3        ì‚¬ì–‘  ì œí’ˆ í¬ê¸°(ê°€ë¡œ Ã— ë†’ì´ Ã— ê¹Šì´)          width         360.0      360 x 783 x 318 mm\n",
      "4        ì‚¬ì–‘  ì œí’ˆ í¬ê¸°(ê°€ë¡œ Ã— ë†’ì´ Ã— ê¹Šì´)         height         783.0      360 x 783 x 318 mm\n",
      "5        ì‚¬ì–‘  ì œí’ˆ í¬ê¸°(ê°€ë¡œ Ã— ë†’ì´ Ã— ê¹Šì´)          depth         318.0      360 x 783 x 318 mm\n",
      "6        ì‚¬ì–‘  ì œí’ˆ í¬ê¸°(ê°€ë¡œ Ã— ë†’ì´ Ã— ê¹Šì´)          width         360.0      360 x 783 x 318 mm\n",
      "7        ì‚¬ì–‘  ì œí’ˆ í¬ê¸°(ê°€ë¡œ Ã— ë†’ì´ Ã— ê¹Šì´)         height         783.0      360 x 783 x 318 mm\n",
      "8        ì‚¬ì–‘  ì œí’ˆ í¬ê¸°(ê°€ë¡œ Ã— ë†’ì´ Ã— ê¹Šì´)          depth         318.0      360 x 783 x 318 mm\n",
      "9        ì‚¬ì–‘  ì œí’ˆ í¬ê¸°(ê°€ë¡œ Ã— ë†’ì´ Ã— ê¹Šì´)          width         349.0      349 x 499 x 236 mm\n",
      "10       ì‚¬ì–‘  ì œí’ˆ í¬ê¸°(ê°€ë¡œ Ã— ë†’ì´ Ã— ê¹Šì´)         height         499.0      349 x 499 x 236 mm\n",
      "11       ì‚¬ì–‘  ì œí’ˆ í¬ê¸°(ê°€ë¡œ Ã— ë†’ì´ Ã— ê¹Šì´)          depth         236.0      349 x 499 x 236 mm\n",
      "12       ì‚¬ì–‘  ì œí’ˆ í¬ê¸°(ê°€ë¡œ Ã— ë†’ì´ Ã— ê¹Šì´)          width         320.0      320 Ã— 545 Ã— 320 mm\n",
      "13       ì‚¬ì–‘  ì œí’ˆ í¬ê¸°(ê°€ë¡œ Ã— ë†’ì´ Ã— ê¹Šì´)         height         545.0      320 Ã— 545 Ã— 320 mm\n",
      "14       ì‚¬ì–‘  ì œí’ˆ í¬ê¸°(ê°€ë¡œ Ã— ë†’ì´ Ã— ê¹Šì´)          depth         320.0      320 Ã— 545 Ã— 320 mm\n",
      "15       ì‚¬ì–‘  ì œí’ˆ í¬ê¸°(ê°€ë¡œ Ã— ë†’ì´ Ã— ê¹Šì´)          width         320.0     320 Ã— 1070 Ã— 320 mm\n",
      "16       ì‚¬ì–‘  ì œí’ˆ í¬ê¸°(ê°€ë¡œ Ã— ë†’ì´ Ã— ê¹Šì´)         height        1070.0     320 Ã— 1070 Ã— 320 mm\n",
      "17       ì‚¬ì–‘  ì œí’ˆ í¬ê¸°(ê°€ë¡œ Ã— ë†’ì´ Ã— ê¹Šì´)          depth         320.0     320 Ã— 1070 Ã— 320 mm\n",
      "18       ì‚¬ì–‘  ì œí’ˆ í¬ê¸°(ê°€ë¡œ Ã— ë†’ì´ Ã— ê¹Šì´)          width         320.0     320 Ã— 1070 Ã— 320 mm\n",
      "19       ì‚¬ì–‘  ì œí’ˆ í¬ê¸°(ê°€ë¡œ Ã— ë†’ì´ Ã— ê¹Šì´)         height        1070.0     320 Ã— 1070 Ã— 320 mm\n",
      "\n",
      "\n",
      "âŒ íŒŒì‹± ì‹¤íŒ¨ ë°ì´í„° (ì²˜ìŒ 20ê°œ):\n",
      "--------------------------------------------------------------------------------\n",
      "    disp_nm1      disp_nm2                                                               value\n",
      "296       ê·œê²©            í¬ê¸°  íƒ€ì›Œ : 320mm x 1769mm(ê° 7.6kg), ì„œë¸Œìš°í¼ : 296mm x 400mm x 296mm(14.6kg)\n",
      "727    ê¸°ë³¸ ì‚¬ì–‘  ì œí’ˆí¬ê¸° (WxDxH)                                                                 NaN\n",
      "728    ê¸°ë³¸ ì‚¬ì–‘  ì œí’ˆí¬ê¸° (WxDxH)                                                                 NaN\n",
      "729    ê¸°ë³¸ ì‚¬ì–‘  ì œí’ˆí¬ê¸° (WxDxH)                                                                 NaN\n",
      "730    ê¸°ë³¸ ì‚¬ì–‘  ì œí’ˆí¬ê¸° (WxDxH)                                                                 NaN\n",
      "\n",
      "\n",
      "âŒ íŒŒì‹± ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„ (disp_nm2ë³„ ê°œìˆ˜):\n",
      "--------------------------------------------------------------------------------\n",
      "disp_nm2\n",
      "ì œí’ˆí¬ê¸° (WxDxH)    4\n",
      "í¬ê¸°              1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def identify_dimension_type(text):\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ì—ì„œ dimension íƒ€ì…ì„ ì‹ë³„\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Depth í‚¤ì›Œë“œ\n",
    "    if any(keyword in text_lower for keyword in ['ë‘ê»˜', 'ê¹Šì´', 'd']):\n",
    "        return 'depth'\n",
    "    # Width í‚¤ì›Œë“œ\n",
    "    elif any(keyword in text_lower for keyword in ['ê°€ë¡œ', 'í­', 'w']):\n",
    "        return 'width'\n",
    "    # Height í‚¤ì›Œë“œ\n",
    "    elif any(keyword in text_lower for keyword in ['ì„¸ë¡œ', 'ë†’ì´', 'h']):\n",
    "        return 'height'\n",
    "    \n",
    "    return None\n",
    "\n",
    "def parse_dimensions_advanced(row):\n",
    "    \"\"\"\n",
    "    disp_nm2ì— ë”°ë¼ valueë¥¼ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜ (í™•ì¥ ë²„ì „)\n",
    "    \"\"\"\n",
    "    parsed_rows = []\n",
    "    value = str(row['value'])\n",
    "    disp_nm2 = str(row['disp_nm2'])\n",
    "    needs_check = False  # ë‹¨ìœ„ê°€ ëª…í™•í•˜ì§€ ì•Šì•„ ì²´í¬ê°€ í•„ìš”í•œ ê²½ìš°\n",
    "    \n",
    "    # íŒ¨í„´ 0: Wìˆ«ì x Dìˆ«ì x Hìˆ«ì í˜•ì‹ (ì˜ˆ: \"W269 x D375 x H269 mm\")\n",
    "    wdh_pattern = r'([WwHhDd])\\s*(\\d+(?:\\.\\d+)?)'\n",
    "    wdh_matches = re.findall(wdh_pattern, value)\n",
    "    \n",
    "    if len(wdh_matches) >= 2:  # ìµœì†Œ 2ê°œ ì´ìƒì˜ dimensionì´ ìˆëŠ” ê²½ìš°\n",
    "        base_row = row.to_dict()\n",
    "        dimension_map = {'w': 'width', 'h': 'height', 'd': 'depth'}\n",
    "        \n",
    "        for dim_letter, num_val in wdh_matches:\n",
    "            dim_type = dimension_map.get(dim_letter.lower())\n",
    "            if dim_type:\n",
    "                new_row = base_row.copy()\n",
    "                new_row['dimension_type'] = dim_type\n",
    "                new_row['parsed_value'] = float(num_val)\n",
    "                new_row['needs_check'] = False\n",
    "                parsed_rows.append(new_row)\n",
    "        \n",
    "        if parsed_rows:\n",
    "            return parsed_rows, True, False\n",
    "    \n",
    "    # íŒ¨í„´ 1: valueì— ìˆ«ì(W), ìˆ«ì(H), ìˆ«ì(D)ê°€ ëª…ì‹œëœ ê²½ìš° (ì˜ˆ: \"276(W) x 327(H) x 293(D) mm\", \"178(W) x 68(H) x 72(D) mm\")\n",
    "    # ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›: ìˆ«ì(W), ìˆ«ìmm(W), W:ìˆ«ì ë“±\n",
    "    whd_pattern = r'(\\d+(?:\\.\\d+)?)\\s*(?:mm)?\\s*\\(?\\s*([WwHhDd])\\s*\\)?'\n",
    "    whd_matches = re.findall(whd_pattern, value)\n",
    "    \n",
    "    if len(whd_matches) >= 2:  # ìµœì†Œ 2ê°œ ì´ìƒì˜ dimensionì´ ìˆëŠ” ê²½ìš°\n",
    "        base_row = row.to_dict()\n",
    "        dimension_map = {'w': 'width', 'h': 'height', 'd': 'depth'}\n",
    "        \n",
    "        for num_val, dim_letter in whd_matches:\n",
    "            dim_type = dimension_map.get(dim_letter.lower())\n",
    "            if dim_type:\n",
    "                new_row = base_row.copy()\n",
    "                new_row['dimension_type'] = dim_type\n",
    "                new_row['parsed_value'] = float(num_val)\n",
    "                new_row['needs_check'] = False\n",
    "                parsed_rows.append(new_row)\n",
    "        \n",
    "        if parsed_rows:\n",
    "            return parsed_rows, True, False\n",
    "    \n",
    "    # íŒ¨í„´ 2: \"ê°€ë¡œxë†’ì´xê¹Šì´\" í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš° (ì˜ˆ: \"820 x 56 x103.5 mm(ê°€ë¡œxë†’ì´xê¹Šì´)\")\n",
    "    if 'ê°€ë¡œ' in value and 'ë†’ì´' in value and 'ê¹Šì´' in value:\n",
    "        nums = re.findall(r'(\\d+(?:\\.\\d+)?)', value)\n",
    "        if len(nums) >= 3:\n",
    "            base_row = row.to_dict()\n",
    "            \n",
    "            # ê°€ë¡œ (width)\n",
    "            row1 = base_row.copy()\n",
    "            row1['dimension_type'] = 'width'\n",
    "            row1['parsed_value'] = float(nums[0])\n",
    "            row1['needs_check'] = False\n",
    "            parsed_rows.append(row1)\n",
    "            \n",
    "            # ë†’ì´ (height)\n",
    "            row2 = base_row.copy()\n",
    "            row2['dimension_type'] = 'height'\n",
    "            row2['parsed_value'] = float(nums[1])\n",
    "            row2['needs_check'] = False\n",
    "            parsed_rows.append(row2)\n",
    "            \n",
    "            # ê¹Šì´ (depth)\n",
    "            row3 = base_row.copy()\n",
    "            row3['dimension_type'] = 'depth'\n",
    "            row3['parsed_value'] = float(nums[2])\n",
    "            row3['needs_check'] = False\n",
    "            parsed_rows.append(row3)\n",
    "            \n",
    "            return parsed_rows, True, False\n",
    "    \n",
    "    # íŒ¨í„´ 3: WxHxD í˜•ì‹ (xë¡œ êµ¬ë¶„, ë‹¨ìœ„ ëª…ì‹œ ì—†ìŒ) (ì˜ˆ: \"180 x 70 x 72 mm\", \"223 x 96.5 x 94 mm\")\n",
    "    wxhxd_match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*[xXÃ—]\\s*(\\d+(?:\\.\\d+)?)\\s*[xXÃ—]\\s*(\\d+(?:\\.\\d+)?)', value)\n",
    "    if wxhxd_match:\n",
    "        val1, val2, val3 = wxhxd_match.groups()\n",
    "        base_row = row.to_dict()\n",
    "        \n",
    "        # ê¸°ë³¸ ê°€ì •: ê°€ë¡œ x ë†’ì´ x ê¹Šì´\n",
    "        dimensions = [\n",
    "            ('width', val1),\n",
    "            ('height', val2),\n",
    "            ('depth', val3)\n",
    "        ]\n",
    "        \n",
    "        for dim_type, val in dimensions:\n",
    "            new_row = base_row.copy()\n",
    "            new_row['dimension_type'] = dim_type\n",
    "            new_row['parsed_value'] = float(val)\n",
    "            new_row['needs_check'] = True  # ë‹¨ìœ„ê°€ ëª…í™•í•˜ì§€ ì•ŠìŒ\n",
    "            parsed_rows.append(new_row)\n",
    "        \n",
    "        return parsed_rows, True, True\n",
    "    \n",
    "    # íŒ¨í„´ 4: WxH í˜•ì‹ (ì˜ˆ: \"500x600 mm\")\n",
    "    wxh_match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*[xXÃ—]\\s*(\\d+(?:\\.\\d+)?)', value)\n",
    "    if wxh_match:\n",
    "        val1, val2 = wxh_match.groups()\n",
    "        base_row = row.to_dict()\n",
    "        \n",
    "        # ê¸°ë³¸ ê°€ì •: ê°€ë¡œ x ë†’ì´\n",
    "        dimensions = [\n",
    "            ('width', val1),\n",
    "            ('height', val2)\n",
    "        ]\n",
    "        \n",
    "        for dim_type, val in dimensions:\n",
    "            new_row = base_row.copy()\n",
    "            new_row['dimension_type'] = dim_type\n",
    "            new_row['parsed_value'] = float(val)\n",
    "            new_row['needs_check'] = True  # ë‹¨ìœ„ê°€ ëª…í™•í•˜ì§€ ì•ŠìŒ\n",
    "            parsed_rows.append(new_row)\n",
    "        \n",
    "        return parsed_rows, True, True\n",
    "    \n",
    "    # íŒ¨í„´ 5: ë‹¨ì¼ ê°’ (disp_nm2ì—ì„œ dimension íƒ€ì… ì‹ë³„)\n",
    "    single_match = re.search(r'(\\d+(?:\\.\\d+)?)', value)\n",
    "    if single_match:\n",
    "        dim_type = identify_dimension_type(disp_nm2)\n",
    "        if dim_type:\n",
    "            base_row = row.to_dict()\n",
    "            base_row['dimension_type'] = dim_type\n",
    "            base_row['parsed_value'] = float(single_match.group(1))\n",
    "            base_row['needs_check'] = False\n",
    "            parsed_rows.append(base_row)\n",
    "            return parsed_rows, True, False\n",
    "    \n",
    "    return parsed_rows, False, False\n",
    "\n",
    "# í•„í„°ë§ëœ ë°ì´í„°ì˜ ëª¨ë“  í–‰ì— ëŒ€í•´ íŒŒì‹± ì‹œë„\n",
    "parsed_data = []\n",
    "parsed_data_needs_check = []\n",
    "unparsed_data = []\n",
    "\n",
    "print(f\"ì „ì²´ ë°ì´í„°: {len(df_filtered)}ê°œ í–‰\")\n",
    "print(\"íŒŒì‹± ì¤‘...\\n\")\n",
    "\n",
    "for idx, row in df_filtered.iterrows():\n",
    "    parsed_rows, success, needs_check = parse_dimensions_advanced(row)\n",
    "    if success and parsed_rows:\n",
    "        if needs_check:\n",
    "            parsed_data_needs_check.extend(parsed_rows)\n",
    "        else:\n",
    "            parsed_data.extend(parsed_rows)\n",
    "    else:\n",
    "        unparsed_data.append(row)\n",
    "\n",
    "# ìƒˆë¡œìš´ DataFrame ìƒì„±\n",
    "df_parsed = pd.DataFrame(parsed_data)\n",
    "df_parsed_needs_check = pd.DataFrame(parsed_data_needs_check)\n",
    "df_unparsed = pd.DataFrame(unparsed_data)\n",
    "\n",
    "# ê²°ê³¼ í†µê³„\n",
    "total_parsed = len(df_parsed) + len(df_parsed_needs_check)\n",
    "total_original_rows = len(df_filtered)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“Š íŒŒì‹± ê²°ê³¼ í†µê³„\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"âœ… íŒŒì‹± ì„±ê³µ (í™•ì‹¤): {len(df_parsed)}ê°œ í–‰\")\n",
    "print(f\"âš ï¸  íŒŒì‹± ì„±ê³µ (ì²´í¬ í•„ìš”): {len(df_parsed_needs_check)}ê°œ í–‰\")\n",
    "print(f\"âŒ íŒŒì‹± ì‹¤íŒ¨: {len(df_unparsed)}ê°œ í–‰\")\n",
    "print(f\"ğŸ“ˆ ì „ì²´ ëŒ€ë¹„ íŒŒì‹±ë¥ : {(total_parsed / total_original_rows * 100):.1f}%\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# íŒŒì‹± ì„±ê³µí•œ ë°ì´í„° ì¶œë ¥ (í™•ì‹¤í•œ ê²ƒ)\n",
    "if len(df_parsed) > 0:\n",
    "    print(\"\\nâœ… íŒŒì‹± ì„±ê³µ ë°ì´í„° - í™•ì‹¤ (ì²˜ìŒ 20ê°œ):\")\n",
    "    print(\"-\" * 80)\n",
    "    display_cols = ['disp_nm1', 'disp_nm2', 'disp_nm3', 'disp_nm4', 'dimension_type', 'parsed_value', 'value']\n",
    "    available_cols = [col for col in display_cols if col in df_parsed.columns]\n",
    "    print(df_parsed[available_cols].head(20).to_string())\n",
    "else:\n",
    "    print(\"\\ní™•ì‹¤í•˜ê²Œ íŒŒì‹±ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# íŒŒì‹± ì„±ê³µí–ˆì§€ë§Œ ì²´í¬ê°€ í•„ìš”í•œ ë°ì´í„° ì¶œë ¥\n",
    "if len(df_parsed_needs_check) > 0:\n",
    "    print(\"\\n\\nâš ï¸  íŒŒì‹± ì„±ê³µ ë°ì´í„° - ì²´í¬ í•„ìš” (ë‹¨ìœ„ ëª…ì‹œ ì—†ìŒ, ì²˜ìŒ 20ê°œ):\")\n",
    "    print(\"-\" * 80)\n",
    "    display_cols = ['disp_nm1', 'disp_nm2', 'disp_nm3', 'disp_nm4', 'dimension_type', 'parsed_value', 'value']\n",
    "    available_cols = [col for col in display_cols if col in df_parsed_needs_check.columns]\n",
    "    print(df_parsed_needs_check[available_cols].head(20).to_string())\n",
    "else:\n",
    "    print(\"\\nì²´í¬ê°€ í•„ìš”í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# íŒŒì‹± ì‹¤íŒ¨í•œ ë°ì´í„° ì¶œë ¥\n",
    "if len(df_unparsed) > 0:\n",
    "    print(\"\\n\\nâŒ íŒŒì‹± ì‹¤íŒ¨ ë°ì´í„° (ì²˜ìŒ 20ê°œ):\")\n",
    "    print(\"-\" * 80)\n",
    "    display_cols = ['disp_nm1', 'disp_nm2', 'disp_nm3', 'disp_nm4', 'value']\n",
    "    available_cols = [col for col in display_cols if col in df_unparsed.columns]\n",
    "    print(df_unparsed[available_cols].head(20).to_string())\n",
    "    \n",
    "    # íŒŒì‹± ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„\n",
    "    print(\"\\n\\nâŒ íŒŒì‹± ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„ (disp_nm2ë³„ ê°œìˆ˜):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(df_unparsed['disp_nm2'].value_counts().head(10))\n",
    "else:\n",
    "    print(\"\\n\\nëª¨ë“  ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ íŒŒì‹±ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace15ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mdl_code</th>\n",
       "      <th>goods_nm</th>\n",
       "      <th>disp_lv1</th>\n",
       "      <th>disp_lv2</th>\n",
       "      <th>disp_lv3</th>\n",
       "      <th>category_lv1</th>\n",
       "      <th>category_lv2</th>\n",
       "      <th>category_lv3</th>\n",
       "      <th>disp_nm1</th>\n",
       "      <th>disp_nm2</th>\n",
       "      <th>value</th>\n",
       "      <th>is_numeric</th>\n",
       "      <th>symbols</th>\n",
       "      <th>dimension_type</th>\n",
       "      <th>parsed_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VS90F40CRG</td>\n",
       "      <td>Bespoke AI ì œíŠ¸ 400W ì¹¨êµ¬ë¸ŒëŸ¬ì‹œ íŒ¨í‚¤ì§€</td>\n",
       "      <td>ë¦¬ë¹™ê°€ì „</td>\n",
       "      <td>ì²­ì†Œê¸°</td>\n",
       "      <td>BESPOKE AI ì œíŠ¸</td>\n",
       "      <td>ì§„ê³µì²­ì†Œê¸°</td>\n",
       "      <td>VC STICK</td>\n",
       "      <td>Bespoke Jet AI</td>\n",
       "      <td>ì™¸ê´€</td>\n",
       "      <td>ì œí’ˆ í¬ê¸°(ë³¸ì²´,WxHxD)</td>\n",
       "      <td>250x970x243 mm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mm</td>\n",
       "      <td>width</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VS90F40CRG</td>\n",
       "      <td>Bespoke AI ì œíŠ¸ 400W ì¹¨êµ¬ë¸ŒëŸ¬ì‹œ íŒ¨í‚¤ì§€</td>\n",
       "      <td>ë¦¬ë¹™ê°€ì „</td>\n",
       "      <td>ì²­ì†Œê¸°</td>\n",
       "      <td>BESPOKE AI ì œíŠ¸</td>\n",
       "      <td>ì§„ê³µì²­ì†Œê¸°</td>\n",
       "      <td>VC STICK</td>\n",
       "      <td>Bespoke Jet AI</td>\n",
       "      <td>ì™¸ê´€</td>\n",
       "      <td>ì œí’ˆ í¬ê¸°(ë³¸ì²´,WxHxD)</td>\n",
       "      <td>250x970x243 mm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mm</td>\n",
       "      <td>height</td>\n",
       "      <td>970.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VS90F40CRG</td>\n",
       "      <td>Bespoke AI ì œíŠ¸ 400W ì¹¨êµ¬ë¸ŒëŸ¬ì‹œ íŒ¨í‚¤ì§€</td>\n",
       "      <td>ë¦¬ë¹™ê°€ì „</td>\n",
       "      <td>ì²­ì†Œê¸°</td>\n",
       "      <td>BESPOKE AI ì œíŠ¸</td>\n",
       "      <td>ì§„ê³µì²­ì†Œê¸°</td>\n",
       "      <td>VC STICK</td>\n",
       "      <td>Bespoke Jet AI</td>\n",
       "      <td>ì™¸ê´€</td>\n",
       "      <td>ì œí’ˆ í¬ê¸°(ë³¸ì²´,WxHxD)</td>\n",
       "      <td>250x970x243 mm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mm</td>\n",
       "      <td>depth</td>\n",
       "      <td>243.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VS90F40CNG</td>\n",
       "      <td>Bespoke AI ì œíŠ¸ 400W í«ë¸ŒëŸ¬ì‹œ íŒ¨í‚¤ì§€</td>\n",
       "      <td>ë¦¬ë¹™ê°€ì „</td>\n",
       "      <td>ì²­ì†Œê¸°</td>\n",
       "      <td>BESPOKE AI ì œíŠ¸</td>\n",
       "      <td>ì§„ê³µì²­ì†Œê¸°</td>\n",
       "      <td>VC STICK</td>\n",
       "      <td>Bespoke Jet AI</td>\n",
       "      <td>ì™¸ê´€</td>\n",
       "      <td>ì œí’ˆ í¬ê¸°(ë³¸ì²´,WxHxD)</td>\n",
       "      <td>250x970x243 mm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mm</td>\n",
       "      <td>width</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VS90F40CNG</td>\n",
       "      <td>Bespoke AI ì œíŠ¸ 400W í«ë¸ŒëŸ¬ì‹œ íŒ¨í‚¤ì§€</td>\n",
       "      <td>ë¦¬ë¹™ê°€ì „</td>\n",
       "      <td>ì²­ì†Œê¸°</td>\n",
       "      <td>BESPOKE AI ì œíŠ¸</td>\n",
       "      <td>ì§„ê³µì²­ì†Œê¸°</td>\n",
       "      <td>VC STICK</td>\n",
       "      <td>Bespoke Jet AI</td>\n",
       "      <td>ì™¸ê´€</td>\n",
       "      <td>ì œí’ˆ í¬ê¸°(ë³¸ì²´,WxHxD)</td>\n",
       "      <td>250x970x243 mm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mm</td>\n",
       "      <td>height</td>\n",
       "      <td>970.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mdl_code                      goods_nm disp_lv1 disp_lv2       disp_lv3  \\\n",
       "0  VS90F40CRG  Bespoke AI ì œíŠ¸ 400W ì¹¨êµ¬ë¸ŒëŸ¬ì‹œ íŒ¨í‚¤ì§€     ë¦¬ë¹™ê°€ì „      ì²­ì†Œê¸°  BESPOKE AI ì œíŠ¸   \n",
       "1  VS90F40CRG  Bespoke AI ì œíŠ¸ 400W ì¹¨êµ¬ë¸ŒëŸ¬ì‹œ íŒ¨í‚¤ì§€     ë¦¬ë¹™ê°€ì „      ì²­ì†Œê¸°  BESPOKE AI ì œíŠ¸   \n",
       "2  VS90F40CRG  Bespoke AI ì œíŠ¸ 400W ì¹¨êµ¬ë¸ŒëŸ¬ì‹œ íŒ¨í‚¤ì§€     ë¦¬ë¹™ê°€ì „      ì²­ì†Œê¸°  BESPOKE AI ì œíŠ¸   \n",
       "3  VS90F40CNG   Bespoke AI ì œíŠ¸ 400W í«ë¸ŒëŸ¬ì‹œ íŒ¨í‚¤ì§€     ë¦¬ë¹™ê°€ì „      ì²­ì†Œê¸°  BESPOKE AI ì œíŠ¸   \n",
       "4  VS90F40CNG   Bespoke AI ì œíŠ¸ 400W í«ë¸ŒëŸ¬ì‹œ íŒ¨í‚¤ì§€     ë¦¬ë¹™ê°€ì „      ì²­ì†Œê¸°  BESPOKE AI ì œíŠ¸   \n",
       "\n",
       "  category_lv1 category_lv2    category_lv3 disp_nm1         disp_nm2  \\\n",
       "0        ì§„ê³µì²­ì†Œê¸°     VC STICK  Bespoke Jet AI       ì™¸ê´€  ì œí’ˆ í¬ê¸°(ë³¸ì²´,WxHxD)   \n",
       "1        ì§„ê³µì²­ì†Œê¸°     VC STICK  Bespoke Jet AI       ì™¸ê´€  ì œí’ˆ í¬ê¸°(ë³¸ì²´,WxHxD)   \n",
       "2        ì§„ê³µì²­ì†Œê¸°     VC STICK  Bespoke Jet AI       ì™¸ê´€  ì œí’ˆ í¬ê¸°(ë³¸ì²´,WxHxD)   \n",
       "3        ì§„ê³µì²­ì†Œê¸°     VC STICK  Bespoke Jet AI       ì™¸ê´€  ì œí’ˆ í¬ê¸°(ë³¸ì²´,WxHxD)   \n",
       "4        ì§„ê³µì²­ì†Œê¸°     VC STICK  Bespoke Jet AI       ì™¸ê´€  ì œí’ˆ í¬ê¸°(ë³¸ì²´,WxHxD)   \n",
       "\n",
       "            value  is_numeric symbols dimension_type  parsed_value  \n",
       "0  250x970x243 mm         NaN      mm          width         250.0  \n",
       "1  250x970x243 mm         NaN      mm         height         970.0  \n",
       "2  250x970x243 mm         NaN      mm          depth         243.0  \n",
       "3  250x970x243 mm         NaN      mm          width         250.0  \n",
       "4  250x970x243 mm         NaN      mm         height         970.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import os\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env íŒŒì¼ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")\n",
    "\n",
    "# 2. PostgreSQL ì—°ê²° ì„¤ì •\n",
    "def get_db_connection():\n",
    "    \"\"\"PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=os.getenv('PG_HOST'),\n",
    "            port=os.getenv('PG_PORT'),\n",
    "            database=os.getenv('PG_DATABASE'),\n",
    "            user=os.getenv('PG_USER'),\n",
    "            password=os.getenv('PG_PASSWORD')\n",
    "        )\n",
    "        print(f\"âœ… PostgreSQL ì—°ê²° ì„±ê³µ: {os.getenv('PG_HOST')}\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ PostgreSQL ì—°ê²° ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "# ì—°ê²° í…ŒìŠ¤íŠ¸\n",
    "conn = get_db_connection()\n",
    "if conn:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8h0wlsefcsi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disp_nm2ì˜ unique ê°’ë“¤:\n",
      "['í¬ê¸°' 'í¬ê¸°(ê°€ë¡œXê¹Šì´Xë†’ì´)' 'ì œí’ˆ í¬ê¸°(ê°€ë¡œ Ã— ë†’ì´ Ã— ê¹Šì´)' 'í¬ê¸°(ì„¸ë¡œxê°€ë¡œxë‘ê»˜, mm)'\n",
      " 'í¬ê¸°(í­ Ã— ë†’ì´ Ã— ê¹Šì´)' 'ì œí’ˆ í¬ê¸°(ë³¸ì²´,WxHxD)' 'ì œí’ˆí¬ê¸° (ê°€ë¡œxë†’ì´xê¹Šì´)' 'ì œí’ˆí¬ê¸°'\n",
      " 'ì œí’ˆí¬ê¸° (WxDxH)' 'ì œí’ˆí¬ê¸° (ê°€ë¡œxì„¸ë¡œxë†’ì´)' 'í¬ê¸° (ê°€ë¡œxë†’ì´xê¹Šì´)' 'í¬ê¸°(ê°€ë¡œxë†’ì´xê¹Šì´)'\n",
      " 'í¬ê¸°(ì„¸ë¡œxê°€ë¡œxë‘ê»˜)' 'í¬ê¸°(WÃ—HÃ—D)' 'ì œí’ˆí¬ê¸° (ê°€ë¡œxë†’ì´x ê¹Šì´)' 'í¬ê¸°(ê°€ë¡œxê¹Šì´xë†’ì´)']\n",
      "\n",
      "ì´ 16ê°œì˜ unique ê°’\n"
     ]
    }
   ],
   "source": [
    "# allowed_disp_nm1ì— ìˆëŠ” ê°’ë“¤ë§Œ í•„í„°ë§\n",
    "df_filtered = df[df['disp_nm1'].isin(allowed_disp_nm1)]\n",
    "\n",
    "# disp_nm2ì˜ unique ê°’ ì¶œë ¥\n",
    "print(\"disp_nm2ì˜ unique ê°’ë“¤:\")\n",
    "print(df_filtered['disp_nm2'].unique())\n",
    "print(f\"\\nì´ {df_filtered['disp_nm2'].nunique()}ê°œì˜ unique ê°’\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bzf1cyvjtvb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disp_nm1ì˜ unique ê°’ë“¤:\n",
      "['ê·œê²©' 'ì‚¬ì–‘' 'ì™¸ê´€ ì‚¬ì–‘' 'ê¸°ë³¸ ì‚¬ì–‘' 'ì™¸ê´€' 'íŒ”ë ˆíŠ¸' 'ê¸°ë³¸ì‚¬ì–‘' 'ê¸°íƒ€ì œì›' 'ë§ˆìŠ¤í„° ë°•ìŠ¤' 'ìœ ë‹ˆíŠ¸ ë°•ìŠ¤'\n",
      " 'ë³¸ì²´ì¹˜ìˆ˜' 'ì£¼ìš”ì‚¬ì–‘' 'í•˜ë‹¨íŒ¨ë„' 'ì¼ì²´í˜• ì²­ì •ìŠ¤í…Œì´ì…˜' 'ì¼ë°˜ì‚¬ì–‘']\n",
      "\n",
      "ì´ 15ê°œì˜ unique ê°’\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TSV íŒŒì¼ ì½ê¸°\n",
    "df = pd.read_csv('/Users/toby/prog/kt/rubicon/data/kt_spec_validation_table_20251021.tsv', sep='\\t')\n",
    "\n",
    "# disp_nm1ì˜ unique ë¦¬ìŠ¤íŠ¸ ì¶œë ¥\n",
    "print(\"disp_nm1ì˜ unique ê°’ë“¤:\")\n",
    "print(df['disp_nm1'].unique())\n",
    "print(f\"\\nì´ {df['disp_nm1'].nunique()}ê°œì˜ unique ê°’\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "062f4abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_disp_nm1 = ['ê·œê²©','ì‚¬ì–‘','ì™¸ê´€ ì‚¬ì–‘','ê¸°ë³¸ ì‚¬ì–‘','ì™¸ê´€','ê¸°ë³¸ì‚¬ì–‘','ë³¸ì²´ì¹˜ìˆ˜','ì£¼ìš”ì‚¬ì–‘','ì¼ë°˜ì‚¬ì–‘']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b692a825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6502af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61ebaaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d52612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b6b667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import os\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from typing import Dict, List, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# .env íŒŒì¼ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932ff9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. PostgreSQL ì—°ê²° ì„¤ì •\n",
    "def get_db_connection():\n",
    "    \"\"\"PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=os.getenv('PG_HOST'),\n",
    "            port=os.getenv('PG_PORT'),\n",
    "            database=os.getenv('PG_DATABASE'),\n",
    "            user=os.getenv('PG_USER'),\n",
    "            password=os.getenv('PG_PASSWORD')\n",
    "        )\n",
    "        print(f\"âœ… PostgreSQL ì—°ê²° ì„±ê³µ: {os.getenv('PG_HOST')}\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ PostgreSQL ì—°ê²° ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "# ì—°ê²° í…ŒìŠ¤íŠ¸\n",
    "conn = get_db_connection()\n",
    "if conn:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •\n",
    "def get_openai_client():\n",
    "    \"\"\"Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±\"\"\"\n",
    "    try:\n",
    "        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸\n",
    "        endpoint = os.getenv('ENDPOINT_URL')\n",
    "        api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "        api_version = os.getenv('AZURE_API_VERSION', '2024-02-01')  # ê¸°ë³¸ê°’ ì œê³µ\n",
    "        \n",
    "        if not endpoint:\n",
    "            print(\"âŒ ENDPOINT_URL í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "            return None\n",
    "        \n",
    "        if not api_key:\n",
    "            print(\"âŒ AZURE_OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "            return None\n",
    "        \n",
    "        # API ë²„ì „ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©\n",
    "        if not api_version:\n",
    "            api_version = '2024-02-01'\n",
    "            print(f\"âš ï¸ AZURE_API_VERSIONì´ ì„¤ì •ë˜ì§€ ì•Šì•„ ê¸°ë³¸ê°’ ì‚¬ìš©: {api_version}\")\n",
    "        \n",
    "        print(f\"ğŸ“‹ Azure OpenAI ì„¤ì •:\")\n",
    "        print(f\"  - Endpoint: {endpoint[:50]}...\")\n",
    "        print(f\"  - API Version: {api_version}\")\n",
    "        print(f\"  - Deployment: {os.getenv('DEPLOYMENT_NAME')}\")\n",
    "        \n",
    "        client = AzureOpenAI(\n",
    "            azure_endpoint=endpoint,\n",
    "            api_key=api_key,\n",
    "            api_version=api_version,\n",
    "        )\n",
    "        print(f\"âœ… Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì„±ê³µ\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "# í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸\n",
    "openai_client = get_openai_client()\n",
    "\n",
    "# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í˜¸ì¶œ\n",
    "if openai_client:\n",
    "    try:\n",
    "        print(\"\\nğŸ§ª API ì—°ê²° í…ŒìŠ¤íŠ¸...\")\n",
    "        test_response = openai_client.chat.completions.create(\n",
    "            model=os.getenv('DEPLOYMENT_NAME'),\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Say 'Hello' in one word.\"}\n",
    "            ],\n",
    "            max_tokens=10\n",
    "        )\n",
    "        if test_response and test_response.choices:\n",
    "            print(f\"âœ… API í…ŒìŠ¤íŠ¸ ì„±ê³µ: '{test_response.choices[0].message.content}'\")\n",
    "        else:\n",
    "            print(\"âš ï¸ API ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ API í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "        print(f\"   ìƒì„¸ ì—ëŸ¬: {type(e).__name__}\")\n",
    "        if hasattr(e, '__dict__'):\n",
    "            print(f\"   ì—ëŸ¬ ì†ì„±: {e.__dict__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3c1ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒ\n",
    "def get_table_schema(table_name='test'):\n",
    "    \"\"\"í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒ (ì½”ë©˜íŠ¸ í¬í•¨)\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    if not conn:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # PostgreSQLì—ì„œ ì»¬ëŸ¼ ì •ë³´ì™€ ì½”ë©˜íŠ¸ë¥¼ í•¨ê»˜ ì¡°íšŒ\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            c.column_name,\n",
    "            c.data_type,\n",
    "            c.character_maximum_length,\n",
    "            c.numeric_precision,\n",
    "            c.numeric_scale,\n",
    "            c.is_nullable,\n",
    "            c.column_default,\n",
    "            pgd.description as column_comment\n",
    "        FROM information_schema.columns c\n",
    "        LEFT JOIN pg_catalog.pg_statio_all_tables as st\n",
    "            ON c.table_schema = st.schemaname \n",
    "            AND c.table_name = st.relname\n",
    "        LEFT JOIN pg_catalog.pg_description pgd \n",
    "            ON pgd.objoid = st.relid \n",
    "            AND pgd.objsubid = c.ordinal_position\n",
    "        WHERE c.table_schema = 'public' \n",
    "        AND c.table_name = %s\n",
    "        ORDER BY c.ordinal_position;\n",
    "        \"\"\"\n",
    "        \n",
    "        df_schema = pd.read_sql_query(query, conn, params=(table_name,))\n",
    "        print(f\"âœ… í…Œì´ë¸” '{table_name}' ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ì„±ê³µ\")\n",
    "        print(f\"   - ì»¬ëŸ¼ ìˆ˜: {len(df_schema)}\")\n",
    "        \n",
    "        # ì½”ë©˜íŠ¸ê°€ ìˆëŠ” ì»¬ëŸ¼ ìˆ˜ í™•ì¸\n",
    "        comment_count = df_schema['column_comment'].notna().sum()\n",
    "        print(f\"   - ì½”ë©˜íŠ¸ê°€ ìˆëŠ” ì»¬ëŸ¼: {comment_count}ê°œ\")\n",
    "        \n",
    "        return df_schema\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# ìŠ¤í‚¤ë§ˆ ì¡°íšŒ\n",
    "df_schema = get_table_schema(table_name=\"kt_merged_product_20250929\")\n",
    "if df_schema is not None:\n",
    "    print(\"\\ní…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´:\")\n",
    "    display(df_schema.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd4856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vc8lj1ptyxi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. kt_merged_product_20250929 í…Œì´ë¸” ë°ì´í„° ì¡°íšŒ\n",
    "def get_product_data():\n",
    "    \"\"\"kt_merged_product_20250929 í…Œì´ë¸”ì—ì„œ íŠ¹ì • ì»¬ëŸ¼ë§Œ ì¡°íšŒ\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    if not conn:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            product_id,\n",
    "            display_category_major,\n",
    "            display_category_middle,\n",
    "            display_category_minor,\n",
    "            model_code,\n",
    "            model_name,\n",
    "            product_name,\n",
    "            product_specification\n",
    "        FROM kt_merged_product_20250929\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        print(f\"âœ… ë°ì´í„° ì¡°íšŒ ì„±ê³µ: {len(df)}ê°œ í–‰\")\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# ë°ì´í„° ì¡°íšŒ ë° í‘œì‹œ\n",
    "df_products = get_product_data()\n",
    "if df_products is not None:\n",
    "    print(\"\\nkt_merged_product_20250929 í…Œì´ë¸” ë°ì´í„° (product_id, model_code, model_name, product_specification):\")\n",
    "    display(df_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53aha0crak",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. product_specification JSON ë¶„ì„\n",
    "def analyze_product_specifications():\n",
    "    \"\"\"product_specification JSON í•„ë“œë¥¼ ìƒì„¸ ë¶„ì„\"\"\"\n",
    "    \n",
    "    conn = get_db_connection()\n",
    "    if not conn:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # display_category_middleê³¼ product_specificationì„ í•¨ê»˜ ì¡°íšŒ\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            display_category_middle,\n",
    "            product_specification\n",
    "        FROM kt_merged_product_20250929\n",
    "        WHERE product_specification IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        print(f\"âœ… ë¶„ì„ìš© ë°ì´í„° ì¡°íšŒ ì„±ê³µ: {len(df)}ê°œ í–‰\\n\")\n",
    "        \n",
    "        # 1. display_category_middleë³„ key-value ë¶„ì„\n",
    "        category_key_stats = {}\n",
    "        all_keys = set()\n",
    "        key_value_examples = {}\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            category = row['display_category_middle']\n",
    "            spec = row['product_specification']\n",
    "            \n",
    "            if category not in category_key_stats:\n",
    "                category_key_stats[category] = {\n",
    "                    'keys': set(),\n",
    "                    'key_counts': {},\n",
    "                    'value_examples': {}\n",
    "                }\n",
    "            \n",
    "            try:\n",
    "                if isinstance(spec, str):\n",
    "                    spec_dict = json.loads(spec)\n",
    "                elif isinstance(spec, dict):\n",
    "                    spec_dict = spec\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                for key, value in spec_dict.items():\n",
    "                    all_keys.add(key)\n",
    "                    category_key_stats[category]['keys'].add(key)\n",
    "                    \n",
    "                    # keyë³„ ì¹´ìš´íŠ¸\n",
    "                    if key not in category_key_stats[category]['key_counts']:\n",
    "                        category_key_stats[category]['key_counts'][key] = 0\n",
    "                    category_key_stats[category]['key_counts'][key] += 1\n",
    "                    \n",
    "                    # value ì˜ˆì‹œ ìˆ˜ì§‘ (ìµœëŒ€ 3ê°œ)\n",
    "                    if key not in category_key_stats[category]['value_examples']:\n",
    "                        category_key_stats[category]['value_examples'][key] = set()\n",
    "                    if len(category_key_stats[category]['value_examples'][key]) < 20:\n",
    "                        if value and str(value).strip():\n",
    "                            category_key_stats[category]['value_examples'][key].add(str(value)[:50])\n",
    "                    \n",
    "                    # ì „ì²´ key-value ì˜ˆì‹œ\n",
    "                    if key not in key_value_examples:\n",
    "                        key_value_examples[key] = set()\n",
    "                    # if len(key_value_examples[key]) < 5:\n",
    "                    #     if value and str(value).strip():\n",
    "                    #         key_value_examples[key].add(str(value)[:50])\n",
    "                    if value and str(value).strip():\n",
    "                        key_value_examples[key].add(str(value)[:50])\n",
    "                            \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # 2. display_category_middleë³„ ì£¼ìš” key ì¶œë ¥\n",
    "        print(\"=\" * 80)\n",
    "        print(\"1. display_category_middleë³„ ì£¼ìš” Keyì™€ Value ì˜ˆì‹œ\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for category in sorted(category_key_stats.keys())[:]:  \n",
    "            stats = category_key_stats[category]\n",
    "            total_products = sum(stats['key_counts'].values()) / len(stats['keys']) if stats['keys'] else 0\n",
    "            \n",
    "            print(f\"\\nğŸ“ {category}\")\n",
    "            print(f\"   ì´ ì œí’ˆ ìˆ˜: ~{int(total_products)}\")\n",
    "            print(f\"   ê³ ìœ  key ìˆ˜: {len(stats['keys'])}\")\n",
    "            \n",
    "            top_keys = sorted(stats['key_counts'].items(), key=lambda x: x[1], reverse=True)[:]\n",
    "            for key, count in top_keys:\n",
    "                examples = list(stats['value_examples'].get(key, []))[:2]\n",
    "                examples_str = \", \".join([f\"'{ex}'\" for ex in examples])\n",
    "                print(f\"   â€¢ {key}: {count}íšŒ (ì˜ˆ: {examples_str})\")\n",
    "        \n",
    "        # 3. íŠ¹ì • keyê°€ ì–´ë–¤ display_category_middleì— ì†í•˜ëŠ”ì§€ ë¶„ì„\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"2. ì£¼ìš” Keyë³„ ì†Œì† Category ë¶„ì„\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        key_to_categories = {}\n",
    "        for category, stats in category_key_stats.items():\n",
    "            for key in stats['keys']:\n",
    "                if key not in key_to_categories:\n",
    "                    key_to_categories[key] = set()\n",
    "                key_to_categories[key].add(category)\n",
    "        \n",
    "        # ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ì— ê±¸ì³ ìˆëŠ” keyë“¤\n",
    "        cross_category_keys = {k: v for k, v in key_to_categories.items() if len(v) > 1}\n",
    "        sorted_keys = sorted(cross_category_keys.items(), key=lambda x: len(x[1]), reverse=True)[:]\n",
    "        \n",
    "        print(\"\\nğŸ“Š ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ì— ê±¸ì³ ìˆëŠ” Key (ìƒìœ„ 10ê°œ):\")\n",
    "        for key, categories in sorted_keys:\n",
    "            print(f\"   â€¢ {key}: {len(categories)}ê°œ ì¹´í…Œê³ ë¦¬\")\n",
    "            print(f\"     â†’ {', '.join(list(categories)[:])}\")\n",
    "        \n",
    "        # ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ì—ë§Œ ìˆëŠ” íŠ¹í™”ëœ keyë“¤\n",
    "        single_category_keys = {k: v for k, v in key_to_categories.items() if len(v) == 1}\n",
    "        \n",
    "        print(\"\\nğŸ“Œ íŠ¹ì • ì¹´í…Œê³ ë¦¬ ì „ìš© Key ì˜ˆì‹œ:\")\n",
    "        category_specific = {}\n",
    "        for key, categories in single_category_keys.items():\n",
    "            cat = list(categories)[0]\n",
    "            if cat not in category_specific:\n",
    "                category_specific[cat] = []\n",
    "            category_specific[cat].append(key)\n",
    "        \n",
    "        for cat, keys in list(category_specific.items())[:5]:\n",
    "            print(f\"   â€¢ {cat}: {', '.join(keys[:3])}\")\n",
    "        \n",
    "        # 4. ì „ì²´ í†µê³„\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"3. ì „ì²´ JSON Key-Value í†µê³„\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # keyë³„ ì „ì²´ ì‚¬ìš© íšŸìˆ˜\n",
    "        total_key_counts = {}\n",
    "        for stats in category_key_stats.values():\n",
    "            for key, count in stats['key_counts'].items():\n",
    "                if key not in total_key_counts:\n",
    "                    total_key_counts[key] = 0\n",
    "                total_key_counts[key] += count\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ ì „ì²´ í†µê³„:\")\n",
    "        print(f\"   â€¢ ì´ ê³ ìœ  key ìˆ˜: {len(all_keys)}\")\n",
    "        print(f\"   â€¢ ì´ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(category_key_stats)}\")\n",
    "        print(f\"   â€¢ ë¶„ì„ëœ ì œí’ˆ ìˆ˜: {len(df)}\")\n",
    "        \n",
    "        print(f\"\\nğŸ“Š ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” Key :\")\n",
    "        top_overall_keys = sorted(total_key_counts.items(), key=lambda x: x[1], reverse=True)[:]\n",
    "        for key, count in top_overall_keys:\n",
    "            percentage = (count / len(df)) * 100\n",
    "            examples = list(key_value_examples.get(key, []))[:2]\n",
    "            examples_str = \", \".join([f\"'{ex}'\" for ex in examples])\n",
    "            print(f\"   {key:30s} : {count:5d}íšŒ ({percentage:5.1f}%) | ì˜ˆ: {examples_str}\")\n",
    "        \n",
    "        # 5. ë°ì´í„° íƒ€ì… ë¶„ì„\n",
    "        print(f\"\\nğŸ“ Value ë°ì´í„° íƒ€ì… íŒ¨í„´:\")\n",
    "        type_patterns = {\n",
    "            'ìˆ«ìí˜•': [],\n",
    "            'ë¶ˆë¦°í˜•': [],\n",
    "            'í…ìŠ¤íŠ¸í˜•': [],\n",
    "            'ë‹¨ìœ„í¬í•¨': [],\n",
    "            'ë¦¬ìŠ¤íŠ¸í˜•': []\n",
    "        }\n",
    "        \n",
    "        for key, examples in key_value_examples.items():\n",
    "            sample = list(examples)[0] if examples else \"\"\n",
    "            if sample:\n",
    "                if sample.lower() in ['true', 'false', 'ì˜ˆ', 'ì•„ë‹ˆì˜¤']:\n",
    "                    type_patterns['ë¶ˆë¦°í˜•'].append(key)\n",
    "                elif any(unit in sample for unit in ['GB', 'MB', 'mm', 'cm', 'kg', 'W', 'Hz', 'mAh']):\n",
    "                    type_patterns['ë‹¨ìœ„í¬í•¨'].append(key)\n",
    "                elif sample.replace('.', '').replace('-', '').isdigit():\n",
    "                    type_patterns['ìˆ«ìí˜•'].append(key)\n",
    "                elif ',' in sample or '|' in sample:\n",
    "                    type_patterns['ë¦¬ìŠ¤íŠ¸í˜•'].append(key)\n",
    "                else:\n",
    "                    type_patterns['í…ìŠ¤íŠ¸í˜•'].append(key)\n",
    "        \n",
    "        for pattern_type, keys in type_patterns.items():\n",
    "            if keys:\n",
    "                print(f\"   â€¢ {pattern_type}: {', '.join(keys[:5])}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# ë¶„ì„ ì‹¤í–‰\n",
    "df_spec_analysis = analyze_product_specifications()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zl7hztfer9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. product_specification ê³ ìœ  key ì¶”ì¶œ ë° ìœ ì‚¬ key ë¶„ì„\n",
    "def analyze_similar_keys():\n",
    "    \"\"\"product_specificationì˜ ëª¨ë“  ê³ ìœ  keyë¥¼ ì¶”ì¶œí•˜ê³  ìœ ì‚¬í•œ key ìŒì„ ì°¾ê¸°\"\"\"\n",
    "    \n",
    "    conn = get_db_connection()\n",
    "    if not conn:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # ëª¨ë“  product_specification ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n",
    "        query = \"\"\"\n",
    "        SELECT product_specification\n",
    "        FROM kt_merged_product_20250929\n",
    "        WHERE product_specification IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        print(f\"âœ… ë°ì´í„° ì¡°íšŒ ì„±ê³µ: {len(df)}ê°œ í–‰\\n\")\n",
    "        \n",
    "        # ëª¨ë“  ê³ ìœ  key ì¶”ì¶œ\n",
    "        all_keys = set()\n",
    "        for spec in df['product_specification']:\n",
    "            try:\n",
    "                if isinstance(spec, str):\n",
    "                    spec_dict = json.loads(spec)\n",
    "                elif isinstance(spec, dict):\n",
    "                    spec_dict = spec\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                all_keys.update(spec_dict.keys())\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # ì •ë ¬ëœ key ë¦¬ìŠ¤íŠ¸\n",
    "        sorted_keys = sorted(list(all_keys))\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(f\"1. ì „ì²´ ê³ ìœ  Key ëª©ë¡ (ì´ {len(sorted_keys)}ê°œ)\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # ì•ŒíŒŒë²³/í•œê¸€ë³„ë¡œ ê·¸ë£¹í™”\n",
    "        english_keys = [k for k in sorted_keys if k and k[0].isascii()]\n",
    "        korean_keys = [k for k in sorted_keys if k and not k[0].isascii()]\n",
    "        \n",
    "        print(\"\\nğŸ“Œ ì˜ë¬¸ Key (í•œ ì¤„ì”©):\")\n",
    "        print(\"-\" * 40)\n",
    "        for key in english_keys:\n",
    "            print(key)\n",
    "        \n",
    "        print(\"\\nğŸ“Œ í•œê¸€ Key (í•œ ì¤„ì”©):\")\n",
    "        print(\"-\" * 40)\n",
    "        for key in korean_keys:\n",
    "            print(key)\n",
    "        \n",
    "        # OpenAIë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ key ë¶„ì„ (ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œ)\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"2. OpenAIë¥¼ í™œìš©í•œ ìœ ì‚¬ Key ìƒì„¸ ë¶„ì„\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # OpenAI í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°\n",
    "        openai_client = get_openai_client()\n",
    "        \n",
    "        all_similar_pairs = []\n",
    "        \n",
    "        if openai_client:\n",
    "            try:\n",
    "                # keyë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ë¶„ì„ (í•œ ë²ˆì— ë„ˆë¬´ ë§ìœ¼ë©´ APIê°€ ì œëŒ€ë¡œ ë¶„ì„ ëª»í•¨)\n",
    "                batch_size = 50\n",
    "                for i in range(0, len(sorted_keys), batch_size):\n",
    "                    batch_keys = sorted_keys[i:i+batch_size]\n",
    "                    keys_str = ', '.join(batch_keys)\n",
    "                    \n",
    "                    print(f\"\\nğŸ¤– ë°°ì¹˜ {i//batch_size + 1}/{(len(sorted_keys)-1)//batch_size + 1} ë¶„ì„ ì¤‘...\")\n",
    "                    \n",
    "                    prompt = f\"\"\"ë‹¤ìŒì€ ì œí’ˆ ì‚¬ì–‘ì„ ë‚˜íƒ€ë‚´ëŠ” JSON key ëª©ë¡ì…ë‹ˆë‹¤:\n",
    "{keys_str}\n",
    "\n",
    "ìœ„ keyë“¤ ì¤‘ì—ì„œ ì˜ë¯¸ê°€ ìœ ì‚¬í•˜ê±°ë‚˜ ì¤‘ë³µë˜ëŠ” key ìŒë“¤ì„ ëª¨ë‘ ì°¾ì•„ì£¼ì„¸ìš”.\n",
    "ì˜ˆë¥¼ ë“¤ì–´:\n",
    "- ê°™ì€ ì†ì„±ì„ ë‹¤ë¥¸ ì´ë¦„ìœ¼ë¡œ í‘œí˜„í•œ ê²½ìš°\n",
    "- í•œê¸€ê³¼ ì˜ë¬¸ìœ¼ë¡œ ì¤‘ë³µëœ ê²½ìš°\n",
    "- ì•½ì–´ì™€ ì „ì²´ ì´ë¦„ì´ í•¨ê»˜ ìˆëŠ” ê²½ìš°\n",
    "- ëŒ€ì†Œë¬¸ìë§Œ ë‹¤ë¥¸ ê²½ìš°\n",
    "- ë„ì–´ì“°ê¸°ë‚˜ ì–¸ë”ìŠ¤ì½”ì–´ ì°¨ì´ë§Œ ìˆëŠ” ê²½ìš°\n",
    "\n",
    "ê°€ëŠ¥í•œ ëª¨ë“  ìœ ì‚¬ ìŒì„ ì°¾ì•„ì„œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:\n",
    "{{\n",
    "  \"similar_pairs\": [\n",
    "    {{\"key1\": \"ì²«ë²ˆì§¸key\", \"key2\": \"ë‘ë²ˆì§¸key\", \"reason\": \"ìœ ì‚¬í•œ ì´ìœ \"}},\n",
    "    ...\n",
    "  ]\n",
    "}}\n",
    "\"\"\"\n",
    "                    \n",
    "                    response = openai_client.chat.completions.create(\n",
    "                        model=os.getenv('DEPLOYMENT_NAME'),\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"You are a data analyst expert in finding similar or duplicate fields in datasets. Find ALL possible similar pairs. Respond in Korean.\"},\n",
    "                            {\"role\": \"user\", \"content\": prompt}\n",
    "                        ],\n",
    "                        max_tokens=3000,\n",
    "                        temperature=0.2\n",
    "                    )\n",
    "                    \n",
    "                    if response and response.choices:\n",
    "                        result_text = response.choices[0].message.content\n",
    "                        \n",
    "                        # JSON íŒŒì‹± ì‹œë„\n",
    "                        try:\n",
    "                            if '```json' in result_text:\n",
    "                                json_str = result_text.split('```json')[1].split('```')[0]\n",
    "                            elif '{' in result_text:\n",
    "                                start = result_text.index('{')\n",
    "                                end = result_text.rindex('}') + 1\n",
    "                                json_str = result_text[start:end]\n",
    "                            else:\n",
    "                                json_str = result_text\n",
    "                                \n",
    "                            similar_data = json.loads(json_str)\n",
    "                            \n",
    "                            if 'similar_pairs' in similar_data:\n",
    "                                all_similar_pairs.extend(similar_data['similar_pairs'])\n",
    "                                \n",
    "                        except json.JSONDecodeError:\n",
    "                            pass\n",
    "                \n",
    "                # ì¤‘ë³µ ì œê±° ë° ì¶œë ¥\n",
    "                print(\"\\nğŸ“Š ë°œê²¬ëœ ëª¨ë“  ìœ ì‚¬ Key ìŒ (ìƒì„¸):\")\n",
    "                print(\"-\" * 60)\n",
    "                \n",
    "                seen_pairs = set()\n",
    "                for idx, pair in enumerate(all_similar_pairs, 1):\n",
    "                    key1, key2 = pair.get('key1', ''), pair.get('key2', '')\n",
    "                    pair_tuple = tuple(sorted([key1, key2]))\n",
    "                    \n",
    "                    if pair_tuple not in seen_pairs:\n",
    "                        seen_pairs.add(pair_tuple)\n",
    "                        print(f\"\\n{idx}. ìœ ì‚¬ ìŒ:\")\n",
    "                        print(f\"   Key 1: {key1}\")\n",
    "                        print(f\"   Key 2: {key2}\")\n",
    "                        print(f\"   ì´ìœ : {pair.get('reason', '')}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        # ìƒì„¸í•œ ê·œì¹™ ê¸°ë°˜ ìœ ì‚¬ì„± ë¶„ì„\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"3. ê·œì¹™ ê¸°ë°˜ ìœ ì‚¬ Key ìƒì„¸ íƒì§€\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        similar_groups = {\n",
    "            'ìƒ‰ìƒ ê´€ë ¨': [],\n",
    "            'í¬ê¸°/ìš©ëŸ‰ ê´€ë ¨': [],\n",
    "            'ë¬´ê²Œ ê´€ë ¨': [],\n",
    "            'í™”ë©´ ê´€ë ¨': [],\n",
    "            'ë°°í„°ë¦¬ ê´€ë ¨': [],\n",
    "            'ë©”ëª¨ë¦¬ ê´€ë ¨': [],\n",
    "            'ì¹´ë©”ë¼ ê´€ë ¨': [],\n",
    "            'ë„¤íŠ¸ì›Œí¬/í†µì‹  ê´€ë ¨': [],\n",
    "            'í”„ë¡œì„¸ì„œ/ì„±ëŠ¥ ê´€ë ¨': [],\n",
    "            'ì˜¤ë””ì˜¤/ì‚¬ìš´ë“œ ê´€ë ¨': [],\n",
    "            'ì—°ê²°/í¬íŠ¸ ê´€ë ¨': [],\n",
    "            'ì „ì› ê´€ë ¨': [],\n",
    "            'ì„¼ì„œ ê´€ë ¨': [],\n",
    "            'ë³´ì•ˆ ê´€ë ¨': [],\n",
    "            'ê¸°íƒ€ ê¸°ëŠ¥': []\n",
    "        }\n",
    "        \n",
    "        for key in sorted_keys:\n",
    "            key_lower = key.lower()\n",
    "            \n",
    "            # ìƒ‰ìƒ ê´€ë ¨\n",
    "            if any(word in key_lower for word in ['color', 'ìƒ‰ìƒ', 'ì»¬ëŸ¬', 'colour']):\n",
    "                similar_groups['ìƒ‰ìƒ ê´€ë ¨'].append(key)\n",
    "            \n",
    "            # í¬ê¸°/ìš©ëŸ‰ ê´€ë ¨\n",
    "            if any(word in key_lower for word in ['size', 'í¬ê¸°', 'ì‚¬ì´ì¦ˆ', 'ìš©ëŸ‰', 'capacity', 'dimension', 'ì¹˜ìˆ˜', 'volume']):\n",
    "                similar_groups['í¬ê¸°/ìš©ëŸ‰ ê´€ë ¨'].append(key)\n",
    "            \n",
    "            # ë¬´ê²Œ ê´€ë ¨\n",
    "            if any(word in key_lower for word in ['weight', 'ë¬´ê²Œ', 'ì¤‘ëŸ‰', 'mass']):\n",
    "                similar_groups['ë¬´ê²Œ ê´€ë ¨'].append(key)\n",
    "            \n",
    "            # í™”ë©´ ê´€ë ¨\n",
    "            if any(word in key_lower for word in ['display', 'í™”ë©´', 'ë””ìŠ¤í”Œë ˆì´', 'screen', 'panel', 'íŒ¨ë„', 'lcd', 'oled', 'resolution', 'í•´ìƒë„']):\n",
    "                similar_groups['í™”ë©´ ê´€ë ¨'].append(key)\n",
    "            \n",
    "            # ë°°í„°ë¦¬ ê´€ë ¨\n",
    "            if any(word in key_lower for word in ['battery', 'ë°°í„°ë¦¬', 'ì „ì§€', 'charge', 'ì¶©ì „', 'power bank']):\n",
    "                similar_groups['ë°°í„°ë¦¬ ê´€ë ¨'].append(key)\n",
    "            \n",
    "            # ë©”ëª¨ë¦¬ ê´€ë ¨\n",
    "            if any(word in key_lower for word in ['memory', 'ë©”ëª¨ë¦¬', 'ram', 'storage', 'ì €ì¥', 'rom', 'ssd', 'hdd']):\n",
    "                similar_groups['ë©”ëª¨ë¦¬ ê´€ë ¨'].append(key)\n",
    "            \n",
    "            # ì¹´ë©”ë¼ ê´€ë ¨\n",
    "            if any(word in key_lower for word in ['camera', 'ì¹´ë©”ë¼', 'ì´¬ì˜', 'ë Œì¦ˆ', 'lens', 'photo', 'ì‚¬ì§„', 'video', 'ë™ì˜ìƒ']):\n",
    "                similar_groups['ì¹´ë©”ë¼ ê´€ë ¨'].append(key)\n",
    "            \n",
    "            # ë„¤íŠ¸ì›Œí¬/í†µì‹  ê´€ë ¨\n",
    "            if any(word in key_lower for word in ['network', 'ë„¤íŠ¸ì›Œí¬', 'wifi', 'wi-fi', 'bluetooth', 'ë¸”ë£¨íˆ¬ìŠ¤', 'lte', '5g', '4g', 'cellular']):\n",
    "                similar_groups['ë„¤íŠ¸ì›Œí¬/í†µì‹  ê´€ë ¨'].append(key)\n",
    "            \n",
    "            # í”„ë¡œì„¸ì„œ/ì„±ëŠ¥ ê´€ë ¨\n",
    "            if any(word in key_lower for word in ['processor', 'í”„ë¡œì„¸ì„œ', 'cpu', 'gpu', 'chip', 'ì¹©', 'performance', 'ì„±ëŠ¥']):\n",
    "                similar_groups['í”„ë¡œì„¸ì„œ/ì„±ëŠ¥ ê´€ë ¨'].append(key)\n",
    "            \n",
    "            # ì˜¤ë””ì˜¤/ì‚¬ìš´ë“œ ê´€ë ¨\n",
    "            if any(word in key_lower for word in ['audio', 'ì˜¤ë””ì˜¤', 'sound', 'ì‚¬ìš´ë“œ', 'speaker', 'ìŠ¤í”¼ì»¤', 'microphone', 'ë§ˆì´í¬']):\n",
    "                similar_groups['ì˜¤ë””ì˜¤/ì‚¬ìš´ë“œ ê´€ë ¨'].append(key)\n",
    "            \n",
    "            # ì—°ê²°/í¬íŠ¸ ê´€ë ¨\n",
    "            if any(word in key_lower for word in ['port', 'í¬íŠ¸', 'usb', 'hdmi', 'connector', 'ì—°ê²°', 'jack', 'ì­']):\n",
    "                similar_groups['ì—°ê²°/í¬íŠ¸ ê´€ë ¨'].append(key)\n",
    "            \n",
    "            # ì „ì› ê´€ë ¨\n",
    "            if any(word in key_lower for word in ['power', 'ì „ì›', 'voltage', 'ì „ì••', 'adapter', 'ì–´ëŒ‘í„°', 'charger']):\n",
    "                similar_groups['ì „ì› ê´€ë ¨'].append(key)\n",
    "            \n",
    "            # ì„¼ì„œ ê´€ë ¨\n",
    "            if any(word in key_lower for word in ['sensor', 'ì„¼ì„œ', 'gyro', 'ìì´ë¡œ', 'accelerometer', 'ê°€ì†ë„ê³„']):\n",
    "                similar_groups['ì„¼ì„œ ê´€ë ¨'].append(key)\n",
    "            \n",
    "            # ë³´ì•ˆ ê´€ë ¨\n",
    "            if any(word in key_lower for word in ['security', 'ë³´ì•ˆ', 'fingerprint', 'ì§€ë¬¸', 'face', 'ì–¼êµ´', 'lock', 'ì ê¸ˆ']):\n",
    "                similar_groups['ë³´ì•ˆ ê´€ë ¨'].append(key)\n",
    "        \n",
    "        print(\"\\nğŸ“Œ ì£¼ì œë³„ ìœ ì‚¬ Key ê·¸ë£¹ (ì „ì²´ ëª©ë¡):\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for group_name, keys in similar_groups.items():\n",
    "            if keys:\n",
    "                print(f\"\\nã€{group_name}ã€‘ ì´ {len(keys)}ê°œ\")\n",
    "                print(\"-\" * 40)\n",
    "                for key in sorted(keys):\n",
    "                    print(key)\n",
    "        \n",
    "        # ëŒ€ì†Œë¬¸ì/ì–¸ë”ìŠ¤ì½”ì–´ ì°¨ì´ë§Œ ìˆëŠ” key ì°¾ê¸°\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"4. í˜•ì‹ë§Œ ë‹¤ë¥¸ ì¤‘ë³µ Key íƒì§€\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        normalized_keys = {}\n",
    "        for key in sorted_keys:\n",
    "            # ì •ê·œí™”: ì†Œë¬¸ìë¡œ ë³€í™˜, ì–¸ë”ìŠ¤ì½”ì–´/í•˜ì´í”ˆ/ê³µë°± ì œê±°\n",
    "            normalized = key.lower().replace('_', '').replace('-', '').replace(' ', '')\n",
    "            if normalized not in normalized_keys:\n",
    "                normalized_keys[normalized] = []\n",
    "            normalized_keys[normalized].append(key)\n",
    "        \n",
    "        print(\"\\nğŸ“Œ í˜•ì‹ë§Œ ë‹¤ë¥¸ ì¤‘ë³µ ê°€ëŠ¥ Key:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        duplicate_count = 0\n",
    "        for normalized, original_keys in normalized_keys.items():\n",
    "            if len(original_keys) > 1:\n",
    "                duplicate_count += 1\n",
    "                print(f\"\\nì¤‘ë³µ ê·¸ë£¹ {duplicate_count}:\")\n",
    "                for key in original_keys:\n",
    "                    print(f\"  - {key}\")\n",
    "        \n",
    "        if duplicate_count == 0:\n",
    "            print(\"í˜•ì‹ë§Œ ë‹¤ë¥¸ ì¤‘ë³µ keyê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        return sorted_keys\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# ë¶„ì„ ì‹¤í–‰\n",
    "unique_keys = analyze_similar_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c92a9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec125c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. product_specification ê³„ì¸µ êµ¬ì¡° ë¶„ì„\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def get_json_depth(obj, current_depth=0):\n",
    "    \"\"\"JSON ê°ì²´ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ê³„ì‚°\"\"\"\n",
    "    if not isinstance(obj, (dict, list)):\n",
    "        return current_depth\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        if not obj:\n",
    "            return current_depth\n",
    "        return max(get_json_depth(v, current_depth + 1) for v in obj.values())\n",
    "    \n",
    "    if isinstance(obj, list):\n",
    "        if not obj:\n",
    "            return current_depth\n",
    "        return max(get_json_depth(item, current_depth) for item in obj)\n",
    "\n",
    "def flatten_json_hierarchy(json_obj, max_depth=5):\n",
    "    \"\"\"JSONì„ ê³„ì¸µë³„ key-value ìŒìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    # ê° ê³„ì¸µ ì´ˆê¸°í™”\n",
    "    for i in range(1, max_depth + 1):\n",
    "        result[f'level_{i}_key'] = None\n",
    "        result[f'level_{i}_value'] = None\n",
    "    \n",
    "    def traverse(obj, path=[], depth=1):\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "            \n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                if depth == 1:\n",
    "                    result[f'level_{depth}_key'] = key\n",
    "                    \n",
    "                    # valueê°€ ë‹¨ìˆœ íƒ€ì…ì¸ ê²½ìš°\n",
    "                    if not isinstance(value, (dict, list)):\n",
    "                        result[f'level_{depth}_value'] = str(value) if value is not None else None\n",
    "                    # valueê°€ dictì¸ ê²½ìš° í•˜ìœ„ ê³„ì¸µ íƒìƒ‰\n",
    "                    elif isinstance(value, dict):\n",
    "                        result[f'level_{depth}_value'] = 'object'\n",
    "                        traverse(value, path + [key], depth + 1)\n",
    "                    # valueê°€ listì¸ ê²½ìš°\n",
    "                    elif isinstance(value, list):\n",
    "                        result[f'level_{depth}_value'] = f'array[{len(value)}]'\n",
    "                        # ì²« ë²ˆì§¸ ìš”ì†Œë§Œ íƒìƒ‰\n",
    "                        if value and isinstance(value[0], dict):\n",
    "                            traverse(value[0], path + [key], depth + 1)\n",
    "    \n",
    "    if isinstance(json_obj, dict):\n",
    "        traverse(json_obj)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# DBì—ì„œ ë°ì´í„° ë¡œë“œ\n",
    "conn = get_db_connection()\n",
    "if conn:\n",
    "    try:\n",
    "        # product_specificationì´ not nullì¸ ë°ì´í„°ë§Œ ì¡°íšŒ\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            product_id,\n",
    "            display_category_major,\n",
    "            display_category_middle,\n",
    "            display_category_minor,\n",
    "            product_specification\n",
    "        FROM kt_merged_product_20250929\n",
    "        WHERE product_specification IS NOT NULL\n",
    "        LIMIT 10000\n",
    "        \"\"\"\n",
    "        \n",
    "        df_products = pd.read_sql_query(query, conn)\n",
    "        print(f\"ğŸ“Š ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_products)}ê°œ í–‰\")\n",
    "        \n",
    "        # JSON íŒŒì‹± ë° ìµœëŒ€ ê¹Šì´ ê³„ì‚°\n",
    "        max_depth = 0\n",
    "        valid_specs = []\n",
    "        \n",
    "        for idx, row in df_products.iterrows():\n",
    "            try:\n",
    "                spec_str = row['product_specification']\n",
    "                if spec_str:\n",
    "                    # JSON íŒŒì‹±\n",
    "                    if isinstance(spec_str, str):\n",
    "                        spec_json = json.loads(spec_str)\n",
    "                    else:\n",
    "                        spec_json = spec_str\n",
    "                    \n",
    "                    # ê¹Šì´ ê³„ì‚°\n",
    "                    depth = get_json_depth(spec_json)\n",
    "                    max_depth = max(max_depth, depth)\n",
    "                    \n",
    "                    valid_specs.append((idx, spec_json))\n",
    "            except Exception as e:\n",
    "                print(f\"  âš ï¸ í–‰ {idx} JSON íŒŒì‹± ì‹¤íŒ¨: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\nğŸ“ ìµœëŒ€ JSON ê³„ì¸µ ê¹Šì´: {max_depth}\")\n",
    "        print(f\"âœ… ìœ íš¨í•œ JSON ë°ì´í„°: {len(valid_specs)}ê°œ\")\n",
    "        \n",
    "        # ê³„ì¸µë³„ë¡œ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "        rows_data = []\n",
    "        \n",
    "        for idx, spec_json in valid_specs[:1000]:  # ì‹œë²”ì ìœ¼ë¡œ 1000ê°œë§Œ ì²˜ë¦¬\n",
    "            row = df_products.iloc[idx]\n",
    "            \n",
    "            # ê¸°ë³¸ ì •ë³´\n",
    "            row_dict = {\n",
    "                'product_id': row['product_id'],\n",
    "                'display_category_major': row['display_category_major'],\n",
    "                'display_category_middle': row['display_category_middle'],\n",
    "                'display_category_minor': row['display_category_minor']\n",
    "            }\n",
    "            \n",
    "            # JSON ê³„ì¸µ ì •ë³´ ì¶”ê°€\n",
    "            hierarchy = flatten_json_hierarchy(spec_json, max_depth)\n",
    "            row_dict.update(hierarchy)\n",
    "            \n",
    "            rows_data.append(row_dict)\n",
    "        \n",
    "        # ìµœì¢… ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "        df_hierarchy = pd.DataFrame(rows_data)\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ ê³„ì¸µ êµ¬ì¡° ë°ì´í„°í”„ë ˆì„ ìƒì„± ì™„ë£Œ\")\n",
    "        print(f\"   - í–‰ ìˆ˜: {len(df_hierarchy)}\")\n",
    "        print(f\"   - ì»¬ëŸ¼ ìˆ˜: {len(df_hierarchy.columns)}\")\n",
    "        print(f\"\\nì»¬ëŸ¼ ëª©ë¡:\")\n",
    "        for col in df_hierarchy.columns:\n",
    "            print(f\"   - {col}\")\n",
    "        \n",
    "        # ì²« 5ê°œ í–‰ ì¶œë ¥\n",
    "        print(\"\\nğŸ” ë°ì´í„° ìƒ˜í”Œ (ì²« 5ê°œ í–‰):\")\n",
    "        display(df_hierarchy.tail(10))\n",
    "        \n",
    "        # ê° ê³„ì¸µë³„ ê³ ìœ  key ë¶„ì„\n",
    "        print(\"\\nğŸ“Š ê° ê³„ì¸µë³„ ê³ ìœ  key ë¶„ì„:\")\n",
    "        for i in range(1, max_depth + 1):\n",
    "            key_col = f'level_{i}_key'\n",
    "            if key_col in df_hierarchy.columns:\n",
    "                unique_keys = df_hierarchy[key_col].dropna().unique()\n",
    "                print(f\"\\n   Level {i}:\")\n",
    "                print(f\"   - ê³ ìœ  key ê°œìˆ˜: {len(unique_keys)}\")\n",
    "                if len(unique_keys) <= 20:\n",
    "                    print(f\"   - Keys: {sorted(unique_keys)}\")\n",
    "                else:\n",
    "                    print(f\"   - Top 10 Keys: {sorted(unique_keys)[:10]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        conn.close()\n",
    "else:\n",
    "    print(\"âŒ DB ì—°ê²° ì‹¤íŒ¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7f18cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. product_specification ê³„ì¸µ êµ¬ì¡° ìƒì„¸ ë¶„ì„ (ê°œì„  ë²„ì „)\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "\n",
    "def extract_nested_values(obj, parent_keys=None, results=None):\n",
    "    \"\"\"\n",
    "    ì¤‘ì²©ëœ JSON ê°ì²´ì—ì„œ ëª¨ë“  leaf ê°’ë“¤ì„ ì¶”ì¶œ\n",
    "    parent_keys: ìƒìœ„ keyë“¤ì˜ ë¦¬ìŠ¤íŠ¸\n",
    "    results: ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    if parent_keys is None:\n",
    "        parent_keys = []\n",
    "    if results is None:\n",
    "        results = []\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        for key, value in obj.items():\n",
    "            current_path = parent_keys + [key]\n",
    "            \n",
    "            if isinstance(value, dict):\n",
    "                # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° ì¬ê·€ í˜¸ì¶œ\n",
    "                extract_nested_values(value, current_path, results)\n",
    "            elif isinstance(value, list):\n",
    "                # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°\n",
    "                if len(value) > 0:\n",
    "                    # ë¦¬ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸ ìš”ì†Œë§Œ ì²˜ë¦¬\n",
    "                    first_item = value[0]\n",
    "                    if isinstance(first_item, dict):\n",
    "                        # ë”•ì…”ë„ˆë¦¬ê°€ í¬í•¨ëœ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°\n",
    "                        extract_nested_values(first_item, current_path, results)\n",
    "                    else:\n",
    "                        # ë‹¨ìˆœ ê°’ì˜ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°\n",
    "                        results.append({\n",
    "                            'level': len(current_path),\n",
    "                            'key': '; '.join(current_path),\n",
    "                            'value': str(first_item)\n",
    "                        })\n",
    "                else:\n",
    "                    # ë¹ˆ ë¦¬ìŠ¤íŠ¸\n",
    "                    results.append({\n",
    "                        'level': len(current_path),\n",
    "                        'key': '; '.join(current_path),\n",
    "                        'value': None\n",
    "                    })\n",
    "            else:\n",
    "                # ë‹¨ìˆœ ê°’ì¸ ê²½ìš° (leaf node)\n",
    "                results.append({\n",
    "                    'level': len(current_path),\n",
    "                    'key': '; '.join(current_path),\n",
    "                    'value': str(value) if value is not None else None\n",
    "                })\n",
    "    elif isinstance(obj, list):\n",
    "        # ìµœìƒìœ„ê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°\n",
    "        if len(obj) > 0 and isinstance(obj[0], dict):\n",
    "            extract_nested_values(obj[0], parent_keys, results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_hierarchical_dataframe(df_products, max_rows=None):\n",
    "    \"\"\"ì œí’ˆë³„ë¡œ ê³„ì¸µ êµ¬ì¡°í™”ëœ ë°ì´í„°í”„ë ˆì„ ìƒì„±\"\"\"\n",
    "    all_rows = []\n",
    "    \n",
    "    # ì²˜ë¦¬í•  í–‰ ìˆ˜ ì œí•œ\n",
    "    rows_to_process = df_products if max_rows is None else df_products.head(max_rows)\n",
    "    \n",
    "    for idx, row in rows_to_process.iterrows():\n",
    "        try:\n",
    "            spec_str = row['product_specification']\n",
    "            if pd.notna(spec_str):\n",
    "                # JSON íŒŒì‹±\n",
    "                if isinstance(spec_str, str):\n",
    "                    spec_json = json.loads(spec_str)\n",
    "                else:\n",
    "                    spec_json = spec_str\n",
    "                \n",
    "                # ì¤‘ì²©ëœ ëª¨ë“  ê°’ ì¶”ì¶œ\n",
    "                nested_values = extract_nested_values(spec_json)\n",
    "                \n",
    "                # ê° ì¶”ì¶œëœ ê°’ì— ëŒ€í•´ í–‰ ìƒì„±\n",
    "                for item in nested_values:\n",
    "                    row_data = {\n",
    "                        'product_id': row['product_id'],\n",
    "                        'display_category_major': row['display_category_major'],\n",
    "                        'display_category_middle': row['display_category_middle'],\n",
    "                        'display_category_minor': row['display_category_minor'],\n",
    "                        'level': item['level'],\n",
    "                        'key': item['key'],\n",
    "                        'value': item['value']\n",
    "                    }\n",
    "                    all_rows.append(row_data)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ í–‰ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì •ë ¬\n",
    "    df_result = pd.DataFrame(all_rows)\n",
    "    if not df_result.empty:\n",
    "        df_result = df_result.sort_values(['product_id', 'key'])\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "def get_max_depth(obj, current_depth=0):\n",
    "    \"\"\"JSON ê°ì²´ì˜ ìµœëŒ€ ê¹Šì´ ê³„ì‚°\"\"\"\n",
    "    if not isinstance(obj, (dict, list)):\n",
    "        return current_depth\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        if not obj:\n",
    "            return current_depth\n",
    "        return max(get_max_depth(v, current_depth + 1) for v in obj.values())\n",
    "    \n",
    "    if isinstance(obj, list):\n",
    "        if not obj:\n",
    "            return current_depth\n",
    "        return max(get_max_depth(item, current_depth) for item in obj)\n",
    "\n",
    "# DBì—ì„œ ë°ì´í„° ë¡œë“œ ë° ë¶„ì„ ì‹¤í–‰\n",
    "conn = get_db_connection()\n",
    "if conn:\n",
    "    try:\n",
    "        # product_specificationì´ not nullì¸ ë°ì´í„°ë§Œ ì¡°íšŒ\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            product_id,\n",
    "            display_category_major,\n",
    "            display_category_middle,\n",
    "            display_category_minor,\n",
    "            product_specification\n",
    "        FROM kt_merged_product_20250929\n",
    "        WHERE product_specification IS NOT NULL\n",
    "        LIMIT 5000\n",
    "        \"\"\"\n",
    "        \n",
    "        df_products = pd.read_sql_query(query, conn)\n",
    "        print(f\"ğŸ“Š ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_products)}ê°œ í–‰\")\n",
    "        \n",
    "        # ìµœëŒ€ ê³„ì¸µ ê¹Šì´ ë¶„ì„\n",
    "        print(\"\\nâ³ JSON ê³„ì¸µ ê¹Šì´ ë¶„ì„ ì¤‘...\")\n",
    "        max_depth = 0\n",
    "        depth_distribution = {}\n",
    "        \n",
    "        for spec in df_products['product_specification']:\n",
    "            try:\n",
    "                if isinstance(spec, str):\n",
    "                    spec_json = json.loads(spec)\n",
    "                else:\n",
    "                    spec_json = spec\n",
    "                \n",
    "                depth = get_max_depth(spec_json)\n",
    "                max_depth = max(max_depth, depth)\n",
    "                \n",
    "                if depth not in depth_distribution:\n",
    "                    depth_distribution[depth] = 0\n",
    "                depth_distribution[depth] += 1\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\nğŸ“ ê³„ì¸µ ê¹Šì´ ë¶„ì„ ê²°ê³¼:\")\n",
    "        print(f\"   - ìµœëŒ€ ê³„ì¸µ ê¹Šì´: {max_depth}\")\n",
    "        for depth, count in sorted(depth_distribution.items()):\n",
    "            print(f\"   - ê¹Šì´ {depth}: {count}ê°œ ì œí’ˆ\")\n",
    "        \n",
    "        # ê³„ì¸µ êµ¬ì¡° ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "        print(\"\\nâ³ ê³„ì¸µ êµ¬ì¡° ë°ì´í„°í”„ë ˆì„ ìƒì„± ì¤‘...\")\n",
    "        df_hierarchical = create_hierarchical_dataframe(df_products, max_rows=1000)\n",
    "        \n",
    "        print(f\"\\nâœ… ë¶„ì„ ì™„ë£Œ!\")\n",
    "        print(f\"   - ì´ ë ˆì½”ë“œ ìˆ˜: {len(df_hierarchical):,}\")\n",
    "        print(f\"   - ê³ ìœ  ì œí’ˆ ìˆ˜: {df_hierarchical['product_id'].nunique():,}\")\n",
    "        print(f\"   - ê³ ìœ  key ìˆ˜: {df_hierarchical['key'].nunique():,}\")\n",
    "        \n",
    "        # ë ˆë²¨ë³„ í†µê³„\n",
    "        print(f\"\\nğŸ“Š ë ˆë²¨ë³„ ë°ì´í„° ë¶„í¬:\")\n",
    "        level_stats = df_hierarchical['level'].value_counts().sort_index()\n",
    "        for level, count in level_stats.items():\n",
    "            print(f\"   - Level {level}: {count:,}ê°œ ë ˆì½”ë“œ\")\n",
    "        \n",
    "        # ìƒ˜í”Œ ë°ì´í„° í‘œì‹œ\n",
    "        print(\"\\nğŸ“‹ ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 30ê°œ í–‰):\")\n",
    "        display(df_hierarchical.head(30))\n",
    "        \n",
    "        # ë‹¤ì–‘í•œ ë ˆë²¨ì˜ ì˜ˆì‹œ ë³´ì—¬ì£¼ê¸°\n",
    "        print(\"\\nğŸ” ë ˆë²¨ë³„ ë°ì´í„° ì˜ˆì‹œ:\")\n",
    "        for level in sorted(df_hierarchical['level'].unique()):\n",
    "            level_sample = df_hierarchical[df_hierarchical['level'] == level].head(3)\n",
    "            if not level_sample.empty:\n",
    "                print(f\"\\n   Level {level} ì˜ˆì‹œ:\")\n",
    "                for _, row in level_sample.iterrows():\n",
    "                    print(f\"      â€¢ key: '{row['key']}' â†’ value: '{row['value']}'\")\n",
    "        \n",
    "        # íŠ¹ì • ì œí’ˆì˜ ì „ì²´ êµ¬ì¡° ë³´ê¸°\n",
    "        sample_product = df_hierarchical['product_id'].iloc[0]\n",
    "        print(f\"\\nğŸ” ì œí’ˆ {sample_product}ì˜ ì „ì²´ specification êµ¬ì¡°:\")\n",
    "        sample_spec = df_hierarchical[df_hierarchical['product_id'] == sample_product][['level', 'key', 'value']]\n",
    "        display(sample_spec)\n",
    "        \n",
    "        # ì¤‘ì²©ëœ key ë¶„ì„ (';'ë¥¼ í¬í•¨í•˜ëŠ” keyë“¤)\n",
    "        nested_keys = df_hierarchical[df_hierarchical['key'].str.contains(';', na=False)]\n",
    "        if not nested_keys.empty:\n",
    "            print(f\"\\nğŸ”— ì¤‘ì²©ëœ êµ¬ì¡°ë¥¼ ê°€ì§„ key í†µê³„:\")\n",
    "            print(f\"   - ì´ ì¤‘ì²© key ìˆ˜: {len(nested_keys):,}\")\n",
    "            print(f\"   - ê³ ìœ  ì¤‘ì²© key íŒ¨í„´: {nested_keys['key'].nunique():,}\")\n",
    "            \n",
    "            # ì¤‘ì²© êµ¬ì¡° ì˜ˆì‹œ\n",
    "            print(\"\\n   ì¤‘ì²© êµ¬ì¡° ì˜ˆì‹œ (ìƒìœ„ 10ê°œ):\")\n",
    "            nested_examples = nested_keys.groupby('key').size().sort_values(ascending=False).head(10)\n",
    "            for key, count in nested_examples.items():\n",
    "                print(f\"      â€¢ {key}: {count}ê°œ\")\n",
    "        \n",
    "        # ì¹´í…Œê³ ë¦¬ë³„ ì£¼ìš” key ë¶„ì„\n",
    "        print(\"\\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ì£¼ìš” key í†µê³„:\")\n",
    "        category_key_stats = df_hierarchical.groupby(['display_category_major', 'key']).size().reset_index(name='count')\n",
    "        \n",
    "        major_categories = df_hierarchical['display_category_major'].value_counts().head(3).index\n",
    "        for category in major_categories:\n",
    "            cat_data = category_key_stats[category_key_stats['display_category_major'] == category]\n",
    "            top_keys = cat_data.nlargest(5, 'count')\n",
    "            \n",
    "            print(f\"\\n   [{category}] - ìƒìœ„ 5ê°œ key:\")\n",
    "            for _, row in top_keys.iterrows():\n",
    "                level_indicator = \"  \" * (row['key'].count(';'))  # ë“¤ì—¬ì“°ê¸°ë¡œ ë ˆë²¨ í‘œì‹œ\n",
    "                print(f\"      {level_indicator}â€¢ {row['key']}: {row['count']}ê°œ\")\n",
    "        \n",
    "        # ê°€ì¥ ê¹Šì€ ì¤‘ì²© êµ¬ì¡° ì°¾ê¸°\n",
    "        if not nested_keys.empty:\n",
    "            max_nesting = nested_keys['key'].str.count(';').max() + 1\n",
    "            deepest_keys = nested_keys[nested_keys['key'].str.count(';') == (max_nesting - 1)]\n",
    "            \n",
    "            print(f\"\\nğŸ”ï¸ ê°€ì¥ ê¹Šì€ ì¤‘ì²© êµ¬ì¡° (Level {max_nesting}):\")\n",
    "            for _, row in deepest_keys.head(5).iterrows():\n",
    "                print(f\"   â€¢ {row['key']}\")\n",
    "                print(f\"     â†’ value: {row['value']}\")\n",
    "                print(f\"     â†’ category: {row['display_category_major']}\")\n",
    "        \n",
    "        # ê²°ê³¼ë¥¼ ë³€ìˆ˜ì— ì €ì¥\n",
    "        df_hierarchical_final = df_hierarchical\n",
    "        \n",
    "        print(\"\\nğŸ’¾ ë°ì´í„°í”„ë ˆì„ì´ ë‹¤ìŒ ë³€ìˆ˜ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:\")\n",
    "        print(\"   - df_hierarchical_final: ê³„ì¸µ êµ¬ì¡° ë°ì´í„°í”„ë ˆì„\")\n",
    "        print(\"     â€¢ level: ê¹Šì´ (1, 2, 3...)\")\n",
    "        print(\"     â€¢ key: ê³„ì¸µ ê²½ë¡œ (';'ë¡œ êµ¬ë¶„)\")\n",
    "        print(\"     â€¢ value: ì‹¤ì œ ê°’\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        conn.close()\n",
    "else:\n",
    "    print(\"âŒ DB ì—°ê²° ì‹¤íŒ¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cfb0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9g7ralqjsna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. product_specification ê¹Šì´ê°€ 2 ì´ìƒì¸ ì œí’ˆ ì°¾ê¸°\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def get_json_depth(obj, current_depth=0):\n",
    "    \"\"\"JSON ê°ì²´ì˜ ìµœëŒ€ ê¹Šì´ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ê³„ì‚°\"\"\"\n",
    "    if not isinstance(obj, (dict, list)):\n",
    "        return current_depth\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        if not obj:\n",
    "            return current_depth\n",
    "        return max(get_json_depth(v, current_depth + 1) for v in obj.values())\n",
    "    \n",
    "    if isinstance(obj, list):\n",
    "        if not obj:\n",
    "            return current_depth\n",
    "        return max(get_json_depth(item, current_depth) for item in obj)\n",
    "\n",
    "# DBì—ì„œ ë°ì´í„° ë¡œë“œ\n",
    "conn = get_db_connection()\n",
    "if conn:\n",
    "    try:\n",
    "        # product_specificationì´ not nullì¸ ëª¨ë“  ë°ì´í„° ì¡°íšŒ\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            product_id,\n",
    "            display_category_major,\n",
    "            display_category_middle,\n",
    "            display_category_minor,\n",
    "            model_name,\n",
    "            product_name,\n",
    "            product_specification\n",
    "        FROM kt_merged_product_20250929\n",
    "        WHERE product_specification IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        df_products = pd.read_sql_query(query, conn)\n",
    "        print(f\"ğŸ“Š ì „ì²´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_products)}ê°œ í–‰\\n\")\n",
    "        \n",
    "        # ê° ì œí’ˆì˜ JSON ê¹Šì´ ê³„ì‚°\n",
    "        products_with_depth = []\n",
    "        \n",
    "        for idx, row in df_products.iterrows():\n",
    "            try:\n",
    "                spec_str = row['product_specification']\n",
    "                if pd.notna(spec_str):\n",
    "                    # JSON íŒŒì‹±\n",
    "                    if isinstance(spec_str, str):\n",
    "                        spec_json = json.loads(spec_str)\n",
    "                    else:\n",
    "                        spec_json = spec_str\n",
    "                    \n",
    "                    # ê¹Šì´ ê³„ì‚°\n",
    "                    depth = get_json_depth(spec_json)\n",
    "                    \n",
    "                    products_with_depth.append({\n",
    "                        'product_id': row['product_id'],\n",
    "                        'display_category_major': row['display_category_major'],\n",
    "                        'display_category_middle': row['display_category_middle'],\n",
    "                        'display_category_minor': row['display_category_minor'],\n",
    "                        'model_name': row['model_name'],\n",
    "                        'product_name': row['product_name'],\n",
    "                        'depth': depth,\n",
    "                        'spec_sample': str(spec_json)[:200] + '...' if len(str(spec_json)) > 200 else str(spec_json)\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"  âš ï¸ í–‰ {idx} (product_id: {row['product_id']}) ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "        df_depth = pd.DataFrame(products_with_depth)\n",
    "        \n",
    "        # ê¹Šì´ë³„ í†µê³„\n",
    "        print(\"ğŸ“Š JSON ê¹Šì´ë³„ ì œí’ˆ ë¶„í¬:\")\n",
    "        depth_stats = df_depth['depth'].value_counts().sort_index()\n",
    "        for depth, count in depth_stats.items():\n",
    "            percentage = (count / len(df_depth)) * 100\n",
    "            print(f\"   - ê¹Šì´ {depth}: {count:4d}ê°œ ({percentage:5.1f}%)\")\n",
    "        \n",
    "        # ê¹Šì´ê°€ 2 ì´ìƒì¸ ì œí’ˆ í•„í„°ë§\n",
    "        df_deep = df_depth[df_depth['depth'] >= 2].copy()\n",
    "        \n",
    "        print(f\"\\nğŸ” ê¹Šì´ê°€ 2 ì´ìƒì¸ ì œí’ˆ: {len(df_deep)}ê°œ\")\n",
    "        \n",
    "        if len(df_deep) > 0:\n",
    "            # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬\n",
    "            print(\"\\nğŸ“¦ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬ (ê¹Šì´ >= 2):\")\n",
    "            category_dist = df_deep['display_category_major'].value_counts()\n",
    "            for category, count in category_dist.head(10).items():\n",
    "                print(f\"   â€¢ {category}: {count}ê°œ\")\n",
    "            \n",
    "            # ê¹Šì´ë³„ ìƒì„¸ ë¶„ì„\n",
    "            print(\"\\nğŸ“‹ ê¹Šì´ë³„ ì œí’ˆ ëª©ë¡:\")\n",
    "            \n",
    "            for depth in sorted(df_deep['depth'].unique(), reverse=True):\n",
    "                depth_products = df_deep[df_deep['depth'] == depth]\n",
    "                print(f\"\\nâ”â”â” ê¹Šì´ {depth} ({len(depth_products)}ê°œ ì œí’ˆ) â”â”â”\")\n",
    "                \n",
    "                # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ\n",
    "                for idx, (_, row) in enumerate(depth_products.head(10).iterrows(), 1):\n",
    "                    print(f\"\\n{idx}. Product ID: {row['product_id']}\")\n",
    "                    print(f\"   - ì œí’ˆëª…: {row['product_name']}\")\n",
    "                    print(f\"   - ëª¨ë¸ëª…: {row['model_name']}\")\n",
    "                    print(f\"   - ì¹´í…Œê³ ë¦¬: {row['display_category_major']} > {row['display_category_middle']} > {row['display_category_minor']}\")\n",
    "                    print(f\"   - JSON ìƒ˜í”Œ: {row['spec_sample']}\")\n",
    "                \n",
    "                if len(depth_products) > 10:\n",
    "                    print(f\"\\n   ... ì™¸ {len(depth_products) - 10}ê°œ ë” ìˆìŒ\")\n",
    "            \n",
    "            # ê¹Šì´ê°€ ê°€ì¥ ê¹Šì€ ì œí’ˆ ìƒì„¸ ë¶„ì„\n",
    "            max_depth = df_deep['depth'].max()\n",
    "            deepest_products = df_deep[df_deep['depth'] == max_depth]\n",
    "            \n",
    "            print(f\"\\nğŸ”ï¸ ê°€ì¥ ê¹Šì€ êµ¬ì¡°ë¥¼ ê°€ì§„ ì œí’ˆ (ê¹Šì´ {max_depth}):\")\n",
    "            for idx, (_, row) in enumerate(deepest_products.iterrows(), 1):\n",
    "                print(f\"\\n{idx}. {row['product_id']} - {row['product_name']}\")\n",
    "                \n",
    "                # ì‹¤ì œ JSON êµ¬ì¡° íŒŒì‹±í•˜ì—¬ ë³´ì—¬ì£¼ê¸°\n",
    "                spec_data = df_products[df_products['product_id'] == row['product_id']]['product_specification'].iloc[0]\n",
    "                if isinstance(spec_data, str):\n",
    "                    spec_json = json.loads(spec_data)\n",
    "                else:\n",
    "                    spec_json = spec_data\n",
    "                \n",
    "                # JSON êµ¬ì¡°ë¥¼ ë“¤ì—¬ì“°ê¸°ë¡œ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥\n",
    "                print(\"   JSON êµ¬ì¡°:\")\n",
    "                print(json.dumps(spec_json, indent=2, ensure_ascii=False)[:1000])\n",
    "                if len(json.dumps(spec_json)) > 1000:\n",
    "                    print(\"   ... (ì´í•˜ ìƒëµ)\")\n",
    "            \n",
    "            # ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ í‘œì‹œ\n",
    "            print(\"\\nğŸ“Š ê¹Šì´ 2 ì´ìƒ ì œí’ˆ ë°ì´í„°í”„ë ˆì„ (ìƒìœ„ 20ê°œ):\")\n",
    "            display(df_deep[['product_id', 'product_name', 'display_category_major', 'display_category_middle', 'depth']].head(20))\n",
    "            \n",
    "            # CSVë¡œ ì €ì¥í•  ìˆ˜ ìˆë„ë¡ ì¤€ë¹„\n",
    "            df_deep_export = df_deep[['product_id', 'product_name', 'model_name', \n",
    "                                      'display_category_major', 'display_category_middle', \n",
    "                                      'display_category_minor', 'depth']]\n",
    "            \n",
    "            print(f\"\\nğŸ’¾ ë³€ìˆ˜ì— ì €ì¥ë¨:\")\n",
    "            print(f\"   - df_deep_products: ê¹Šì´ 2 ì´ìƒì¸ ì œí’ˆ ì „ì²´ ë°ì´í„°\")\n",
    "            print(f\"   - df_deep_export: ë‚´ë³´ë‚´ê¸°ìš© ë°ì´í„° (spec_sample ì œì™¸)\")\n",
    "            \n",
    "            # ë³€ìˆ˜ì— ì €ì¥\n",
    "            df_deep_products = df_deep\n",
    "            df_deep_export = df_deep_export\n",
    "            \n",
    "        else:\n",
    "            print(\"\\nâœ… ëª¨ë“  ì œí’ˆì˜ JSON ê¹Šì´ê°€ 1ì…ë‹ˆë‹¤.\")\n",
    "            df_deep_products = pd.DataFrame()\n",
    "            df_deep_export = pd.DataFrame()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        conn.close()\n",
    "else:\n",
    "    print(\"âŒ DB ì—°ê²° ì‹¤íŒ¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uywec00nvf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. ì¹´í…Œê³ ë¦¬ë³„ ìƒìœ„ 10ê°œ keyì˜ value ë¶„í¬ ë° outlier ë¶„ì„\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def detect_outliers_in_values(values_list):\n",
    "    \"\"\"ê°’ ë¦¬ìŠ¤íŠ¸ì—ì„œ outlierë¥¼ ê°ì§€\"\"\"\n",
    "    if len(values_list) < 3:\n",
    "        return []\n",
    "    \n",
    "    # ìˆ«ìí˜• ê°’ ì‹œë„\n",
    "    numeric_values = []\n",
    "    for v in values_list:\n",
    "        try:\n",
    "            # ë‹¨ìœ„ ì œê±°í•˜ê³  ìˆ«ìë§Œ ì¶”ì¶œ ì‹œë„\n",
    "            import re\n",
    "            num_match = re.findall(r'[-+]?\\d*\\.?\\d+', str(v))\n",
    "            if num_match:\n",
    "                numeric_values.append(float(num_match[0]))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # ìˆ«ìí˜• outlier ê°ì§€ (IQR ë°©ë²•)\n",
    "    if len(numeric_values) > 3:\n",
    "        q1 = np.percentile(numeric_values, 25)\n",
    "        q3 = np.percentile(numeric_values, 75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        \n",
    "        outliers = []\n",
    "        for i, val in enumerate(numeric_values):\n",
    "            if val < lower_bound or val > upper_bound:\n",
    "                outliers.append((values_list[i], val, 'numeric'))\n",
    "        return outliers\n",
    "    \n",
    "    # ë¬¸ìí˜• ê°’ì˜ ë¹ˆë„ ê¸°ë°˜ outlier (ë§¤ìš° ë“œë¬¸ ê°’)\n",
    "    value_counts = Counter(values_list)\n",
    "    total = len(values_list)\n",
    "    outliers = []\n",
    "    \n",
    "    for value, count in value_counts.items():\n",
    "        if count == 1 and total > 10:  # ì „ì²´ 10ê°œ ì´ìƒ ì¤‘ 1ë²ˆë§Œ ë‚˜íƒ€ë‚œ ê°’\n",
    "            outliers.append((value, count/total, 'frequency'))\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# DBì—ì„œ ë°ì´í„° ë¡œë“œ\n",
    "conn = get_db_connection()\n",
    "if conn:\n",
    "    try:\n",
    "        # product_specificationì´ ìˆëŠ” ëª¨ë“  ë°ì´í„° ì¡°íšŒ\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            product_id,\n",
    "            display_category_major,\n",
    "            display_category_middle,\n",
    "            display_category_minor,\n",
    "            product_name,\n",
    "            product_specification\n",
    "        FROM kt_merged_product_20250929\n",
    "        WHERE product_specification IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        df_products = pd.read_sql_query(query, conn)\n",
    "        print(f\"ğŸ“Š ì „ì²´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_products)}ê°œ í–‰\\n\")\n",
    "        \n",
    "        # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë°ì´í„° ì •ë¦¬\n",
    "        category_key_values = {}\n",
    "        \n",
    "        for idx, row in df_products.iterrows():\n",
    "            try:\n",
    "                category = row['display_category_major']\n",
    "                spec_str = row['product_specification']\n",
    "                \n",
    "                if pd.notna(spec_str):\n",
    "                    # JSON íŒŒì‹±\n",
    "                    if isinstance(spec_str, str):\n",
    "                        spec_json = json.loads(spec_str)\n",
    "                    else:\n",
    "                        spec_json = spec_str\n",
    "                    \n",
    "                    if category not in category_key_values:\n",
    "                        category_key_values[category] = {}\n",
    "                    \n",
    "                    # ê° key-value ì €ì¥\n",
    "                    for key, value in spec_json.items():\n",
    "                        if key not in category_key_values[category]:\n",
    "                            category_key_values[category][key] = []\n",
    "                        \n",
    "                        # value ì²˜ë¦¬\n",
    "                        if isinstance(value, list) and len(value) > 0:\n",
    "                            value_str = str(value[0])\n",
    "                        elif isinstance(value, (dict, list)):\n",
    "                            value_str = json.dumps(value, ensure_ascii=False)\n",
    "                        else:\n",
    "                            value_str = str(value) if value is not None else None\n",
    "                        \n",
    "                        if value_str:\n",
    "                            category_key_values[category][key].append({\n",
    "                                'product_id': row['product_id'],\n",
    "                                'product_name': row['product_name'],\n",
    "                                'value': value_str\n",
    "                            })\n",
    "                            \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # ê° ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„\n",
    "        print(\"=\" * 80)\n",
    "        print(\"ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ìƒìœ„ 10ê°œ Keyì˜ Value ë¶„í¬ ë° Outlier ë¶„ì„\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        all_outliers = []\n",
    "        \n",
    "        for category in sorted(category_key_values.keys()):\n",
    "            print(f\"\\n\\n{'='*60}\")\n",
    "            print(f\"ğŸ“¦ ì¹´í…Œê³ ë¦¬: {category}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ keyë¥¼ ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "            key_frequencies = [(key, len(values)) for key, values in category_key_values[category].items()]\n",
    "            top_keys = sorted(key_frequencies, key=lambda x: x[1], reverse=True)[:10]\n",
    "            \n",
    "            print(f\"ì´ {len(key_frequencies)}ê°œ key ì¤‘ ìƒìœ„ 10ê°œ ë¶„ì„\\n\")\n",
    "            \n",
    "            category_outliers = []\n",
    "            \n",
    "            for i, (key, freq) in enumerate(top_keys, 1):\n",
    "                values_data = category_key_values[category][key]\n",
    "                values_list = [v['value'] for v in values_data]\n",
    "                unique_values = list(set(values_list))\n",
    "                \n",
    "                print(f\"\\n{i}. Key: '{key}'\")\n",
    "                print(f\"   - ì´ ì œí’ˆ ìˆ˜: {len(values_list)}\")\n",
    "                print(f\"   - ê³ ìœ  ê°’ ê°œìˆ˜: {len(unique_values)}\")\n",
    "                \n",
    "                # value ë¶„í¬ ë¶„ì„\n",
    "                value_counter = Counter(values_list)\n",
    "                \n",
    "                # ìƒìœ„ 5ê°œ ë¹ˆë„ ê°’\n",
    "                top_values = value_counter.most_common(5)\n",
    "                print(f\"   - ìµœë¹ˆ ê°’ Top 5:\")\n",
    "                for value, count in top_values:\n",
    "                    percentage = (count / len(values_list)) * 100\n",
    "                    display_value = value[:50] + '...' if len(value) > 50 else value\n",
    "                    print(f\"      â€¢ '{display_value}': {count}ê°œ ({percentage:.1f}%)\")\n",
    "                \n",
    "                # Outlier ê°ì§€\n",
    "                outliers = detect_outliers_in_values(values_list)\n",
    "                \n",
    "                if outliers:\n",
    "                    print(f\"   \\n   ğŸ”´ Outlier ë°œê²¬ ({len(outliers)}ê°œ):\")\n",
    "                    \n",
    "                    # outlier íƒ€ì…ë³„ë¡œ ì •ë¦¬\n",
    "                    numeric_outliers = [o for o in outliers if o[2] == 'numeric']\n",
    "                    frequency_outliers = [o for o in outliers if o[2] == 'frequency']\n",
    "                    \n",
    "                    if numeric_outliers:\n",
    "                        print(f\"      [ìˆ˜ì¹˜í˜• Outlier]\")\n",
    "                        for out_value, numeric_val, _ in numeric_outliers[:5]:\n",
    "                            # í•´ë‹¹ ê°’ì„ ê°€ì§„ ì œí’ˆ ì°¾ê¸°\n",
    "                            products = [v for v in values_data if v['value'] == out_value]\n",
    "                            if products:\n",
    "                                product = products[0]\n",
    "                                print(f\"         - ê°’: '{out_value}' (ìˆ˜ì¹˜: {numeric_val:.2f})\")\n",
    "                                print(f\"           ì œí’ˆ: {product['product_id']} - {product['product_name'][:50]}\")\n",
    "                            \n",
    "                            # ì „ì²´ outlier ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "                            category_outliers.append({\n",
    "                                'category': category,\n",
    "                                'key': key,\n",
    "                                'value': out_value,\n",
    "                                'outlier_type': 'numeric',\n",
    "                                'product_id': product['product_id'] if products else None,\n",
    "                                'product_name': product['product_name'] if products else None\n",
    "                            })\n",
    "                    \n",
    "                    if frequency_outliers and len(unique_values) > 10:\n",
    "                        print(f\"      [ë¹ˆë„ ê¸°ë°˜ Outlier (í¬ê·€ ê°’)]\")\n",
    "                        for out_value, freq_ratio, _ in frequency_outliers[:5]:\n",
    "                            # í•´ë‹¹ ê°’ì„ ê°€ì§„ ì œí’ˆ ì°¾ê¸°\n",
    "                            products = [v for v in values_data if v['value'] == out_value]\n",
    "                            if products:\n",
    "                                product = products[0]\n",
    "                                display_value = out_value[:50] + '...' if len(out_value) > 50 else out_value\n",
    "                                print(f\"         - ê°’: '{display_value}'\")\n",
    "                                print(f\"           ì œí’ˆ: {product['product_id']} - {product['product_name'][:50]}\")\n",
    "                            \n",
    "                            # ì „ì²´ outlier ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "                            category_outliers.append({\n",
    "                                'category': category,\n",
    "                                'key': key,\n",
    "                                'value': out_value,\n",
    "                                'outlier_type': 'frequency',\n",
    "                                'product_id': product['product_id'] if products else None,\n",
    "                                'product_name': product['product_name'] if products else None\n",
    "                            })\n",
    "                \n",
    "                # ê°’ì˜ íŒ¨í„´ ë¶„ì„\n",
    "                if len(unique_values) <= 10:\n",
    "                    print(f\"   - ëª¨ë“  ê³ ìœ  ê°’:\")\n",
    "                    for value in sorted(unique_values)[:10]:\n",
    "                        display_value = value[:50] + '...' if len(value) > 50 else value\n",
    "                        count = value_counter[value]\n",
    "                        print(f\"      â€¢ '{display_value}': {count}ê°œ\")\n",
    "            \n",
    "            all_outliers.extend(category_outliers)\n",
    "            \n",
    "            # ì¹´í…Œê³ ë¦¬ë³„ outlier ìš”ì•½\n",
    "            if category_outliers:\n",
    "                print(f\"\\n   ğŸ“Š {category} ì¹´í…Œê³ ë¦¬ Outlier ìš”ì•½:\")\n",
    "                print(f\"      - ì´ {len(category_outliers)}ê°œ outlier ë°œê²¬\")\n",
    "                outlier_keys = Counter([o['key'] for o in category_outliers])\n",
    "                print(f\"      - Outlierê°€ ìˆëŠ” key: {', '.join(outlier_keys.keys())}\")\n",
    "        \n",
    "        # ì „ì²´ Outlier ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "        if all_outliers:\n",
    "            df_outliers = pd.DataFrame(all_outliers)\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"ğŸ“Š ì „ì²´ Outlier ìš”ì•½\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            print(f\"\\nì´ {len(df_outliers)}ê°œ outlier ë°œê²¬\")\n",
    "            \n",
    "            print(\"\\nì¹´í…Œê³ ë¦¬ë³„ outlier ìˆ˜:\")\n",
    "            category_counts = df_outliers['category'].value_counts()\n",
    "            for cat, count in category_counts.items():\n",
    "                print(f\"   â€¢ {cat}: {count}ê°œ\")\n",
    "            \n",
    "            print(\"\\nOutlier íƒ€ì…ë³„ ë¶„í¬:\")\n",
    "            type_counts = df_outliers['outlier_type'].value_counts()\n",
    "            for type_name, count in type_counts.items():\n",
    "                print(f\"   â€¢ {type_name}: {count}ê°œ\")\n",
    "            \n",
    "            print(\"\\nìƒìœ„ 10ê°œ Outlier ìƒì„¸:\")\n",
    "            display(df_outliers[['category', 'key', 'value', 'outlier_type', 'product_id', 'product_name']].head(10))\n",
    "            \n",
    "            # íŠ¹ì • keyì—ì„œ ìì£¼ outlierê°€ ë°œìƒí•˜ëŠ”ì§€ í™•ì¸\n",
    "            key_outlier_counts = df_outliers.groupby(['category', 'key']).size().sort_values(ascending=False).head(10)\n",
    "            \n",
    "            print(\"\\nOutlierê°€ ë§ì´ ë°œìƒí•œ Key Top 10:\")\n",
    "            for (cat, key), count in key_outlier_counts.items():\n",
    "                print(f\"   â€¢ [{cat}] {key}: {count}ê°œ outlier\")\n",
    "            \n",
    "            print(\"\\nğŸ’¾ ë³€ìˆ˜ì— ì €ì¥ë¨:\")\n",
    "            print(\"   - df_outliers: ì „ì²´ outlier ë°ì´í„°í”„ë ˆì„\")\n",
    "            print(\"   - category_key_values: ì¹´í…Œê³ ë¦¬ë³„ key-value ì „ì²´ ë°ì´í„°\")\n",
    "        else:\n",
    "            print(\"\\nâœ… ëª…í™•í•œ outlierê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "            df_outliers = pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        conn.close()\n",
    "else:\n",
    "    print(\"âŒ DB ì—°ê²° ì‹¤íŒ¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wagr185p7o",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. display_category_major, display_category_middle, keyë³„ ë¶„í¬ ë° outlier ë¶„ì„\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def detect_distribution_outliers(groups_data):\n",
    "    \"\"\"ê·¸ë£¹ë³„ count ë¶„í¬ì—ì„œ outlier ê°ì§€\"\"\"\n",
    "    counts = [g['count'] for g in groups_data]\n",
    "    \n",
    "    if len(counts) < 4:\n",
    "        return []\n",
    "    \n",
    "    # IQR ë°©ë²•ìœ¼ë¡œ outlier ê°ì§€\n",
    "    q1 = np.percentile(counts, 25)\n",
    "    q3 = np.percentile(counts, 75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    outliers = []\n",
    "    for group in groups_data:\n",
    "        if group['count'] < lower_bound or group['count'] > upper_bound:\n",
    "            group['outlier_type'] = 'count_outlier'\n",
    "            group['lower_bound'] = lower_bound\n",
    "            group['upper_bound'] = upper_bound\n",
    "            outliers.append(group)\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# DBì—ì„œ ë°ì´í„° ë¡œë“œ\n",
    "conn = get_db_connection()\n",
    "if conn:\n",
    "    try:\n",
    "        # product_specificationì´ ìˆëŠ” ëª¨ë“  ë°ì´í„° ì¡°íšŒ\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            product_id,\n",
    "            display_category_major,\n",
    "            display_category_middle,\n",
    "            display_category_minor,\n",
    "            product_name,\n",
    "            product_specification\n",
    "        FROM kt_merged_product_20250929\n",
    "        WHERE product_specification IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        df_products = pd.read_sql_query(query, conn)\n",
    "        print(f\"ğŸ“Š ì „ì²´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_products)}ê°œ í–‰\\n\")\n",
    "        \n",
    "        # ì¹´í…Œê³ ë¦¬(major, middle)ì™€ keyë³„ë¡œ ë°ì´í„° ì •ë¦¬\n",
    "        category_key_data = []\n",
    "        \n",
    "        for idx, row in df_products.iterrows():\n",
    "            try:\n",
    "                spec_str = row['product_specification']\n",
    "                \n",
    "                if pd.notna(spec_str):\n",
    "                    # JSON íŒŒì‹±\n",
    "                    if isinstance(spec_str, str):\n",
    "                        spec_json = json.loads(spec_str)\n",
    "                    else:\n",
    "                        spec_json = spec_str\n",
    "                    \n",
    "                    # ê° keyì— ëŒ€í•´ ë°ì´í„° ìˆ˜ì§‘\n",
    "                    for key, value in spec_json.items():\n",
    "                        # value ì²˜ë¦¬\n",
    "                        if isinstance(value, list) and len(value) > 0:\n",
    "                            value_str = str(value[0])\n",
    "                        elif isinstance(value, (dict, list)):\n",
    "                            value_str = json.dumps(value, ensure_ascii=False)\n",
    "                        else:\n",
    "                            value_str = str(value) if value is not None else None\n",
    "                        \n",
    "                        if value_str:\n",
    "                            category_key_data.append({\n",
    "                                'display_category_major': row['display_category_major'],\n",
    "                                'display_category_middle': row['display_category_middle'],\n",
    "                                'display_category_minor': row['display_category_minor'],\n",
    "                                'key': key,\n",
    "                                'value': value_str,\n",
    "                                'product_id': row['product_id'],\n",
    "                                'product_name': row['product_name']\n",
    "                            })\n",
    "                            \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "        df_category_key = pd.DataFrame(category_key_data)\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"ğŸ“Š Category Major, Middle, Keyë³„ ë¶„í¬ ë¶„ì„\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Group by major, middle, key\n",
    "        grouped = df_category_key.groupby(['display_category_major', 'display_category_middle', 'key']).agg({\n",
    "            'product_id': 'count',\n",
    "            'value': lambda x: len(set(x))  # ê³ ìœ  value ê°œìˆ˜\n",
    "        }).rename(columns={'product_id': 'count', 'value': 'unique_values'}).reset_index()\n",
    "        \n",
    "        print(f\"\\nì´ {len(grouped)}ê°œì˜ (major, middle, key) ì¡°í•©\\n\")\n",
    "        \n",
    "        # ê¸°ë³¸ í†µê³„\n",
    "        print(\"ğŸ“ˆ Count ë¶„í¬ í†µê³„:\")\n",
    "        print(f\"   - í‰ê· : {grouped['count'].mean():.2f}\")\n",
    "        print(f\"   - ì¤‘ì•™ê°’: {grouped['count'].median():.2f}\")\n",
    "        print(f\"   - ìµœì†Œê°’: {grouped['count'].min()}\")\n",
    "        print(f\"   - ìµœëŒ€ê°’: {grouped['count'].max()}\")\n",
    "        print(f\"   - í‘œì¤€í¸ì°¨: {grouped['count'].std():.2f}\")\n",
    "        \n",
    "        # Count ê¸°ì¤€ ìƒìœ„/í•˜ìœ„ ê·¸ë£¹\n",
    "        print(\"\\nğŸ“Š Count ê¸°ì¤€ Top 10 ì¡°í•©:\")\n",
    "        top_combinations = grouped.nlargest(10, 'count')\n",
    "        for idx, row in top_combinations.iterrows():\n",
    "            print(f\"   {row['count']:3d}ê°œ: [{row['display_category_major']}] > [{row['display_category_middle']}] > '{row['key']}'\")\n",
    "        \n",
    "        print(\"\\nğŸ“Š Count ê¸°ì¤€ Bottom 10 ì¡°í•© (ê°€ì¥ ì ì€):\")\n",
    "        bottom_combinations = grouped.nsmallest(10, 'count')\n",
    "        for idx, row in bottom_combinations.iterrows():\n",
    "            print(f\"   {row['count']:3d}ê°œ: [{row['display_category_major']}] > [{row['display_category_middle']}] > '{row['key']}'\")\n",
    "        \n",
    "        # ê° major ì¹´í…Œê³ ë¦¬ë³„ outlier ë¶„ì„\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸ” Major ì¹´í…Œê³ ë¦¬ë³„ Outlier ë¶„ì„\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        all_outliers = []\n",
    "        \n",
    "        for major_cat in grouped['display_category_major'].unique():\n",
    "            major_data = grouped[grouped['display_category_major'] == major_cat]\n",
    "            \n",
    "            if len(major_data) < 4:\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nğŸ“¦ {major_cat}\")\n",
    "            print(f\"   - ì´ {len(major_data)}ê°œ (middle, key) ì¡°í•©\")\n",
    "            \n",
    "            # Count ê¸°ë°˜ outlier ì°¾ê¸°\n",
    "            groups_list = major_data.to_dict('records')\n",
    "            outliers = detect_distribution_outliers(groups_list)\n",
    "            \n",
    "            if outliers:\n",
    "                print(f\"   - ğŸ”´ {len(outliers)}ê°œ outlier ë°œê²¬\")\n",
    "                \n",
    "                # ë†’ì€ outlier (ë¹„ì •ìƒì ìœ¼ë¡œ ë§ì€)\n",
    "                high_outliers = [o for o in outliers if o['count'] > o['upper_bound']]\n",
    "                if high_outliers:\n",
    "                    print(f\"\\n   [ë¹„ì •ìƒì ìœ¼ë¡œ ë§ì€ count]\")\n",
    "                    for out in sorted(high_outliers, key=lambda x: x['count'], reverse=True)[:5]:\n",
    "                        print(f\"      â€¢ {out['count']}ê°œ: [{out['display_category_middle']}] > '{out['key']}'\")\n",
    "                        print(f\"        (ì •ìƒ ë²”ìœ„: {out['lower_bound']:.1f} ~ {out['upper_bound']:.1f})\")\n",
    "                \n",
    "                # ë‚®ì€ outlier (ë¹„ì •ìƒì ìœ¼ë¡œ ì ì€)\n",
    "                low_outliers = [o for o in outliers if o['count'] < o['lower_bound']]\n",
    "                if low_outliers:\n",
    "                    print(f\"\\n   [ë¹„ì •ìƒì ìœ¼ë¡œ ì ì€ count]\")\n",
    "                    for out in sorted(low_outliers, key=lambda x: x['count'])[:5]:\n",
    "                        print(f\"      â€¢ {out['count']}ê°œ: [{out['display_category_middle']}] > '{out['key']}'\")\n",
    "                \n",
    "                all_outliers.extend(outliers)\n",
    "        \n",
    "        # Middle ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸ” Middle ì¹´í…Œê³ ë¦¬ë³„ Key ë¶„í¬ ë¶„ì„\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        middle_key_stats = grouped.groupby('display_category_middle').agg({\n",
    "            'key': 'count',\n",
    "            'count': ['sum', 'mean', 'std']\n",
    "        }).round(2)\n",
    "        \n",
    "        middle_key_stats.columns = ['unique_keys', 'total_products', 'avg_products_per_key', 'std_products']\n",
    "        middle_key_stats = middle_key_stats.sort_values('total_products', ascending=False)\n",
    "        \n",
    "        print(\"\\nMiddle ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ (ìƒìœ„ 15ê°œ):\")\n",
    "        display(middle_key_stats.head(15))\n",
    "        \n",
    "        # Keyë³„ ì¹´í…Œê³ ë¦¬ ë¶„í¬ ë¶„ì„\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸ”‘ Keyë³„ ì¹´í…Œê³ ë¦¬ ë¶„í¬ ë¶„ì„\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        key_category_stats = grouped.groupby('key').agg({\n",
    "            'display_category_major': 'nunique',\n",
    "            'display_category_middle': 'nunique',\n",
    "            'count': ['sum', 'mean', 'std']\n",
    "        }).round(2)\n",
    "        \n",
    "        key_category_stats.columns = ['major_categories', 'middle_categories', 'total_products', 'avg_products', 'std_products']\n",
    "        key_category_stats = key_category_stats.sort_values('total_products', ascending=False)\n",
    "        \n",
    "        print(\"\\nKeyë³„ ì¹´í…Œê³ ë¦¬ ë¶„í¬ (ìƒìœ„ 20ê°œ):\")\n",
    "        display(key_category_stats.head(20))\n",
    "        \n",
    "        # íŠ¹ì´ íŒ¨í„´ ì°¾ê¸°\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸ¯ íŠ¹ì´ íŒ¨í„´ ë¶„ì„\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # 1. í•œ middle ì¹´í…Œê³ ë¦¬ì—ë§Œ ìˆëŠ” unique key\n",
    "        unique_keys_per_middle = grouped.groupby('key')['display_category_middle'].nunique()\n",
    "        single_middle_keys = unique_keys_per_middle[unique_keys_per_middle == 1].index\n",
    "        \n",
    "        if len(single_middle_keys) > 0:\n",
    "            print(f\"\\n1. ë‹¨ì¼ Middle ì¹´í…Œê³ ë¦¬ ì „ìš© Key ({len(single_middle_keys)}ê°œ ì¤‘ 10ê°œ):\")\n",
    "            for key in list(single_middle_keys)[:10]:\n",
    "                middle_cat = grouped[grouped['key'] == key]['display_category_middle'].iloc[0]\n",
    "                count = grouped[grouped['key'] == key]['count'].iloc[0]\n",
    "                print(f\"   â€¢ '{key}' â†’ [{middle_cat}] ({count}ê°œ ì œí’ˆ)\")\n",
    "        \n",
    "        # 2. ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ì— ê±¸ì³ìˆëŠ” key\n",
    "        multi_category_keys = key_category_stats[key_category_stats['middle_categories'] >= 5].head(10)\n",
    "        \n",
    "        if len(multi_category_keys) > 0:\n",
    "            print(f\"\\n2. 5ê°œ ì´ìƒ Middle ì¹´í…Œê³ ë¦¬ì— ê±¸ì¹œ ë²”ìš© Key:\")\n",
    "            for key, row in multi_category_keys.iterrows():\n",
    "                print(f\"   â€¢ '{key}': {int(row['middle_categories'])}ê°œ middle ì¹´í…Œê³ ë¦¬, {int(row['total_products'])}ê°œ ì œí’ˆ\")\n",
    "        \n",
    "        # 3. Value ë‹¤ì–‘ì„±ì´ ë†’ì€ ì¡°í•©\n",
    "        high_diversity = grouped.nlargest(10, 'unique_values')\n",
    "        \n",
    "        print(f\"\\n3. Value ë‹¤ì–‘ì„±ì´ ë†’ì€ ì¡°í•© (ê³ ìœ ê°’ì´ ë§ì€):\")\n",
    "        for idx, row in high_diversity.iterrows():\n",
    "            print(f\"   â€¢ {row['unique_values']}ê°œ ê³ ìœ ê°’: [{row['display_category_major']}] > [{row['display_category_middle']}] > '{row['key']}'\")\n",
    "        \n",
    "        # Outlier ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "        if all_outliers:\n",
    "            df_group_outliers = pd.DataFrame(all_outliers)\n",
    "            print(f\"\\n\\nğŸ’¾ ê²°ê³¼ ì €ì¥:\")\n",
    "            print(f\"   - df_grouped: ì „ì²´ ê·¸ë£¹ë³„ í†µê³„ ë°ì´í„°í”„ë ˆì„\")\n",
    "            print(f\"   - df_group_outliers: ê·¸ë£¹ outlier ë°ì´í„°í”„ë ˆì„\")\n",
    "            print(f\"   - middle_key_stats: Middle ì¹´í…Œê³ ë¦¬ë³„ í†µê³„\")\n",
    "            print(f\"   - key_category_stats: Keyë³„ ì¹´í…Œê³ ë¦¬ ë¶„í¬ í†µê³„\")\n",
    "        else:\n",
    "            df_group_outliers = pd.DataFrame()\n",
    "            print(f\"\\n\\nğŸ’¾ ê²°ê³¼ ì €ì¥:\")\n",
    "            print(f\"   - df_grouped: ì „ì²´ ê·¸ë£¹ë³„ í†µê³„ ë°ì´í„°í”„ë ˆì„\")\n",
    "            print(f\"   - middle_key_stats: Middle ì¹´í…Œê³ ë¦¬ë³„ í†µê³„\")\n",
    "            print(f\"   - key_category_stats: Keyë³„ ì¹´í…Œê³ ë¦¬ ë¶„í¬ í†µê³„\")\n",
    "        \n",
    "        # ë³€ìˆ˜ ì €ì¥\n",
    "        df_grouped = grouped\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        conn.close()\n",
    "else:\n",
    "    print(\"âŒ DB ì—°ê²° ì‹¤íŒ¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mund3u8cv5j",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ unique valuesë¥¼ ë¶„ì„í•˜ì—¬ íŒŒì¼ë¡œ ì¶œë ¥ (ë¹„ìœ¨ í¬í•¨)\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def analyze_all_categories_to_file():\n",
    "    \"\"\"ëª¨ë“  middle ì¹´í…Œê³ ë¦¬ì˜ ê° keyë³„ unique valuesë¥¼ ë¶„ì„í•˜ì—¬ íŒŒì¼ë¡œ ì €ì¥ (ë¹„ìœ¨ í¬í•¨)\"\"\"\n",
    "    \n",
    "    conn = get_db_connection()\n",
    "    if not conn:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # ëª¨ë“  ë°ì´í„° ì¡°íšŒ\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            display_category_major,\n",
    "            display_category_middle,\n",
    "            display_category_minor,\n",
    "            product_id,\n",
    "            product_specification\n",
    "        FROM kt_merged_product_20250929\n",
    "        WHERE product_specification IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        df_products = pd.read_sql_query(query, conn)\n",
    "        print(f\"ğŸ“Š ì „ì²´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_products)}ê°œ ì œí’ˆ\")\n",
    "        \n",
    "        # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì œí’ˆ ìˆ˜ ê³„ì‚° (ë¹„ìœ¨ ê³„ì‚°ìš©)\n",
    "        category_product_counts = {}\n",
    "        \n",
    "        # ì¹´í…Œê³ ë¦¬ë³„ë¡œ unique values ìˆ˜ì§‘\n",
    "        category_key_values = {}\n",
    "        category_key_products = {}  # ê° keyê°€ ë‚˜íƒ€ë‚œ ì œí’ˆ ID ì¶”ì \n",
    "        \n",
    "        for idx, row in df_products.iterrows():\n",
    "            try:\n",
    "                major = row['display_category_major']\n",
    "                middle = row['display_category_middle']\n",
    "                product_id = row['product_id']\n",
    "                spec_str = row['product_specification']\n",
    "                \n",
    "                if pd.notna(spec_str):\n",
    "                    # JSON íŒŒì‹±\n",
    "                    if isinstance(spec_str, str):\n",
    "                        spec_json = json.loads(spec_str)\n",
    "                    else:\n",
    "                        spec_json = spec_str\n",
    "                    \n",
    "                    # ì¹´í…Œê³ ë¦¬ë³„ ì œí’ˆ ìˆ˜ ì¹´ìš´íŠ¸\n",
    "                    if major not in category_product_counts:\n",
    "                        category_product_counts[major] = {'total': set(), 'middle': {}}\n",
    "                    if middle not in category_product_counts[major]['middle']:\n",
    "                        category_product_counts[major]['middle'][middle] = set()\n",
    "                    \n",
    "                    category_product_counts[major]['total'].add(product_id)\n",
    "                    category_product_counts[major]['middle'][middle].add(product_id)\n",
    "                    \n",
    "                    # ì¹´í…Œê³ ë¦¬ ì¡°í•© í‚¤ ìƒì„±\n",
    "                    category_key = (major, middle)\n",
    "                    \n",
    "                    if category_key not in category_key_values:\n",
    "                        category_key_values[category_key] = {}\n",
    "                        category_key_products[category_key] = {}\n",
    "                    \n",
    "                    # ê° key-value ìˆ˜ì§‘\n",
    "                    for key, value in spec_json.items():\n",
    "                        if key not in category_key_values[category_key]:\n",
    "                            category_key_values[category_key][key] = set()\n",
    "                            category_key_products[category_key][key] = set()\n",
    "                        \n",
    "                        # value ì²˜ë¦¬\n",
    "                        if isinstance(value, list) and len(value) > 0:\n",
    "                            value_str = str(value[0])\n",
    "                        elif isinstance(value, (dict, list)):\n",
    "                            value_str = json.dumps(value, ensure_ascii=False)\n",
    "                        else:\n",
    "                            value_str = str(value) if value is not None else None\n",
    "                        \n",
    "                        if value_str:\n",
    "                            category_key_values[category_key][key].add(value_str)\n",
    "                            category_key_products[category_key][key].add(product_id)\n",
    "                            \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        print(f\"âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(category_key_values)}ê°œ ì¹´í…Œê³ ë¦¬ ì¡°í•©\")\n",
    "        \n",
    "        # ì¹´í…Œê³ ë¦¬ë³„ ì´ ì œí’ˆ ìˆ˜ ê³„ì‚°\n",
    "        category_totals = {}\n",
    "        for major, data in category_product_counts.items():\n",
    "            category_totals[major] = {\n",
    "                'total': len(data['total']),\n",
    "                'middle': {middle: len(products) for middle, products in data['middle'].items()}\n",
    "            }\n",
    "        \n",
    "        # ê²°ê³¼ë¥¼ ì •ë¦¬í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "        result_data = []\n",
    "        \n",
    "        for (major, middle), keys_dict in category_key_values.items():\n",
    "            major_total = category_totals[major]['total']\n",
    "            middle_total = category_totals[major]['middle'][middle]\n",
    "            \n",
    "            for key, values in keys_dict.items():\n",
    "                sorted_values = sorted(list(values))\n",
    "                \n",
    "                # ì´ keyë¥¼ ê°€ì§„ ì œí’ˆ ìˆ˜\n",
    "                products_with_key = len(category_key_products[(major, middle)][key])\n",
    "                \n",
    "                # ë¹„ìœ¨ ê³„ì‚°\n",
    "                ratio_in_middle = (products_with_key / middle_total * 100) if middle_total > 0 else 0\n",
    "                ratio_in_major = (products_with_key / major_total * 100) if major_total > 0 else 0\n",
    "                \n",
    "                # íƒ­ìœ¼ë¡œ êµ¬ë¶„ëœ ê°’ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "                # ê°’ë“¤ ì‚¬ì´ëŠ” ì„¸ë¯¸ì½œë¡ (;)ìœ¼ë¡œ êµ¬ë¶„\n",
    "                values_str = '; '.join(sorted_values)\n",
    "                \n",
    "                result_data.append({\n",
    "                    'display_category_major': major,\n",
    "                    'display_category_middle': middle,\n",
    "                    'key': key,\n",
    "                    'product_count': products_with_key,\n",
    "                    'middle_total': middle_total,\n",
    "                    'ratio_in_middle': round(ratio_in_middle, 2),\n",
    "                    'major_total': major_total,\n",
    "                    'ratio_in_major': round(ratio_in_major, 2),\n",
    "                    'unique_values_count': len(sorted_values),\n",
    "                    'unique_values': values_str\n",
    "                })\n",
    "        \n",
    "        # ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì •ë ¬\n",
    "        df_result = pd.DataFrame(result_data)\n",
    "        df_result = df_result.sort_values(['display_category_major', 'display_category_middle', 'key'])\n",
    "        \n",
    "        # í†µê³„ ì¶œë ¥\n",
    "        print(f\"\\nğŸ“Š ë¶„ì„ í†µê³„:\")\n",
    "        print(f\"   - ì´ ë ˆì½”ë“œ ìˆ˜: {len(df_result):,}\")\n",
    "        print(f\"   - Major ì¹´í…Œê³ ë¦¬ ìˆ˜: {df_result['display_category_major'].nunique()}\")\n",
    "        print(f\"   - Middle ì¹´í…Œê³ ë¦¬ ìˆ˜: {df_result['display_category_middle'].nunique()}\")\n",
    "        print(f\"   - ê³ ìœ  Key ìˆ˜: {df_result['key'].nunique()}\")\n",
    "        \n",
    "        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„\n",
    "        print(f\"\\nğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ í†µê³„:\")\n",
    "        category_stats = df_result.groupby(['display_category_major', 'display_category_middle']).agg({\n",
    "            'key': 'count',\n",
    "            'middle_total': 'first'\n",
    "        }).rename(columns={'key': 'key_count', 'middle_total': 'product_count'}).reset_index()\n",
    "        category_stats = category_stats.sort_values('product_count', ascending=False)\n",
    "        \n",
    "        print(\"\\nìƒìœ„ 10ê°œ ì¹´í…Œê³ ë¦¬:\")\n",
    "        for idx, row in category_stats.head(10).iterrows():\n",
    "            print(f\"   â€¢ [{row['display_category_major']}] {row['display_category_middle']}: \"\n",
    "                  f\"{row['product_count']}ê°œ ì œí’ˆ, {row['key_count']}ê°œ key\")\n",
    "        \n",
    "        # íŒŒì¼ë¡œ ì €ì¥ (íƒ­ êµ¬ë¶„)\n",
    "        output_file = 'category_middle_spec_analysis.txt'\n",
    "        \n",
    "        # ì €ì¥í•  ì»¬ëŸ¼ ìˆœì„œ ì •ì˜\n",
    "        columns_order = [\n",
    "            'display_category_major', \n",
    "            'display_category_middle', \n",
    "            'key', \n",
    "            'product_count',\n",
    "            'middle_total',\n",
    "            'ratio_in_middle',\n",
    "            'major_total', \n",
    "            'ratio_in_major',\n",
    "            'unique_values_count',\n",
    "            'unique_values'\n",
    "        ]\n",
    "        \n",
    "        # íƒ­ìœ¼ë¡œ êµ¬ë¶„ëœ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥\n",
    "        df_result[columns_order].to_csv(output_file, sep='\\t', index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}\")\n",
    "        print(f\"   - íŒŒì¼ í¬ê¸°: {os.path.getsize(output_file) / 1024:.2f} KB\")\n",
    "        print(f\"   - ì´ {len(df_result):,}ê°œ í–‰\")\n",
    "        print(f\"   - ì—‘ì…€ì—ì„œ ì—´ ë•Œ: í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° â†’ íƒ­ìœ¼ë¡œ êµ¬ë¶„\")\n",
    "        \n",
    "        # ìƒ˜í”Œ ë°ì´í„° í‘œì‹œ (ë¹„ìœ¨ í¬í•¨)\n",
    "        print(f\"\\nğŸ“‹ ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ í–‰):\")\n",
    "        display_cols = ['display_category_major', 'display_category_middle', 'key', \n",
    "                       'product_count', 'ratio_in_middle', 'ratio_in_major', 'unique_values_count']\n",
    "        display(df_result[display_cols].head(10))\n",
    "        \n",
    "        # ë¹„ìœ¨ì´ ë†’ì€ keyë“¤ ë¶„ì„\n",
    "        print(f\"\\nğŸ“ˆ Middle ì¹´í…Œê³ ë¦¬ ë‚´ ë¹„ìœ¨ì´ ë†’ì€ Key Top 10 (90% ì´ìƒ):\")\n",
    "        high_ratio = df_result[df_result['ratio_in_middle'] >= 90].sort_values('ratio_in_middle', ascending=False)\n",
    "        for idx, row in high_ratio.head(10).iterrows():\n",
    "            print(f\"   â€¢ {row['ratio_in_middle']:.1f}%: [{row['display_category_major']}] \"\n",
    "                  f\"{row['display_category_middle']} > '{row['key']}' ({row['product_count']}/{row['middle_total']})\")\n",
    "        \n",
    "        # ë¹„ìœ¨ì´ ë‚®ì€ keyë“¤ ë¶„ì„\n",
    "        print(f\"\\nğŸ“‰ Middle ì¹´í…Œê³ ë¦¬ ë‚´ ë¹„ìœ¨ì´ ë‚®ì€ Key (10% ë¯¸ë§Œ, ìµœì†Œ 5ê°œ ì œí’ˆ):\")\n",
    "        low_ratio = df_result[(df_result['ratio_in_middle'] < 10) & (df_result['product_count'] >= 5)]\n",
    "        low_ratio = low_ratio.sort_values('ratio_in_middle')\n",
    "        for idx, row in low_ratio.head(10).iterrows():\n",
    "            print(f\"   â€¢ {row['ratio_in_middle']:.1f}%: [{row['display_category_major']}] \"\n",
    "                  f\"{row['display_category_middle']} > '{row['key']}' ({row['product_count']}/{row['middle_total']})\")\n",
    "        \n",
    "        # íŠ¹ì • ì¹´í…Œê³ ë¦¬ ìƒì„¸ ë³´ê¸°\n",
    "        print(f\"\\nğŸ” íŠ¹ì • ì¹´í…Œê³ ë¦¬ ì˜ˆì‹œ (ê°¤ëŸ­ì‹œ ìŠ¤ë§ˆíŠ¸í°):\")\n",
    "        galaxy_data = df_result[df_result['display_category_middle'] == 'ê°¤ëŸ­ì‹œ ìŠ¤ë§ˆíŠ¸í°']\n",
    "        if not galaxy_data.empty:\n",
    "            galaxy_data = galaxy_data.sort_values('ratio_in_middle', ascending=False)\n",
    "            for idx, row in galaxy_data.head(5).iterrows():\n",
    "                print(f\"\\n   Key: '{row['key']}'\")\n",
    "                print(f\"   ì œí’ˆ ìˆ˜: {row['product_count']}/{row['middle_total']} ({row['ratio_in_middle']:.1f}%)\")\n",
    "                print(f\"   Major ë‚´ ë¹„ìœ¨: {row['ratio_in_major']:.1f}%\")\n",
    "                print(f\"   ê³ ìœ ê°’ ê°œìˆ˜: {row['unique_values_count']}ê°œ\")\n",
    "                # ê°’ì´ ë„ˆë¬´ ê¸¸ë©´ ì¼ë¶€ë§Œ í‘œì‹œ\n",
    "                values_preview = row['unique_values']\n",
    "                if len(values_preview) > 150:\n",
    "                    values_preview = values_preview[:150] + '...'\n",
    "                print(f\"   Values: {values_preview}\")\n",
    "        \n",
    "        # ì¶”ê°€ë¡œ Excel í˜•ì‹ìœ¼ë¡œë„ ì €ì¥ (ì„ íƒì‚¬í•­)\n",
    "        excel_file = 'category_middle_spec_analysis.xlsx'\n",
    "        try:\n",
    "            # unique_valuesê°€ ë„ˆë¬´ ê¸¸ë©´ Excel ì…€ ì œí•œ(32,767ì)ì— ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œí•œ\n",
    "            df_excel = df_result.copy()\n",
    "            df_excel['unique_values'] = df_excel['unique_values'].apply(\n",
    "                lambda x: x[:32000] + '...(truncated)' if len(x) > 32000 else x\n",
    "            )\n",
    "            df_excel[columns_order].to_excel(excel_file, index=False, engine='openpyxl')\n",
    "            print(f\"\\nğŸ’¾ Excel íŒŒì¼ë„ ì €ì¥ë¨: {excel_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâš ï¸ Excel ì €ì¥ ì‹¤íŒ¨ (íŒŒì¼ì´ ë„ˆë¬´ í¼): {e}\")\n",
    "            print(f\"   â†’ í…ìŠ¤íŠ¸ íŒŒì¼({output_file})ì„ ì‚¬ìš©í•˜ì„¸ìš”\")\n",
    "        \n",
    "        print(f\"\\nâœ… ë¶„ì„ ì™„ë£Œ!\")\n",
    "        print(f\"   - df_all_categories_analysis: ì „ì²´ ë¶„ì„ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„\")\n",
    "        \n",
    "        return df_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# ëª¨ë“  ì¹´í…Œê³ ë¦¬ ë¶„ì„ ì‹¤í–‰\n",
    "df_all_categories_analysis = analyze_all_categories_to_file()\n",
    "\n",
    "# íŒŒì¼ì´ ì œëŒ€ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
    "if os.path.exists('category_middle_spec_analysis.txt'):\n",
    "    print(f\"\\nâœ… íŒŒì¼ í™•ì¸: category_middle_spec_analysis.txt\")\n",
    "    print(f\"   íŒŒì¼ ê²½ë¡œ: {os.path.abspath('category_middle_spec_analysis.txt')}\")\n",
    "    \n",
    "    # íŒŒì¼ ì²˜ìŒ ëª‡ ì¤„ ë¯¸ë¦¬ë³´ê¸°\n",
    "    with open('category_middle_spec_analysis.txt', 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()[:5]\n",
    "        print(f\"\\nğŸ“„ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 5ì¤„):\")\n",
    "        for i, line in enumerate(lines, 1):\n",
    "            # íƒ­ìœ¼ë¡œ êµ¬ë¶„ëœ í•„ë“œ í™•ì¸\n",
    "            fields = line.strip().split('\\t')\n",
    "            if i == 1:  # í—¤ë”\n",
    "                print(f\"   í—¤ë”: {len(fields)}ê°œ ì»¬ëŸ¼\")\n",
    "                print(f\"   ì»¬ëŸ¼: {fields[:5]}... (ì²˜ìŒ 5ê°œ)\")\n",
    "            else:\n",
    "                print(f\"   {i}: {fields[2] if len(fields) > 2 else ''} - \"\n",
    "                      f\"{fields[5] if len(fields) > 5 else ''}% in middle, \"\n",
    "                      f\"{fields[7] if len(fields) > 7 else ''}% in major\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vknoybp5b5o",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. display_category_minorê¹Œì§€ í¬í•¨í•œ ìƒì„¸ ë¶„ì„\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def analyze_all_categories_with_minor():\n",
    "    \"\"\"major, middle, minor ì¹´í…Œê³ ë¦¬ì˜ ê° keyë³„ unique valuesë¥¼ ë¶„ì„í•˜ì—¬ íŒŒì¼ë¡œ ì €ì¥ (ë¹„ìœ¨ í¬í•¨)\"\"\"\n",
    "    \n",
    "    conn = get_db_connection()\n",
    "    if not conn:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # ëª¨ë“  ë°ì´í„° ì¡°íšŒ\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            display_category_major,\n",
    "            display_category_middle,\n",
    "            display_category_minor,\n",
    "            product_id,\n",
    "            product_specification\n",
    "        FROM kt_merged_product_20250929\n",
    "        WHERE product_specification IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        df_products = pd.read_sql_query(query, conn)\n",
    "        print(f\"ğŸ“Š ì „ì²´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df_products)}ê°œ ì œí’ˆ\")\n",
    "        \n",
    "        # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì œí’ˆ ìˆ˜ ê³„ì‚° (ë¹„ìœ¨ ê³„ì‚°ìš©)\n",
    "        category_product_counts = {}\n",
    "        \n",
    "        # ì¹´í…Œê³ ë¦¬ë³„ë¡œ unique values ìˆ˜ì§‘\n",
    "        category_key_values = {}\n",
    "        category_key_products = {}  # ê° keyê°€ ë‚˜íƒ€ë‚œ ì œí’ˆ ID ì¶”ì \n",
    "        \n",
    "        for idx, row in df_products.iterrows():\n",
    "            try:\n",
    "                major = row['display_category_major']\n",
    "                middle = row['display_category_middle']\n",
    "                minor = row['display_category_minor']\n",
    "                product_id = row['product_id']\n",
    "                spec_str = row['product_specification']\n",
    "                \n",
    "                if pd.notna(spec_str):\n",
    "                    # JSON íŒŒì‹±\n",
    "                    if isinstance(spec_str, str):\n",
    "                        spec_json = json.loads(spec_str)\n",
    "                    else:\n",
    "                        spec_json = spec_str\n",
    "                    \n",
    "                    # ì¹´í…Œê³ ë¦¬ë³„ ì œí’ˆ ìˆ˜ ì¹´ìš´íŠ¸\n",
    "                    if major not in category_product_counts:\n",
    "                        category_product_counts[major] = {'total': set(), 'middle': {}}\n",
    "                    if middle not in category_product_counts[major]['middle']:\n",
    "                        category_product_counts[major]['middle'][middle] = {'total': set(), 'minor': {}}\n",
    "                    if minor not in category_product_counts[major]['middle'][middle]['minor']:\n",
    "                        category_product_counts[major]['middle'][middle]['minor'][minor] = set()\n",
    "                    \n",
    "                    category_product_counts[major]['total'].add(product_id)\n",
    "                    category_product_counts[major]['middle'][middle]['total'].add(product_id)\n",
    "                    category_product_counts[major]['middle'][middle]['minor'][minor].add(product_id)\n",
    "                    \n",
    "                    # ì¹´í…Œê³ ë¦¬ ì¡°í•© í‚¤ ìƒì„± (minorê¹Œì§€ í¬í•¨)\n",
    "                    category_key = (major, middle, minor)\n",
    "                    \n",
    "                    if category_key not in category_key_values:\n",
    "                        category_key_values[category_key] = {}\n",
    "                        category_key_products[category_key] = {}\n",
    "                    \n",
    "                    # ê° key-value ìˆ˜ì§‘\n",
    "                    for key, value in spec_json.items():\n",
    "                        if key not in category_key_values[category_key]:\n",
    "                            category_key_values[category_key][key] = set()\n",
    "                            category_key_products[category_key][key] = set()\n",
    "                        \n",
    "                        # value ì²˜ë¦¬\n",
    "                        if isinstance(value, list) and len(value) > 0:\n",
    "                            value_str = str(value[0])\n",
    "                        elif isinstance(value, (dict, list)):\n",
    "                            value_str = json.dumps(value, ensure_ascii=False)\n",
    "                        else:\n",
    "                            value_str = str(value) if value is not None else None\n",
    "                        \n",
    "                        if value_str:\n",
    "                            category_key_values[category_key][key].add(value_str)\n",
    "                            category_key_products[category_key][key].add(product_id)\n",
    "                            \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        print(f\"âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(category_key_values)}ê°œ ì¹´í…Œê³ ë¦¬ ì¡°í•© (minor í¬í•¨)\")\n",
    "        \n",
    "        # ì¹´í…Œê³ ë¦¬ë³„ ì´ ì œí’ˆ ìˆ˜ ê³„ì‚°\n",
    "        category_totals = {}\n",
    "        for major, major_data in category_product_counts.items():\n",
    "            category_totals[major] = {\n",
    "                'total': len(major_data['total']),\n",
    "                'middle': {}\n",
    "            }\n",
    "            for middle, middle_data in major_data['middle'].items():\n",
    "                category_totals[major]['middle'][middle] = {\n",
    "                    'total': len(middle_data['total']),\n",
    "                    'minor': {minor: len(products) for minor, products in middle_data['minor'].items()}\n",
    "                }\n",
    "        \n",
    "        # ê²°ê³¼ë¥¼ ì •ë¦¬í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "        result_data = []\n",
    "        \n",
    "        for (major, middle, minor), keys_dict in category_key_values.items():\n",
    "            major_total = category_totals[major]['total']\n",
    "            middle_total = category_totals[major]['middle'][middle]['total']\n",
    "            minor_total = category_totals[major]['middle'][middle]['minor'].get(minor, 0)\n",
    "            \n",
    "            for key, values in keys_dict.items():\n",
    "                sorted_values = sorted(list(values))\n",
    "                \n",
    "                # ì´ keyë¥¼ ê°€ì§„ ì œí’ˆ ìˆ˜\n",
    "                products_with_key = len(category_key_products[(major, middle, minor)][key])\n",
    "                \n",
    "                # ë¹„ìœ¨ ê³„ì‚°\n",
    "                ratio_in_minor = (products_with_key / minor_total * 100) if minor_total > 0 else 0\n",
    "                ratio_in_middle = (products_with_key / middle_total * 100) if middle_total > 0 else 0\n",
    "                ratio_in_major = (products_with_key / major_total * 100) if major_total > 0 else 0\n",
    "                \n",
    "                # íƒ­ìœ¼ë¡œ êµ¬ë¶„ëœ ê°’ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "                # ê°’ë“¤ ì‚¬ì´ëŠ” ì„¸ë¯¸ì½œë¡ (;)ìœ¼ë¡œ êµ¬ë¶„\n",
    "                values_str = '; '.join(sorted_values)\n",
    "                \n",
    "                result_data.append({\n",
    "                    'display_category_major': major,\n",
    "                    'display_category_middle': middle,\n",
    "                    'display_category_minor': minor,\n",
    "                    'key': key,\n",
    "                    'product_count': products_with_key,\n",
    "                    'minor_total': minor_total,\n",
    "                    'ratio_in_minor': round(ratio_in_minor, 2),\n",
    "                    'middle_total': middle_total,\n",
    "                    'ratio_in_middle': round(ratio_in_middle, 2),\n",
    "                    'major_total': major_total,\n",
    "                    'ratio_in_major': round(ratio_in_major, 2),\n",
    "                    'unique_values_count': len(sorted_values),\n",
    "                    'unique_values': values_str\n",
    "                })\n",
    "        \n",
    "        # ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì •ë ¬\n",
    "        df_result = pd.DataFrame(result_data)\n",
    "        df_result = df_result.sort_values(['display_category_major', 'display_category_middle', 'display_category_minor', 'key'])\n",
    "        \n",
    "        # í†µê³„ ì¶œë ¥\n",
    "        print(f\"\\nğŸ“Š ë¶„ì„ í†µê³„ (Minor í¬í•¨):\")\n",
    "        print(f\"   - ì´ ë ˆì½”ë“œ ìˆ˜: {len(df_result):,}\")\n",
    "        print(f\"   - Major ì¹´í…Œê³ ë¦¬ ìˆ˜: {df_result['display_category_major'].nunique()}\")\n",
    "        print(f\"   - Middle ì¹´í…Œê³ ë¦¬ ìˆ˜: {df_result['display_category_middle'].nunique()}\")\n",
    "        print(f\"   - Minor ì¹´í…Œê³ ë¦¬ ìˆ˜: {df_result['display_category_minor'].nunique()}\")\n",
    "        print(f\"   - ê³ ìœ  Key ìˆ˜: {df_result['key'].nunique()}\")\n",
    "        \n",
    "        # Minor ì¹´í…Œê³ ë¦¬ë³„ í†µê³„\n",
    "        print(f\"\\nğŸ“‹ Minor ì¹´í…Œê³ ë¦¬ë³„ í†µê³„:\")\n",
    "        category_stats = df_result.groupby(['display_category_major', 'display_category_middle', 'display_category_minor']).agg({\n",
    "            'key': 'count',\n",
    "            'minor_total': 'first'\n",
    "        }).rename(columns={'key': 'key_count', 'minor_total': 'product_count'}).reset_index()\n",
    "        category_stats = category_stats.sort_values('product_count', ascending=False)\n",
    "        \n",
    "        print(\"\\nìƒìœ„ 10ê°œ Minor ì¹´í…Œê³ ë¦¬:\")\n",
    "        for idx, row in category_stats.head(10).iterrows():\n",
    "            print(f\"   â€¢ [{row['display_category_major']}] > [{row['display_category_middle']}] > [{row['display_category_minor']}]: \"\n",
    "                  f\"{row['product_count']}ê°œ ì œí’ˆ, {row['key_count']}ê°œ key\")\n",
    "        \n",
    "        # íŒŒì¼ë¡œ ì €ì¥ (íƒ­ êµ¬ë¶„)\n",
    "        output_file = 'category_minor_spec_analysis.txt'\n",
    "        \n",
    "        # ì €ì¥í•  ì»¬ëŸ¼ ìˆœì„œ ì •ì˜\n",
    "        columns_order = [\n",
    "            'display_category_major', \n",
    "            'display_category_middle',\n",
    "            'display_category_minor',\n",
    "            'key', \n",
    "            'product_count',\n",
    "            'minor_total',\n",
    "            'ratio_in_minor',\n",
    "            'middle_total',\n",
    "            'ratio_in_middle',\n",
    "            'major_total', \n",
    "            'ratio_in_major',\n",
    "            'unique_values_count',\n",
    "            'unique_values'\n",
    "        ]\n",
    "        \n",
    "        # íƒ­ìœ¼ë¡œ êµ¬ë¶„ëœ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥\n",
    "        df_result[columns_order].to_csv(output_file, sep='\\t', index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}\")\n",
    "        print(f\"   - íŒŒì¼ í¬ê¸°: {os.path.getsize(output_file) / 1024:.2f} KB\")\n",
    "        print(f\"   - ì´ {len(df_result):,}ê°œ í–‰\")\n",
    "        print(f\"   - ì—‘ì…€ì—ì„œ ì—´ ë•Œ: í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° â†’ íƒ­ìœ¼ë¡œ êµ¬ë¶„\")\n",
    "        \n",
    "        # ìƒ˜í”Œ ë°ì´í„° í‘œì‹œ (ë¹„ìœ¨ í¬í•¨)\n",
    "        print(f\"\\nğŸ“‹ ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ í–‰):\")\n",
    "        display_cols = ['display_category_major', 'display_category_middle', 'display_category_minor', 'key', \n",
    "                       'product_count', 'ratio_in_minor', 'ratio_in_middle', 'ratio_in_major', 'unique_values_count']\n",
    "        display(df_result[display_cols].head(10))\n",
    "        \n",
    "        # Minor ì¹´í…Œê³ ë¦¬ ë‚´ ë¹„ìœ¨ì´ ë†’ì€ keyë“¤ ë¶„ì„\n",
    "        print(f\"\\nğŸ“ˆ Minor ì¹´í…Œê³ ë¦¬ ë‚´ ë¹„ìœ¨ì´ 100%ì¸ Key (ëª¨ë“  ì œí’ˆì´ ê°€ì§„ key):\")\n",
    "        perfect_ratio = df_result[df_result['ratio_in_minor'] == 100].sort_values(['minor_total', 'display_category_minor'], ascending=[False, True])\n",
    "        for idx, row in perfect_ratio.head(15).iterrows():\n",
    "            print(f\"   â€¢ [{row['display_category_major']}] > [{row['display_category_middle']}] > [{row['display_category_minor']}] > '{row['key']}' \"\n",
    "                  f\"({row['product_count']}/{row['minor_total']})\")\n",
    "        \n",
    "        # Minorë³„ key ë‹¤ì–‘ì„± ë¶„ì„\n",
    "        minor_diversity = df_result.groupby(['display_category_minor']).agg({\n",
    "            'key': 'nunique',\n",
    "            'minor_total': 'first'\n",
    "        }).rename(columns={'key': 'unique_keys'}).reset_index()\n",
    "        minor_diversity = minor_diversity.sort_values('unique_keys', ascending=False)\n",
    "        \n",
    "        print(f\"\\nğŸ¯ Key ë‹¤ì–‘ì„±ì´ ë†’ì€ Minor ì¹´í…Œê³ ë¦¬ Top 10:\")\n",
    "        for idx, row in minor_diversity.head(10).iterrows():\n",
    "            print(f\"   â€¢ {row['display_category_minor']}: {row['unique_keys']}ê°œ ê³ ìœ  key ({row['minor_total']}ê°œ ì œí’ˆ)\")\n",
    "        \n",
    "        # íŠ¹ì • Minor ì¹´í…Œê³ ë¦¬ ìƒì„¸ ë³´ê¸°\n",
    "        print(f\"\\nğŸ” íŠ¹ì • Minor ì¹´í…Œê³ ë¦¬ ì˜ˆì‹œ (ê°¤ëŸ­ì‹œ S):\")\n",
    "        galaxy_s_data = df_result[df_result['display_category_minor'] == 'ê°¤ëŸ­ì‹œ S']\n",
    "        if not galaxy_s_data.empty:\n",
    "            galaxy_s_data = galaxy_s_data.sort_values('ratio_in_minor', ascending=False)\n",
    "            for idx, row in galaxy_s_data.head(5).iterrows():\n",
    "                print(f\"\\n   Key: '{row['key']}'\")\n",
    "                print(f\"   ì œí’ˆ ìˆ˜: {row['product_count']}/{row['minor_total']} (Minor: {row['ratio_in_minor']:.1f}%)\")\n",
    "                print(f\"   Middle ë‚´ ë¹„ìœ¨: {row['ratio_in_middle']:.1f}%\")\n",
    "                print(f\"   Major ë‚´ ë¹„ìœ¨: {row['ratio_in_major']:.1f}%\")\n",
    "                print(f\"   ê³ ìœ ê°’ ê°œìˆ˜: {row['unique_values_count']}ê°œ\")\n",
    "                # ê°’ì´ ë„ˆë¬´ ê¸¸ë©´ ì¼ë¶€ë§Œ í‘œì‹œ\n",
    "                values_preview = row['unique_values']\n",
    "                if len(values_preview) > 100:\n",
    "                    values_preview = values_preview[:100] + '...'\n",
    "                print(f\"   Values: {values_preview}\")\n",
    "        \n",
    "        # Minor ì¹´í…Œê³ ë¦¬ê°€ ì—†ëŠ” ê²½ìš° ë¶„ì„\n",
    "        null_minor = df_result[df_result['display_category_minor'].isna() | (df_result['display_category_minor'] == '')]\n",
    "        if not null_minor.empty:\n",
    "            print(f\"\\nâš ï¸ Minor ì¹´í…Œê³ ë¦¬ê°€ ì—†ëŠ” ë ˆì½”ë“œ: {len(null_minor)}ê°œ\")\n",
    "        \n",
    "        # ì¶”ê°€ë¡œ Excel í˜•ì‹ìœ¼ë¡œë„ ì €ì¥ (ì„ íƒì‚¬í•­)\n",
    "        excel_file = 'category_minor_spec_analysis.xlsx'\n",
    "        try:\n",
    "            # unique_valuesê°€ ë„ˆë¬´ ê¸¸ë©´ Excel ì…€ ì œí•œ(32,767ì)ì— ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œí•œ\n",
    "            df_excel = df_result.copy()\n",
    "            df_excel['unique_values'] = df_excel['unique_values'].apply(\n",
    "                lambda x: x[:32000] + '...(truncated)' if len(x) > 32000 else x\n",
    "            )\n",
    "            df_excel[columns_order].to_excel(excel_file, index=False, engine='openpyxl')\n",
    "            print(f\"\\nğŸ’¾ Excel íŒŒì¼ë„ ì €ì¥ë¨: {excel_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâš ï¸ Excel ì €ì¥ ì‹¤íŒ¨ (íŒŒì¼ì´ ë„ˆë¬´ í¼): {e}\")\n",
    "            print(f\"   â†’ í…ìŠ¤íŠ¸ íŒŒì¼({output_file})ì„ ì‚¬ìš©í•˜ì„¸ìš”\")\n",
    "        \n",
    "        print(f\"\\nâœ… ë¶„ì„ ì™„ë£Œ!\")\n",
    "        print(f\"   - df_all_categories_minor_analysis: Minorê¹Œì§€ í¬í•¨í•œ ì „ì²´ ë¶„ì„ ê²°ê³¼\")\n",
    "        \n",
    "        return df_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Minor ì¹´í…Œê³ ë¦¬ê¹Œì§€ í¬í•¨í•œ ë¶„ì„ ì‹¤í–‰\n",
    "df_all_categories_minor_analysis = analyze_all_categories_with_minor()\n",
    "\n",
    "# íŒŒì¼ì´ ì œëŒ€ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
    "if os.path.exists('category_minor_spec_analysis.txt'):\n",
    "    print(f\"\\nâœ… íŒŒì¼ í™•ì¸: category_minor_spec_analysis.txt\")\n",
    "    print(f\"   íŒŒì¼ ê²½ë¡œ: {os.path.abspath('category_minor_spec_analysis.txt')}\")\n",
    "    \n",
    "    # íŒŒì¼ ì²˜ìŒ ëª‡ ì¤„ ë¯¸ë¦¬ë³´ê¸°\n",
    "    with open('category_minor_spec_analysis.txt', 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()[:5]\n",
    "        print(f\"\\nğŸ“„ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 5ì¤„):\")\n",
    "        for i, line in enumerate(lines, 1):\n",
    "            # íƒ­ìœ¼ë¡œ êµ¬ë¶„ëœ í•„ë“œ í™•ì¸\n",
    "            fields = line.strip().split('\\t')\n",
    "            if i == 1:  # í—¤ë”\n",
    "                print(f\"   í—¤ë”: {len(fields)}ê°œ ì»¬ëŸ¼\")\n",
    "                print(f\"   ì»¬ëŸ¼ ëª©ë¡:\")\n",
    "                for j, col in enumerate(fields[:10]):  # ì²˜ìŒ 10ê°œ ì»¬ëŸ¼ë§Œ\n",
    "                    print(f\"      {j+1}. {col}\")\n",
    "            else:\n",
    "                if len(fields) >= 7:\n",
    "                    print(f\"   {i}: [{fields[2]}] '{fields[3]}' - \"\n",
    "                          f\"{fields[6]}% in minor, {fields[8]}% in middle, {fields[10]}% in major\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381110fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993f387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_galaxy_smartphone_values.sort_values(['display_category_major','display_category_middle','key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0ebdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monitor_values.sort_values(['display_category_major','display_category_middle','key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a0df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tv_values.sort_values(['display_category_major','display_category_middle','key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e21140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PC/ì£¼ë³€ê¸°ê¸°\tëª¨ë‹ˆí„°\t215\n",
    "# ëª¨ë°”ì¼\tê°¤ëŸ­ì‹œ ìŠ¤ë§ˆíŠ¸í°\t94\n",
    "# TV\tTV\t175\n",
    "\n",
    "result = df_grouped.groupby(['display_category_major', 'display_category_middle']).size().reset_index().rename(columns={0:'count'})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4671a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.sort_values(by=['count'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d877b156",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_grouped' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m pd.set_option(\u001b[33m'\u001b[39m\u001b[33mdisplay.max_columns\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;66;03m## ëª¨ë“  ì—´ì„ ì¶œë ¥í•œë‹¤.\u001b[39;00m\n\u001b[32m      6\u001b[39m pd.set_option(\u001b[33m'\u001b[39m\u001b[33mdisplay.max_rows\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;66;03m## ëª¨ë“  ì—´ì„ ì¶œë ¥í•œë‹¤.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mdf_grouped\u001b[49m[df_grouped[\u001b[33m'\u001b[39m\u001b[33mdisplay_category_middle\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mê°¤ëŸ­ì‹œ ìŠ¤ë§ˆíŠ¸í°\u001b[39m\u001b[33m'\u001b[39m].head(\u001b[32m100\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# for idx, token in enumerate(df_grouped[df_grouped['display_category_middle'] == 'ê°¤ëŸ­ì‹œ ìŠ¤ë§ˆíŠ¸í°'].iterrows()):\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m#     print(f\"{idx} : {token[1]}\")\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df_grouped' is not defined"
     ]
    }
   ],
   "source": [
    "# PC/ì£¼ë³€ê¸°ê¸°\tëª¨ë‹ˆí„°\t215\n",
    "# ëª¨ë°”ì¼\tê°¤ëŸ­ì‹œ ìŠ¤ë§ˆíŠ¸í°\t94\n",
    "# TV\tTV\t175\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None) ## ëª¨ë“  ì—´ì„ ì¶œë ¥í•œë‹¤.\n",
    "pd.set_option('display.max_rows', None) ## ëª¨ë“  ì—´ì„ ì¶œë ¥í•œë‹¤.\n",
    " \n",
    "df_grouped[df_grouped['display_category_middle'] == 'ê°¤ëŸ­ì‹œ ìŠ¤ë§ˆíŠ¸í°'].head(100)\n",
    "# for idx, token in enumerate(df_grouped[df_grouped['display_category_middle'] == 'ê°¤ëŸ­ì‹œ ìŠ¤ë§ˆíŠ¸í°'].iterrows()):\n",
    "#     print(f\"{idx} : {token[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab9833",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f9cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "/Users/toby/prog/kt/rubicon/data/kt_spec_validation_table_20251021.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6694ccc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc435c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
