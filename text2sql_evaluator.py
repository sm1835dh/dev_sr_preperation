import pandas as pd
import numpy as np
from typing import List, Dict, Tuple, Optional
from datetime import datetime
import json
import os
from dotenv import load_dotenv
import psycopg2
from sqlalchemy import create_engine

# Load environment variables
load_dotenv('.env')

# PostgreSQL settings
PG_HOST = os.getenv('PG_HOST')
PG_PORT = os.getenv('PG_PORT')
PG_DATABASE = os.getenv('PG_DATABASE')
PG_USER = os.getenv('PG_USER')
PG_PASSWORD = os.getenv('PG_PASSWORD')

def get_db_connection():
    """PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return psycopg2.connect(
        host=PG_HOST,
        port=PG_PORT,
        database=PG_DATABASE,
        user=PG_USER,
        password=PG_PASSWORD
    )

class Text2SQLEvaluator:
    """Text2SQL í‰ê°€ë¥¼ ìœ„í•œ í´ë˜ìŠ¤"""

    def __init__(self):
        self.connection = None
        self.evaluation_results = []

    def connect_db(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        try:
            self.connection = get_db_connection()
            print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
            return True
        except Exception as e:
            print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            return False

    def close_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ"""
        if self.connection:
            self.connection.close()
            print("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ")

    def get_ground_truth(self, query_number: int) -> Tuple[Optional[List[str]], Optional[str], Optional[str]]:
        """
        ì •ë‹µ product_id list ì¡°íšŒ

        Returns:
            Tuple of (product_id_list, category, instruction)
        """
        try:
            cursor = self.connection.cursor()
            query = """
                SELECT product_id_list, category, instruction
                FROM tc_check_table_20251015
                WHERE query_number = %s
            """
            cursor.execute(query, (query_number,))
            result = cursor.fetchone()
            cursor.close()

            if result:
                return result[0], result[1], result[2]
            else:
                print(f"âš ï¸ ì¿¼ë¦¬ ë²ˆí˜¸ {query_number}ì— ëŒ€í•œ ì •ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None, None, None

        except Exception as e:
            print(f"âŒ ì •ë‹µ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None, None, None

    def calculate_metrics(self, predicted: List[str], ground_truth: List[str]) -> Dict:
        """
        ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µì„ ë¹„êµí•˜ì—¬ í‰ê°€ ì§€í‘œ ê³„ì‚°

        Args:
            predicted: ì˜ˆì¸¡ëœ product_id ë¦¬ìŠ¤íŠ¸
            ground_truth: ì •ë‹µ product_id ë¦¬ìŠ¤íŠ¸

        Returns:
            í‰ê°€ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
        """
        # Setìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ
        pred_set = set(predicted) if predicted else set()
        truth_set = set(ground_truth) if ground_truth else set()

        # True Positives: ì˜ˆì¸¡ê³¼ ì •ë‹µ ëª¨ë‘ì— ìˆëŠ” í•­ëª©
        tp = len(pred_set.intersection(truth_set))

        # False Positives: ì˜ˆì¸¡ì—ë§Œ ìˆê³  ì •ë‹µì—ëŠ” ì—†ëŠ” í•­ëª©
        fp = len(pred_set - truth_set)

        # False Negatives: ì •ë‹µì—ë§Œ ìˆê³  ì˜ˆì¸¡ì—ëŠ” ì—†ëŠ” í•­ëª©
        fn = len(truth_set - pred_set)

        # Precision, Recall, F1 Score ê³„ì‚°
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

        # Exact Match (ì™„ì „ ì¼ì¹˜ ì—¬ë¶€)
        exact_match = 1 if pred_set == truth_set else 0

        # Jaccard Similarity (IoU)
        jaccard = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0

        return {
            'TP': tp,
            'FP': fp,
            'FN': fn,
            'Precision': round(precision, 4),
            'Recall': round(recall, 4),
            'F1_Score': round(f1_score, 4),
            'Exact_Match': exact_match,
            'Jaccard_Similarity': round(jaccard, 4),
            'Predicted_Count': len(pred_set),
            'Ground_Truth_Count': len(truth_set)
        }

    def evaluate_single_query(self, query_number: int, predicted_ids: List[str],
                            verbose: bool = False) -> Dict:
        """
        ë‹¨ì¼ ì¿¼ë¦¬ì— ëŒ€í•œ í‰ê°€ ìˆ˜í–‰

        Args:
            query_number: ì¿¼ë¦¬ ë²ˆí˜¸
            predicted_ids: ì˜ˆì¸¡ëœ product_id ë¦¬ìŠ¤íŠ¸
            verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€

        Returns:
            í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # ì •ë‹µ ì¡°íšŒ
        ground_truth_ids, category, instruction = self.get_ground_truth(query_number)

        if ground_truth_ids is None:
            return None

        # í‰ê°€ ì§€í‘œ ê³„ì‚°
        metrics = self.calculate_metrics(predicted_ids, ground_truth_ids)

        # ê²°ê³¼ì— ë©”íƒ€ ì •ë³´ ì¶”ê°€
        result = {
            'query_number': query_number,
            'category': category,
            'instruction': instruction[:50] + '...' if instruction and len(instruction) > 50 else instruction,
            **metrics
        }

        # ê²°ê³¼ ì €ì¥
        self.evaluation_results.append(result)

        if verbose:
            print(f"\n{'='*60}")
            print(f"ì¿¼ë¦¬ ë²ˆí˜¸: {query_number}")
            print(f"ì¹´í…Œê³ ë¦¬: {category}")
            print(f"{'='*60}")
            print(f"ì˜ˆì¸¡ëœ product_id ìˆ˜: {metrics['Predicted_Count']}")
            print(f"ì •ë‹µ product_id ìˆ˜: {metrics['Ground_Truth_Count']}")
            print(f"\n[í‰ê°€ ì§€í‘œ]")
            print(f"  - TP (True Positives): {metrics['TP']}")
            print(f"  - FP (False Positives): {metrics['FP']}")
            print(f"  - FN (False Negatives): {metrics['FN']}")
            print(f"  - Precision: {metrics['Precision']:.2%}")
            print(f"  - Recall: {metrics['Recall']:.2%}")
            print(f"  - F1 Score: {metrics['F1_Score']:.2%}")
            print(f"  - Exact Match: {'âœ…' if metrics['Exact_Match'] else 'âŒ'}")
            print(f"  - Jaccard Similarity: {metrics['Jaccard_Similarity']:.2%}")

        return result

    def evaluate_batch(self, query_predictions: List[Tuple[int, List[str]]],
                      verbose: bool = False) -> pd.DataFrame:
        """
        ì—¬ëŸ¬ ì¿¼ë¦¬ì— ëŒ€í•œ ì¼ê´„ í‰ê°€

        Args:
            query_predictions: [(query_number, predicted_ids), ...] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸
            verbose: ê° ì¿¼ë¦¬ë³„ ìƒì„¸ ì¶œë ¥ ì—¬ë¶€

        Returns:
            í‰ê°€ ê²°ê³¼ DataFrame
        """
        # í‰ê°€ ì‹œì‘ ì „ì— ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”í•˜ì§€ ì•ŠìŒ (ëˆ„ì  í‰ê°€ë¥¼ ìœ„í•´)
        batch_results = []

        for query_number, predicted_ids in query_predictions:
            result = self.evaluate_single_query(query_number, predicted_ids, verbose)
            if result:
                batch_results.append(result)

        # ë°°ì¹˜ ê²°ê³¼ë§Œ DataFrameìœ¼ë¡œ ë°˜í™˜
        return pd.DataFrame(batch_results)

    def get_overall_metrics(self) -> Dict:
        """
        ì „ì²´ í‰ê°€ ê²°ê³¼ ìš”ì•½

        Returns:
            ì „ì²´ í‰ê°€ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
        """
        if not self.evaluation_results:
            print("í‰ê°€ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        df = pd.DataFrame(self.evaluation_results)

        # ì¤‘ë³µ ì œê±° (ê°™ì€ query_numberê°€ ì—¬ëŸ¬ ë²ˆ í‰ê°€ëœ ê²½ìš° ìµœì‹  ê²°ê³¼ë§Œ ì‚¬ìš©)
        df = df.drop_duplicates(subset=['query_number'], keep='last')

        # ì „ì²´ í†µê³„
        total_tp = df['TP'].sum()
        total_fp = df['FP'].sum()
        total_fn = df['FN'].sum()

        # Micro-averaged metrics (ì „ì²´ TP, FP, FN ê¸°ì¤€)
        micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
        micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
        micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0

        # Macro-averaged metrics (ê° ì¿¼ë¦¬ë³„ í‰ê· )
        macro_precision = df['Precision'].mean()
        macro_recall = df['Recall'].mean()
        macro_f1 = df['F1_Score'].mean()

        # ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥
        category_metrics = None
        if 'category' in df.columns and df['category'].notna().any():
            category_df = df.groupby('category').agg({
                'Precision': 'mean',
                'Recall': 'mean',
                'F1_Score': 'mean',
                'Exact_Match': 'mean',
                'query_number': 'count'
            }).round(4)
            category_df.rename(columns={'query_number': 'Count'}, inplace=True)
            # DataFrameì„ Dictë¡œ ë³€í™˜
            category_metrics = category_df.to_dict('index')

        overall_metrics = {
            'total_queries': len(df),
            'total_tp': int(total_tp),
            'total_fp': int(total_fp),
            'total_fn': int(total_fn),
            'micro_precision': round(micro_precision, 4),
            'micro_recall': round(micro_recall, 4),
            'micro_f1': round(micro_f1, 4),
            'macro_precision': round(macro_precision, 4),
            'macro_recall': round(macro_recall, 4),
            'macro_f1': round(macro_f1, 4),
            'exact_match_rate': round(df['Exact_Match'].mean(), 4),
            'avg_jaccard': round(df['Jaccard_Similarity'].mean(), 4),
            'category_metrics': category_metrics
        }

        return overall_metrics

    def print_overall_report(self):
        """
        ì „ì²´ í‰ê°€ ë¦¬í¬íŠ¸ ì¶œë ¥
        """
        metrics = self.get_overall_metrics()

        if not metrics:
            return

        print("\n" + "="*70)
        print(" " * 25 + "ğŸ“Š ì „ì²´ í‰ê°€ ê²°ê³¼")
        print("="*70)

        print(f"\n[í‰ê°€ ëŒ€ìƒ]")
        print(f"  ì´ ì¿¼ë¦¬ ìˆ˜: {metrics['total_queries']}ê°œ")

        print(f"\n[ì „ì²´ í†µê³„]")
        print(f"  - Total TP: {metrics['total_tp']}")
        print(f"  - Total FP: {metrics['total_fp']}")
        print(f"  - Total FN: {metrics['total_fn']}")

        print(f"\n[Micro-averaged Metrics] (ì „ì²´ TP, FP, FN ê¸°ì¤€)")
        print(f"  - Precision: {metrics['micro_precision']:.2%}")
        print(f"  - Recall: {metrics['micro_recall']:.2%}")
        print(f"  - F1 Score: {metrics['micro_f1']:.2%}")

        print(f"\n[Macro-averaged Metrics] (ì¿¼ë¦¬ë³„ í‰ê· )")
        print(f"  - Precision: {metrics['macro_precision']:.2%}")
        print(f"  - Recall: {metrics['macro_recall']:.2%}")
        print(f"  - F1 Score: {metrics['macro_f1']:.2%}")

        print(f"\n[ì¶”ê°€ ì§€í‘œ]")
        print(f"  - Exact Match Rate: {metrics['exact_match_rate']:.2%}")
        print(f"  - Average Jaccard Similarity: {metrics['avg_jaccard']:.2%}")

        if metrics['category_metrics'] is not None:
            print(f"\n[ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥]")
            for category, cat_metrics in metrics['category_metrics'].items():
                print(f"  {category}:")
                for key, value in cat_metrics.items():
                    print(f"    - {key}: {value}")

        print("\n" + "="*70)

    def save_results(self, filename: str = None):
        """
        í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥

        Args:
            filename: ì €ì¥í•  íŒŒì¼ëª… (ê¸°ë³¸ê°’: text2sql_eval_YYYYMMDD_HHMMSS.csv)
        """
        if not self.evaluation_results:
            print("ì €ì¥í•  í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"text2sql_eval_{timestamp}.csv"

        df = pd.DataFrame(self.evaluation_results)
        # ì¤‘ë³µ ì œê±°
        df = df.drop_duplicates(subset=['query_number'], keep='last')
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… í‰ê°€ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ì´ {len(df)}ê°œ ì¿¼ë¦¬)")

    def reset_results(self):
        """í‰ê°€ ê²°ê³¼ ì´ˆê¸°í™”"""
        self.evaluation_results = []
        print("í‰ê°€ ê²°ê³¼ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def get_results_dataframe(self) -> pd.DataFrame:
        """
        í˜„ì¬ê¹Œì§€ì˜ í‰ê°€ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë°˜í™˜

        Returns:
            í‰ê°€ ê²°ê³¼ DataFrame
        """
        if not self.evaluation_results:
            return pd.DataFrame()

        df = pd.DataFrame(self.evaluation_results)
        # ì¤‘ë³µ ì œê±°
        df = df.drop_duplicates(subset=['query_number'], keep='last')
        return df