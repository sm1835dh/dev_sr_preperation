import logging
import json
import csv
from datetime import datetime, date
from decimal import Decimal
from db_connection import DatabaseConnection
from table_discovery import TableDiscovery
from column_extractor import ColumnExtractor
from data_sampler import DataSampler

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class TableAnalyzer:
    def __init__(self, env_file='.env'):
        """í…Œì´ë¸” ë¶„ì„ê¸° í´ë˜ìŠ¤"""
        self.db_conn = DatabaseConnection(env_file)
        self.engine = None
        self.table_discovery = None
        self.column_extractor = None
        self.data_sampler = None

    def initialize(self):
        """ëª¨ë“ˆ ì´ˆê¸°í™”"""
        try:
            self.engine = self.db_conn.connect()
            self.table_discovery = TableDiscovery(self.engine)
            self.column_extractor = ColumnExtractor(self.engine)
            self.data_sampler = DataSampler(self.engine)
            logger.info("í…Œì´ë¸” ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def analyze_tables_with_prefix(self, prefix='table_', schema='public'):
        """ì§€ì •ëœ ì ‘ë‘ì‚¬ë¥¼ ê°€ì§„ í…Œì´ë¸”ë“¤ ë¶„ì„"""
        try:
            # í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ
            tables = self.table_discovery.find_tables_by_prefix(prefix, schema)

            if not tables:
                logger.info(f"'{prefix}' ì ‘ë‘ì‚¬ë¥¼ ê°€ì§„ í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤.")
                return []

            results = []
            for table in tables:
                table_name = table['table_name']
                logger.info(f"í…Œì´ë¸” '{table_name}' ë¶„ì„ ì‹œì‘")

                table_result = self.analyze_single_table(table_name, schema)
                results.append(table_result)

            logger.info(f"ì´ {len(results)}ê°œ í…Œì´ë¸” ë¶„ì„ ì™„ë£Œ")
            return results

        except Exception as e:
            logger.error(f"í…Œì´ë¸” ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def analyze_single_table(self, table_name, schema='public'):
        """ë‹¨ì¼ í…Œì´ë¸” ë¶„ì„"""
        try:
            # ì»¬ëŸ¼ ì •ë³´ ì¶”ì¶œ
            columns = self.column_extractor.get_table_columns(table_name, schema)

            # ë°ì´í„° ìƒ˜í”Œ ì¶”ì¶œ
            samples = self.data_sampler.get_recent_data_samples(table_name, columns, schema, 3)

            # ë°ì´í„° ìš”ì•½ ì •ë³´
            data_summary = self.data_sampler.get_table_data_summary(table_name, columns, schema)

            # ê²°ê³¼ êµ¬ì„±
            table_result = {
                'table_name': table_name,
                'schema': schema,
                'total_columns': len(columns),
                'total_rows': data_summary['total_rows'],
                'columns': [],
                'sample_data': samples,
                'analyzed_at': datetime.now().isoformat()
            }

            # ê° ì»¬ëŸ¼ì— ëŒ€í•œ ìƒì„¸ ì •ë³´
            for col in columns:
                col_name = col['column_name']
                formatted_type = self.column_extractor.format_column_type(col)

                # ìƒ˜í”Œ ë°ì´í„°ì—ì„œ í•´ë‹¹ ì»¬ëŸ¼ì˜ ê°’ë“¤ ì¶”ì¶œ
                sample_values = []
                for sample in samples[:3]:  # ìµœê·¼ 3ê°œ ë°ì´í„°
                    value = sample.get(col_name)
                    formatted_value = self.data_sampler.format_sample_value(value)
                    sample_values.append(formatted_value)

                # ë¶€ì¡±í•œ ìƒ˜í”Œ ë°ì´í„°ëŠ” 'N/A'ë¡œ ì±„ì›€
                while len(sample_values) < 3:
                    sample_values.append('N/A')

                column_info = {
                    'column_name': col_name,
                    'data_type': formatted_type,
                    'is_nullable': col['is_nullable'],
                    'comment': col['comment'] or '',
                    'recent_data_1': sample_values[0],
                    'recent_data_2': sample_values[1],
                    'recent_data_3': sample_values[2],
                    'null_count': data_summary['column_stats'].get(col_name, {}).get('null_count', 0)
                }

                table_result['columns'].append(column_info)

            return table_result

        except Exception as e:
            logger.error(f"í…Œì´ë¸” '{table_name}' ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒì‹œì—ë„ ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
            return {
                'table_name': table_name,
                'schema': schema,
                'error': str(e),
                'analyzed_at': datetime.now().isoformat()
            }

    def export_to_json(self, results, filename=None):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"table_analysis_result_{timestamp}.json"

        class DateTimeEncoder(json.JSONEncoder):
            """datetime ë° ê¸°íƒ€ íƒ€ì…ì„ JSON ì§ë ¬í™”í•˜ê¸° ìœ„í•œ ì»¤ìŠ¤í…€ ì¸ì½”ë”"""
            def default(self, obj):
                if isinstance(obj, (datetime, date)):
                    return obj.isoformat()
                elif isinstance(obj, Decimal):
                    return float(obj)
                elif hasattr(obj, '__dict__'):
                    return str(obj)
                return super().default(obj)

        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, cls=DateTimeEncoder)

            logger.info(f"ë¶„ì„ ê²°ê³¼ê°€ '{filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return filename

        except Exception as e:
            logger.error(f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def export_to_csv(self, results, prefix=None):
        """ê° í…Œì´ë¸”ë³„ë¡œ CSV íŒŒì¼ë¡œ ì €ì¥"""
        if prefix is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            prefix = f"table_columns_{timestamp}"

        csv_files = []

        for result in results:
            if 'error' in result:
                logger.warning(f"í…Œì´ë¸” '{result['table_name']}' CSV ì €ì¥ ê±´ë„ˆëœ€ (ì˜¤ë¥˜ ë°œìƒ)")
                continue

            table_name = result['table_name']
            filename = f"{prefix}_{table_name}.csv"

            try:
                with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
                    # CSV í—¤ë” ì •ì˜
                    fieldnames = [
                        'í…Œì´ë¸”ëª…',
                        'ì»¬ëŸ¼ëª…',
                        'íƒ€ì…',
                        'ì½”ë©˜íŠ¸',
                        'ìµœê·¼ ë°ì´í„° 1',
                        'ìµœê·¼ ë°ì´í„° 2',
                        'ìµœê·¼ ë°ì´í„° 3',
                        'NULL í—ˆìš©',
                        'NULL ê°œìˆ˜',
                        'ì´ í–‰ ìˆ˜'
                    ]

                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()

                    # ê° ì»¬ëŸ¼ ì •ë³´ë¥¼ í–‰ìœ¼ë¡œ ì‘ì„±
                    for col in result.get('columns', []):
                        row = {
                            'í…Œì´ë¸”ëª…': table_name,
                            'ì»¬ëŸ¼ëª…': col['column_name'],
                            'íƒ€ì…': col['data_type'],
                            'ì½”ë©˜íŠ¸': col.get('comment', ''),
                            'ìµœê·¼ ë°ì´í„° 1': self._format_csv_value(col.get('recent_data_1', '')),
                            'ìµœê·¼ ë°ì´í„° 2': self._format_csv_value(col.get('recent_data_2', '')),
                            'ìµœê·¼ ë°ì´í„° 3': self._format_csv_value(col.get('recent_data_3', '')),
                            'NULL í—ˆìš©': 'Y' if col.get('is_nullable') == 'YES' else 'N',
                            'NULL ê°œìˆ˜': col.get('null_count', 0),
                            'ì´ í–‰ ìˆ˜': result.get('total_rows', 0)
                        }
                        writer.writerow(row)

                logger.info(f"CSV íŒŒì¼ '{filename}' ì €ì¥ ì™„ë£Œ")
                csv_files.append(filename)

            except Exception as e:
                logger.error(f"í…Œì´ë¸” '{table_name}' CSV ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

        return csv_files

    def export_combined_csv(self, results, filename=None):
        """ëª¨ë“  í…Œì´ë¸” ì •ë³´ë¥¼ í•˜ë‚˜ì˜ CSV íŒŒì¼ë¡œ ì €ì¥"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"all_tables_columns_{timestamp}.csv"

        try:
            with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
                fieldnames = [
                    'í…Œì´ë¸”ëª…',
                    'ì»¬ëŸ¼ëª…',
                    'íƒ€ì…',
                    'ì½”ë©˜íŠ¸',
                    'ìµœê·¼ ë°ì´í„° 1',
                    'ìµœê·¼ ë°ì´í„° 2',
                    'ìµœê·¼ ë°ì´í„° 3',
                    'NULL í—ˆìš©',
                    'NULL ê°œìˆ˜',
                    'ì´ í–‰ ìˆ˜'
                ]

                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()

                for result in results:
                    if 'error' in result:
                        continue

                    table_name = result['table_name']
                    total_rows = result.get('total_rows', 0)

                    for col in result.get('columns', []):
                        row = {
                            'í…Œì´ë¸”ëª…': table_name,
                            'ì»¬ëŸ¼ëª…': col['column_name'],
                            'íƒ€ì…': col['data_type'],
                            'ì½”ë©˜íŠ¸': col.get('comment', ''),
                            'ìµœê·¼ ë°ì´í„° 1': self._format_csv_value(col.get('recent_data_1', '')),
                            'ìµœê·¼ ë°ì´í„° 2': self._format_csv_value(col.get('recent_data_2', '')),
                            'ìµœê·¼ ë°ì´í„° 3': self._format_csv_value(col.get('recent_data_3', '')),
                            'NULL í—ˆìš©': 'Y' if col.get('is_nullable') == 'YES' else 'N',
                            'NULL ê°œìˆ˜': col.get('null_count', 0),
                            'ì´ í–‰ ìˆ˜': total_rows
                        }
                        writer.writerow(row)

            logger.info(f"í†µí•© CSV íŒŒì¼ '{filename}' ì €ì¥ ì™„ë£Œ")
            return filename

        except Exception as e:
            logger.error(f"í†µí•© CSV íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def _format_csv_value(self, value):
        """CSV ê°’ í¬ë§·íŒ…"""
        if value is None or value == 'NULL':
            return ''
        elif value == 'N/A':
            return ''
        elif isinstance(value, (datetime, date)):
            return value.isoformat()
        elif isinstance(value, (dict, list)):
            return json.dumps(value, ensure_ascii=False)
        else:
            # CSVì—ì„œ ë¬¸ì œë  ìˆ˜ ìˆëŠ” ë¬¸ì ì²˜ë¦¬
            str_value = str(value)
            # ì¤„ë°”ê¿ˆ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ëŒ€ì²´
            str_value = str_value.replace('\n', ' ').replace('\r', ' ')
            return str_value

    def print_summary(self, results):
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("í…Œì´ë¸” ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("="*60)

        for result in results:
            if 'error' in result:
                print(f"âŒ í…Œì´ë¸”: {result['table_name']} (ì˜¤ë¥˜: {result['error']})")
                continue

            print(f"\nğŸ“‹ í…Œì´ë¸”: {result['table_name']}")
            print(f"   - ì»¬ëŸ¼ ìˆ˜: {result['total_columns']}")
            print(f"   - ì´ í–‰ ìˆ˜: {result['total_rows']:,}")

            if result.get('columns'):
                print("   - ì»¬ëŸ¼ ì •ë³´:")
                for col in result['columns'][:5]:  # ì²˜ìŒ 5ê°œ ì»¬ëŸ¼ë§Œ í‘œì‹œ
                    print(f"     â€¢ {col['column_name']} ({col['data_type']})")
                if len(result['columns']) > 5:
                    print(f"     ... ì™¸ {len(result['columns']) - 5}ê°œ ì»¬ëŸ¼")

    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.db_conn:
            self.db_conn.close()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    analyzer = None
    try:
        # í…Œì´ë¸” ë¶„ì„ê¸° ì´ˆê¸°í™”
        analyzer = TableAnalyzer()
        analyzer.initialize()

        print("PostgreSQL ì—°ê²° ì„±ê³µ!")
        print(f"Host: {analyzer.db_conn.host}:{analyzer.db_conn.port}")
        print(f"Database: {analyzer.db_conn.database}")

        # 'table_' ì ‘ë‘ì‚¬ë¥¼ ê°€ì§„ í…Œì´ë¸”ë“¤ ë¶„ì„
        print("\n'table_' ì ‘ë‘ì‚¬ë¥¼ ê°€ì§„ í…Œì´ë¸”ë“¤ì„ ë¶„ì„í•©ë‹ˆë‹¤...")
        results = analyzer.analyze_tables_with_prefix('table_')

        if results:
            # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            analyzer.print_summary(results)

            # JSON íŒŒì¼ë¡œ ì €ì¥
            json_filename = analyzer.export_to_json(results)
            print(f"\nâœ… JSON ë¶„ì„ ê²°ê³¼ê°€ '{json_filename}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

            # ê° í…Œì´ë¸”ë³„ CSV íŒŒì¼ë¡œ ì €ì¥
            csv_files = analyzer.export_to_csv(results)
            if csv_files:
                print(f"âœ… í…Œì´ë¸”ë³„ CSV íŒŒì¼ {len(csv_files)}ê°œ ìƒì„±:")
                for csv_file in csv_files:
                    print(f"   - {csv_file}")

            # í†µí•© CSV íŒŒì¼ë¡œ ì €ì¥
            combined_csv = analyzer.export_combined_csv(results)
            print(f"âœ… í†µí•© CSV íŒŒì¼: {combined_csv}")

        else:
            print("\nâš ï¸ ë¶„ì„í•  í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"\nâŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(f"ë©”ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")

    finally:
        if analyzer:
            analyzer.close()


if __name__ == "__main__":
    main()