#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
PostgreSQL ìŠ¤í™ ë°ì´í„° ë³€í™˜ íŒŒì´í”„ë¼ì¸ - ê²€ì¦ ê·œì¹™ ê¸°ë°˜ (Validation Rule Based)
================================================================================

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ê²€ì¦ ê·œì¹™ í…Œì´ë¸”(staging)ì„ ê¸°ë°˜ìœ¼ë¡œ PostgreSQL í…Œì´ë¸”ì˜ ìŠ¤í™ ë°ì´í„°ë¥¼
goal ê°’ì— ë”°ë¼ ì ì ˆí•œ íŒŒì„œë¥¼ ì„ íƒí•˜ì—¬ íŒŒì‹±í•˜ê³  ë³€í™˜í•©ë‹ˆë‹¤.

í…Œì´ë¸” êµ¬ì¡°:
-----------
1. kt_spec_validation_table_v03_20251023_staging: ê²€ì¦ ê·œì¹™ í…Œì´ë¸”
   - goal: íŒŒì‹± ëª©ì  (ì˜ˆ: 'í¬ê¸°ì‘ì—…', 'ìƒ‰ìƒì‘ì—…' ë“±)
   - is_target=trueì¸ ë ˆì½”ë“œë§Œ ì²˜ë¦¬
   - is_completed: íŒŒì‹± ì™„ë£Œ ì—¬ë¶€

2. kt_spec_validation_table_v03_20251023: ì†ŒìŠ¤ ë°ì´í„° í…Œì´ë¸”
   - ì‹¤ì œ ìŠ¤í™ ë°ì´í„° í¬í•¨

3. kt_spec_validation_table_v03_20251023_result: íŒŒì‹± ê²°ê³¼ í…Œì´ë¸”
   - target_disp_nm2: ì‚¬ìš©ì ì •ì˜ ëª…ì¹­
   - dimension_type: íŒŒì‹±ëœ íƒ€ì… (goalì— ë”°ë¼ ë‹¤ë¦„)
   - parsed_value: íŒŒì‹±ëœ ê°’

ì‚¬ìš©ë²•:
------
1. ê¸°ë³¸ ì‹¤í–‰ (goal í•„ìˆ˜):
   python transform_spec.py --goal í¬ê¸°ì‘ì—…

2. mod í…Œì´ë¸” ë°ì´í„° ìœ ì§€í•˜ë©° ì‹¤í–‰:
   python transform_spec.py --goal í¬ê¸°ì‘ì—… --no-truncate

í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ (.env íŒŒì¼ì— ì„¤ì •):
--------------------------------
PG_HOST=localhost
PG_PORT=5432
PG_DATABASE=your_database
PG_USER=your_username
PG_PASSWORD=your_password

ì£¼ìš” ê¸°ëŠ¥:
---------
1. goal íŒŒë¼ë¯¸í„°ì— ë”°ë¥¸ íŒŒì„œ ìë™ ì„ íƒ
2. staging í…Œì´ë¸”ì—ì„œ is_target=trueì´ê³  goalì´ ì¼ì¹˜í•˜ëŠ” ê²€ì¦ ê·œì¹™ ë¡œë“œ
3. ì†ŒìŠ¤ í…Œì´ë¸”ì—ì„œ ë§¤ì¹­ë˜ëŠ” ë°ì´í„° ì¡°íšŒ
4. ì„ íƒëœ íŒŒì„œë¡œ ë°ì´í„° íŒŒì‹±
5. result í…Œì´ë¸”ì— íŒŒì‹± ê²°ê³¼ ì €ì¥
6. staging í…Œì´ë¸”ì˜ is_completed ì—…ë°ì´íŠ¸

ì§€ì›í•˜ëŠ” íŒŒì„œ (goal ê°’):
--------------------
- 'í¬ê¸°ì‘ì—…': ì œí’ˆ í¬ê¸° ì •ë³´ (width, height, depth) íŒŒì‹±
- (ì¶”ê°€ ì˜ˆì •) 'ìƒ‰ìƒì‘ì—…', 'ì†Œì¬ì‘ì—…', 'ê¸°ëŠ¥ì‘ì—…' ë“±

================================================================================
"""

import os
import sys
import argparse
import pandas as pd
from sqlalchemy import create_engine, text
from dotenv import load_dotenv
from datetime import datetime
import time

# íŒŒì„œ ëª¨ë“ˆ ì„í¬íŠ¸
from parsers import get_parser, list_available_parsers

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# í…Œì´ë¸” ì´ë¦„ ì •ì˜
STAGING_TABLE = 'kt_spec_validation_table_v03_20251023_staging'
SOURCE_TABLE = 'kt_spec_validation_table_v03_20251023'
# ê°œë°œ ì¤‘ì—ëŠ” temp_result ì‚¬ìš©, í”„ë¡œë•ì…˜ì—ì„œëŠ” ì›ë˜ í…Œì´ë¸” ì‚¬ìš©
USE_TEMP_TABLE = True  # ê°œë°œ ì™„ë£Œ í›„ Falseë¡œ ë³€ê²½
MOD_TABLE = 'temp_result' if USE_TEMP_TABLE else 'kt_spec_validation_table_v03_20251023_result'

def get_sqlalchemy_engine():
    """SQLAlchemy ì—”ì§„ ìƒì„±"""
    try:
        connection_string = f"postgresql://{os.getenv('PG_USER')}:{os.getenv('PG_PASSWORD')}@{os.getenv('PG_HOST')}:{os.getenv('PG_PORT')}/{os.getenv('PG_DATABASE')}"
        engine = create_engine(connection_string)
        print(f"âœ… SQLAlchemy ì—”ì§„ ìƒì„± ì„±ê³µ")
        return engine
    except Exception as e:
        print(f"âŒ SQLAlchemy ì—”ì§„ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def load_validation_rules(engine, goal):
    """
    staging í…Œì´ë¸”ì—ì„œ is_target=trueì´ê³  goalì´ ì¼ì¹˜í•˜ëŠ” validation ê·œì¹™ ë¡œë“œ

    Parameters:
    - engine: SQLAlchemy engine
    - goal: íŒŒì‹± ëª©ì  (ì˜ˆ: 'í¬ê¸°ì‘ì—…')

    Returns:
    - DataFrame with validation rules
    """
    try:
        query = text(f"""
        SELECT disp_lv1, disp_lv2, disp_lv3, disp_nm1, disp_nm2,
               target_disp_nm2, dimension_type, is_target, is_completed, goal
        FROM {STAGING_TABLE}
        WHERE is_target = true
          AND goal = :goal
          AND (is_completed = false OR is_completed IS NULL)
        """)
        df = pd.read_sql(query, engine, params={'goal': goal})
        print(f"âœ… ê²€ì¦ ê·œì¹™ {len(df)}ê°œ ë¡œë“œ ì™„ë£Œ (is_target=true, goal='{goal}', is_completed=false)")
        return df
    except Exception as e:
        print(f"âŒ ê²€ì¦ ê·œì¹™ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def load_data_with_validation_rules(engine, validation_rules_df):
    """
    validation ê·œì¹™ì— ë§¤ì¹­ë˜ëŠ” ë°ì´í„°ë¥¼ ì†ŒìŠ¤ í…Œì´ë¸”ì—ì„œ ë¡œë“œ

    Parameters:
    - engine: SQLAlchemy engine
    - validation_rules_df: ê²€ì¦ ê·œì¹™ DataFrame

    Returns:
    - DataFrame with matched data and validation rules
    """
    try:
        all_data = []

        for _, rule in validation_rules_df.iterrows():
            # NULL ê°’ ì²˜ë¦¬
            conditions = []
            params = {}

            for idx, col in enumerate(['disp_lv1', 'disp_lv2', 'disp_lv3', 'disp_nm1', 'disp_nm2']):
                if pd.notna(rule[col]):
                    conditions.append(f"{col} = :param_{idx}")
                    params[f'param_{idx}'] = rule[col]
                else:
                    conditions.append(f"{col} IS NULL")

            where_clause = " AND ".join(conditions)
            query = text(f"SELECT * FROM {SOURCE_TABLE} WHERE {where_clause}")

            df_part = pd.read_sql(query, engine, params=params)

            # validation ê·œì¹™ ì •ë³´ ì¶”ê°€
            df_part['target_disp_nm2'] = rule['target_disp_nm2']
            df_part['validation_rule_id'] = f"{rule['disp_lv1']}|{rule['disp_lv2']}|{rule['disp_lv3']}|{rule['disp_nm1']}|{rule['disp_nm2']}"

            all_data.append(df_part)

        if all_data:
            df_combined = pd.concat(all_data, ignore_index=True)
            print(f"âœ… ê²€ì¦ ê·œì¹™ì— ë§¤ì¹­ë˜ëŠ” {len(df_combined)}ê°œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
            return df_combined
        else:
            print("âš ï¸ ë§¤ì¹­ë˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()

    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None



def truncate_table(engine, table_name):
    """
    í…Œì´ë¸”ì˜ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
    
    Parameters:
    - engine: SQLAlchemy engine
    - table_name: í…Œì´ë¸”ëª…
    """
    try:
        with engine.connect() as conn:
            conn.execute(text(f"TRUNCATE TABLE {table_name} RESTART IDENTITY CASCADE"))
            conn.commit()
        print(f"âœ… í…Œì´ë¸” '{table_name}'ì˜ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
        return True
    except Exception as e:
        print(f"âŒ ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨: {e}")
        return False


# ============================================
# ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨ í•¨ìˆ˜
# ============================================

def update_staging_table(engine, validation_rules_df, parsed_results, dimension_summaries):
    """
    staging í…Œì´ë¸”ì˜ is_completed ê°’, dimension_type, from_disp_nm2 ì—…ë°ì´íŠ¸

    Parameters:
    - engine: SQLAlchemy engine
    - validation_rules_df: ì²˜ë¦¬í•œ ê²€ì¦ ê·œì¹™
    - parsed_results: íŒŒì‹± ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ {validation_rule_id: success}
    - dimension_summaries: {validation_rule_id: ['depth', 'width', ...]}
    """
    try:
        with engine.begin() as conn:
            for _, rule in validation_rules_df.iterrows():
                rule_id = f"{rule['disp_lv1']}|{rule['disp_lv2']}|{rule['disp_lv3']}|{rule['disp_nm1']}|{rule['disp_nm2']}"

                if rule_id in parsed_results and parsed_results[rule_id]:
                    # dimension_type ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
                    dimension_list = dimension_summaries.get(rule_id, [])
                    dimension_str = str(dimension_list) if dimension_list else None

                    # from_disp_nm1, from_disp_nm2 ì •ë³´ ì¤€ë¹„ (ë””ë²„ê¹…ìš©)
                    from_disp_nm1 = str(rule['disp_nm1']) if pd.notna(rule['disp_nm1']) else None
                    from_disp_nm2 = str(rule['disp_nm2']) if pd.notna(rule['disp_nm2']) else None

                    # íŒŒì‹± ì„±ê³µí•œ ê²½ìš° is_completed, dimension_type, from_disp_nm1, from_disp_nm2 ì—…ë°ì´íŠ¸
                    conditions = []
                    params = {}

                    for idx, col in enumerate(['disp_lv1', 'disp_lv2', 'disp_lv3', 'disp_nm1', 'disp_nm2']):
                        if pd.notna(rule[col]):
                            conditions.append(f"{col} = :param_{idx}")
                            params[f'param_{idx}'] = rule[col]
                        else:
                            conditions.append(f"{col} IS NULL")

                    where_clause = " AND ".join(conditions)

                    # ëª¨ë“  ì •ë³´ í¬í•¨í•œ ì—…ë°ì´íŠ¸ (PostgreSQL ë¬¸ë²•)
                    params['from_disp_nm1'] = from_disp_nm1
                    params['from_disp_nm2'] = from_disp_nm2

                    if dimension_str:
                        params['dimension_type'] = dimension_str
                        update_query = text(f"""
                            UPDATE {STAGING_TABLE}
                            SET is_completed = true,
                                dimension_type = :dimension_type,
                                from_disp_nm1 = :from_disp_nm1,
                                from_disp_nm2 = :from_disp_nm2
                            WHERE {where_clause}
                        """)
                    else:
                        update_query = text(f"""
                            UPDATE {STAGING_TABLE}
                            SET is_completed = true,
                                from_disp_nm1 = :from_disp_nm1,
                                from_disp_nm2 = :from_disp_nm2
                            WHERE {where_clause}
                        """)

                    conn.execute(update_query, params)

        print(f"âœ… Staging í…Œì´ë¸” ì—…ë°ì´íŠ¸ ì™„ë£Œ ({len([v for v in parsed_results.values() if v])}ê°œ ê·œì¹™ ì™„ë£Œ)")
        return True

    except Exception as e:
        print(f"âŒ Staging í…Œì´ë¸” ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def save_to_mod_table(engine, df_parsed):
    """
    íŒŒì‹± ê²°ê³¼ë¥¼ mod í…Œì´ë¸”ì— ì €ì¥ (ì¤‘ë³µ ì²´í¬ í¬í•¨)

    Parameters:
    - engine: SQLAlchemy engine
    - df_parsed: íŒŒì‹±ëœ ë°ì´í„° DataFrame

    Returns:
    - tuple: (success, duplicate_count)
    """
    try:
        if len(df_parsed) == 0:
            print("âš ï¸ ì €ì¥í•  íŒŒì‹± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return True, 0

        # ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•œ í‚¤ ì»¬ëŸ¼ë“¤
        duplicate_check_cols = [
            'mdl_code', 'goods_nm', 'category_lv1', 'category_lv2',
            'disp_nm1', 'disp_nm2', 'value', 'target_disp_nm2',
            'dimension_type', 'parsed_value'
        ]

        # parsed_string_value ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš° ì¶”ê°€
        if 'parsed_string_value' in df_parsed.columns:
            duplicate_check_cols.append('parsed_string_value')

        # parsed_symbols ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš° ì¶”ê°€
        if 'parsed_symbols' in df_parsed.columns:
            duplicate_check_cols.append('parsed_symbols')

        # ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ (ì¤‘ë³µ ì²´í¬ìš©)
        try:
            # ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (parsed_string_value, parsed_symbols)
            check_columns_query = text("""
                SELECT column_name
                FROM information_schema.columns
                WHERE table_name = :table_name
                AND column_name IN ('parsed_string_value', 'parsed_symbols')
            """)
            col_result = pd.read_sql(check_columns_query, engine, params={'table_name': MOD_TABLE.lower()})

            existing_columns = col_result['column_name'].tolist()
            has_string_value = 'parsed_string_value' in existing_columns
            has_symbols = 'parsed_symbols' in existing_columns

            # ê¸°ë³¸ ì»¬ëŸ¼
            select_cols = [
                'mdl_code', 'goods_nm', 'category_lv1', 'category_lv2',
                'disp_nm1', 'disp_nm2', 'value', 'target_disp_nm2',
                'dimension_type', 'parsed_value'
            ]

            # ì„ íƒì  ì»¬ëŸ¼ ì¶”ê°€
            if has_string_value:
                select_cols.append('parsed_string_value')
            if has_symbols:
                select_cols.append('parsed_symbols')

            existing_query = f"""
                SELECT {', '.join(select_cols)}
                FROM {MOD_TABLE}
            """
            df_existing = pd.read_sql(existing_query, engine)
        except Exception as e:
            # í…Œì´ë¸”ì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆëŠ” ê²½ìš°
            df_existing = pd.DataFrame(columns=duplicate_check_cols)

        # íŒŒì‹±ëœ ë°ì´í„°ë¥¼ ì €ì¥ í˜•ì‹ìœ¼ë¡œ ì¤€ë¹„
        rows_to_insert = []
        duplicate_count = 0

        for _, row in df_parsed.iterrows():
            row_dict = row.to_dict()

            # ì¤‘ë³µ ì²´í¬ìš© ë°ì´í„° ì¤€ë¹„
            check_data = {
                'mdl_code': row_dict.get('mdl_code'),
                'goods_nm': row_dict.get('goods_nm'),
                'category_lv1': row_dict.get('category_lv1'),
                'category_lv2': row_dict.get('category_lv2'),
                'disp_nm1': row_dict.get('disp_nm1'),
                'disp_nm2': row_dict.get('disp_nm2'),
                'value': row_dict.get('value'),
                'target_disp_nm2': row_dict.get('target_disp_nm2'),
                'dimension_type': row_dict.get('dimension_type'),
                'parsed_value': row_dict.get('parsed_value')
            }

            # parsed_string_valueê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€
            if 'parsed_string_value' in row_dict:
                check_data['parsed_string_value'] = row_dict.get('parsed_string_value')

            # parsed_symbolsê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€
            if 'parsed_symbols' in row_dict:
                check_data['parsed_symbols'] = row_dict.get('parsed_symbols')

            # ì¤‘ë³µ ì²´í¬
            is_duplicate = False
            if len(df_existing) > 0:
                # ëª¨ë“  í‚¤ ì»¬ëŸ¼ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                mask = True
                for col in duplicate_check_cols:
                    # NaN ì²˜ë¦¬ë¥¼ ìœ„í•œ íŠ¹ë³„ ë¡œì§
                    col_value = check_data.get(col)
                    if pd.isna(col_value):
                        mask = mask & df_existing[col].isna()
                    else:
                        mask = mask & (df_existing[col] == col_value)

                if mask.any():
                    is_duplicate = True
                    duplicate_count += 1

            if not is_duplicate:
                # mod í…Œì´ë¸”ì— ì €ì¥í•  ì „ì²´ ë°ì´í„° ì¤€ë¹„
                insert_data = {
                    'mdl_code': row_dict.get('mdl_code'),
                    'goods_nm': row_dict.get('goods_nm'),
                    'disp_lv1': row_dict.get('disp_lv1'),
                    'disp_lv2': row_dict.get('disp_lv2'),
                    'disp_lv3': row_dict.get('disp_lv3'),
                    'category_lv1': row_dict.get('category_lv1'),
                    'category_lv2': row_dict.get('category_lv2'),
                    'category_lv3': row_dict.get('category_lv3'),
                    'disp_nm1': row_dict.get('disp_nm1'),
                    'disp_nm2': row_dict.get('disp_nm2'),
                    'value': row_dict.get('value'),
                    'is_numeric': row_dict.get('is_numeric'),
                    'symbols': row_dict.get('symbols'),
                    'new_value': row_dict.get('new_value'),
                    'target_disp_nm2': row_dict.get('target_disp_nm2'),
                    'dimension_type': row_dict.get('dimension_type'),
                    'parsed_value': row_dict.get('parsed_value'),
                    'needs_check': row_dict.get('needs_check', False),
                    'goal': row_dict.get('goal')  # goal í•„ë“œ ì¶”ê°€
                }

                # parsed_string_valueê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€
                if 'parsed_string_value' in row_dict:
                    insert_data['parsed_string_value'] = row_dict.get('parsed_string_value')

                # parsed_symbolsê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€
                if 'parsed_symbols' in row_dict:
                    insert_data['parsed_symbols'] = row_dict.get('parsed_symbols')

                rows_to_insert.append(insert_data)

                # ë©”ëª¨ë¦¬ìƒì˜ ê¸°ì¡´ ë°ì´í„°ì—ë„ ì¶”ê°€ (í›„ì† ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•´)
                df_existing = pd.concat([df_existing, pd.DataFrame([check_data])], ignore_index=True)

        # ìƒˆë¡œìš´ ë°ì´í„°ë§Œ ì €ì¥
        if len(rows_to_insert) > 0:
            df_to_save = pd.DataFrame(rows_to_insert)
            df_to_save.to_sql(MOD_TABLE, engine, if_exists='append', index=False)
            print(f"âœ… {len(rows_to_insert)}ê°œì˜ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        else:
            print("â„¹ï¸ ëª¨ë“  ë°ì´í„°ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        if duplicate_count > 0:
            print(f"âš ï¸ {duplicate_count}ê°œì˜ ì¤‘ë³µ ë°ì´í„°ëŠ” ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤.")

        # validation_rule_idë³„ë¡œ dimension_type ì§‘ê³„ (ì‚¬ìš©ì í™•ì¸ìš©)
        rule_summary = df_parsed.groupby('validation_rule_id')['dimension_type'].apply(
            lambda x: sorted([item for item in set(x) if item is not None]) + ([None] if None in set(x) else [])
        )

        print(f"ğŸ“Š ì²˜ë¦¬ëœ ê·œì¹™ë³„ dimension íƒ€ì…:")
        for rule_id, dimensions in rule_summary.items():
            print(f"   - {dimensions}")

        return True, duplicate_count

    except Exception as e:
        print(f"âŒ Mod í…Œì´ë¸” ì €ì¥ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False, 0


def parse_data_with_parser(row, parser):
    """
    ì£¼ì–´ì§„ íŒŒì„œë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ íŒŒì‹±

    Parameters:
    -----------
    row : pandas.Series
        íŒŒì‹±í•  ë°ì´í„° í–‰
    parser : BaseParser instance
        ì‚¬ìš©í•  íŒŒì„œ ì¸ìŠ¤í„´ìŠ¤

    Returns:
    --------
    tuple : (parsed_rows, success, needs_check)
    """
    if parser is None:
        return [], False, False

    return parser.parse(row)

# ============================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================

def process_spec_data_with_validation(engine, goal, truncate_before_insert=True, verbose=True):
    """
    ê²€ì¦ ê·œì¹™ ê¸°ë°˜ ìŠ¤í™ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

    Parameters:
    -----------
    engine : SQLAlchemy engine
        ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    goal : str
        íŒŒì‹± ëª©ì  (ì˜ˆ: 'í¬ê¸°ì‘ì—…')
    truncate_before_insert : bool
        Trueì´ë©´ mod í…Œì´ë¸”ì˜ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ì‚½ì…
    verbose : bool
        ìƒì„¸ ì¶œë ¥ ì—¬ë¶€

    Returns:
    --------
    bool : ì„±ê³µ ì—¬ë¶€
    """
    # ì „ì²´ ìˆ˜í–‰ ì‹œê°„ ì¸¡ì • ì‹œì‘
    start_time = time.time()

    try:
        # 0. íŒŒì„œ ê°€ì ¸ì˜¤ê¸°
        parser = get_parser(goal)
        if parser is None:
            print(f"âŒ '{goal}'ì— ëŒ€í•œ íŒŒì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì„œ ëª©ë¡: {list_available_parsers()}")
            return False

        print(f"âœ… '{goal}' íŒŒì„œ ë¡œë“œ ì™„ë£Œ")

        # 1. validation ê·œì¹™ ë¡œë“œ
        print("\n" + "="*80)
        print("ğŸ“¥ ê²€ì¦ ê·œì¹™ ë¡œë“œ ì¤‘...")
        print("="*80)
        validation_rules = load_validation_rules(engine, goal)

        if validation_rules is None or len(validation_rules) == 0:
            print(f"\nâš ï¸ ì²˜ë¦¬í•  ê²€ì¦ ê·œì¹™ì´ ì—†ìŠµë‹ˆë‹¤ (is_target=true, goal='{goal}', is_completed=false)")
            return True

        # 2. ê²€ì¦ ê·œì¹™ì— ë§¤ì¹­ë˜ëŠ” ë°ì´í„° ë¡œë“œ
        print("\n" + "="*80)
        print("ğŸ“¥ ì†ŒìŠ¤ ë°ì´í„° ë¡œë“œ ì¤‘...")
        print("="*80)
        df_filtered = load_data_with_validation_rules(engine, validation_rules)

        if df_filtered is None or len(df_filtered) == 0:
            print("\nâŒ ë§¤ì¹­ë˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return False

        # 3. ë°ì´í„° íŒŒì‹±
        print("\n" + "="*80)
        print("ğŸ”„ ë°ì´í„° íŒŒì‹± ì¤‘...")
        print("="*80)

        parsed_data = []
        parsed_results = {}  # {validation_rule_id: success}
        unparsed_data = []

        for _, row in df_filtered.iterrows():
            parsed_rows, success, needs_check = parse_data_with_parser(row, parser)
            rule_id = row['validation_rule_id']

            if success and parsed_rows:
                # validation_rule_id, target_disp_nm2, goal ì¶”ê°€
                for parsed_row in parsed_rows:
                    parsed_row['validation_rule_id'] = rule_id
                    parsed_row['target_disp_nm2'] = row['target_disp_nm2']
                    parsed_row['goal'] = goal  # í•¨ìˆ˜ íŒŒë¼ë¯¸í„°ì—ì„œ ì§ì ‘ ê°€ì ¸ì˜´

                parsed_data.extend(parsed_rows)
                parsed_results[rule_id] = True
            else:
                unparsed_data.append(row)
                if rule_id not in parsed_results:
                    parsed_results[rule_id] = False

        df_parsed = pd.DataFrame(parsed_data)
        df_unparsed = pd.DataFrame(unparsed_data)

        # íŒŒì‹± í†µê³„ ì¶œë ¥
        successful_rules = len([v for v in parsed_results.values() if v])
        total_rules = len(validation_rules)
        print(f"âœ… íŒŒì‹± ì„±ê³µ: {len(df_parsed)}ê°œ dimension ê°’")
        print(f"âœ… ì„±ê³µí•œ ê²€ì¦ ê·œì¹™: {successful_rules}/{total_rules}ê°œ")
        print(f"âŒ íŒŒì‹± ì‹¤íŒ¨: {len(df_unparsed)}ê°œ í–‰")
        print(f"ğŸ“ˆ ì „ì²´ ëŒ€ë¹„ íŒŒì‹±ë¥ : {(len(df_parsed) / len(df_filtered) * 100 if len(df_filtered) > 0 else 0):.1f}%")

        # íŒŒì‹± ì‹¤íŒ¨ ë°ì´í„° ìƒì„¸ ì¶œë ¥
        if len(df_unparsed) > 0:
            print("\n" + "="*80)
            print(f"âŒ íŒŒì‹± ì‹¤íŒ¨ ë°ì´í„° ìƒì„¸ ({len(df_unparsed)}ê°œ í–‰)")
            print("="*80)

            # í™”ë©´ ì¶œë ¥
            display_cols = ['mdl_code', 'goods_nm', 'disp_nm1', 'disp_nm2', 'value', 'validation_rule_id']
            available_cols = [col for col in display_cols if col in df_unparsed.columns]
            print(df_unparsed[available_cols].to_string(index=False))

            # CSV íŒŒì¼ë¡œ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            failed_file = f"parsing_failed_{timestamp}.csv"
            df_unparsed[available_cols].to_csv(failed_file, index=False, encoding='utf-8-sig')
            print(f"\nğŸ’¾ íŒŒì‹± ì‹¤íŒ¨ ë°ì´í„°ë¥¼ '{failed_file}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            print("="*80)

        # ìƒì„¸ ì¶œë ¥ (verbose ëª¨ë“œ)
        if verbose and len(df_parsed) > 0:
            print("\nâœ… íŒŒì‹± ì„±ê³µ ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 20ê°œ):")
            print("-" * 80)
            display_cols = ['mdl_code', 'goods_nm', 'disp_nm1', 'disp_nm2', 'target_disp_nm2', 'dimension_type', 'parsed_value', 'value']
            available_cols = [col for col in display_cols if col in df_parsed.columns]
            print(df_parsed[available_cols].head(20).to_string())

        # 4. Mod í…Œì´ë¸”ì— ì €ì¥
        print("\n" + "="*80)
        print("ğŸ’¾ Mod í…Œì´ë¸”ì— ì €ì¥ ì¤‘...")
        print("="*80)

        if truncate_before_insert:
            truncate_table(engine, MOD_TABLE)

        success_mod, duplicate_count = save_to_mod_table(engine, df_parsed)

        # 5. Staging í…Œì´ë¸” ì—…ë°ì´íŠ¸
        print("\n" + "="*80)
        print("ğŸ’¾ Staging í…Œì´ë¸” ì—…ë°ì´íŠ¸ ì¤‘...")
        print("="*80)

        # validation_rule_idë³„ë¡œ dimension_type ì§‘ê³„
        dimension_summaries = {}
        if len(df_parsed) > 0:
            dimension_summaries = df_parsed.groupby('validation_rule_id')['dimension_type'].apply(
                lambda x: sorted([item for item in set(x) if item is not None]) + ([None] if None in set(x) else [])
            ).to_dict()

        success_staging = update_staging_table(engine, validation_rules, parsed_results, dimension_summaries)

        success = success_mod and success_staging

        # ì „ì²´ ìˆ˜í–‰ ì‹œê°„ ê³„ì‚°
        end_time = time.time()
        elapsed_time = end_time - start_time
        elapsed_minutes = int(elapsed_time // 60)
        elapsed_seconds = elapsed_time % 60

        if success:
            print("\n" + "="*80)
            print("âœ… ì „ì²´ ì‘ì—… ì™„ë£Œ!")
            print("="*80)
            print(f"ğŸ“Š ìš”ì•½:")
            print(f"  - ì²˜ë¦¬ëœ ê²€ì¦ ê·œì¹™: {successful_rules}/{total_rules}ê°œ")
            print(f"  - íŒŒì‹±ëœ dimension ê°’: {len(df_parsed)}ê°œ")

            # DBì—ì„œ ì‹¤ì œ ì €ì¥ëœ ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ (mdl_code, goods_nm)ë³„ í†µê³„ ìƒì„±
            print(f"\nğŸ“ˆ ì œí’ˆë³„ DB ì €ì¥ rows í†µê³„ (mdl_code + goods_nm ì¡°í•© ê¸°ì¤€):")

            # í˜„ì¬ ì²˜ë¦¬ëœ ì œí’ˆë“¤ì˜ ì¡°í•©
            unique_products = df_parsed[['mdl_code', 'goods_nm']].drop_duplicates()

            try:
                # DBì—ì„œ ì‹¤ì œ ì €ì¥ëœ ë°ì´í„° ì¡°íšŒ - mdl_codeì™€ goods_nm ì¡°í•©ìœ¼ë¡œ
                actual_stats_query = f"""
                    SELECT mdl_code, goods_nm, COUNT(*) as row_count
                    FROM {MOD_TABLE}
                    GROUP BY mdl_code, goods_nm
                """
                df_actual_stats = pd.read_sql(actual_stats_query, engine)

                # í˜„ì¬ ì²˜ë¦¬ëœ ì œí’ˆë“¤ë§Œ í•„í„°ë§
                df_actual_stats_filtered = df_actual_stats.merge(
                    unique_products,
                    on=['mdl_code', 'goods_nm'],
                    how='inner'
                )

                if len(df_actual_stats_filtered) > 0:
                    # DB ê¸°ì¤€ í†µê³„ ê³„ì‚°
                    df_actual_stats_filtered['product_key'] = df_actual_stats_filtered['mdl_code'] + '_' + df_actual_stats_filtered['goods_nm']
                    product_row_counts = df_actual_stats_filtered.set_index('product_key')['row_count']
                    product_stats = product_row_counts.value_counts().sort_index()

                    print(f"  DBì— ì‹¤ì œ ì €ì¥ëœ ì œí’ˆë³„ í†µê³„:")
                    for row_count, product_count in product_stats.items():
                        print(f"  - {row_count}ê°œ row ì €ì¥: {product_count}ê°œ ì œí’ˆ")
                    print(f"  - ì „ì²´ ì œí’ˆ ìˆ˜: {len(df_actual_stats_filtered)}ê°œ")

                    # mdl_code_row_countsë¥¼ product ê¸°ì¤€ìœ¼ë¡œ ì¬ì„¤ì •
                    mdl_code_row_counts = product_row_counts
                else:
                    # íŒŒì‹±ëœ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ fallback
                    df_parsed['product_key'] = df_parsed['mdl_code'] + '_' + df_parsed['goods_nm']
                    product_row_counts = df_parsed.groupby('product_key').size()
                    product_stats = product_row_counts.value_counts().sort_index()

                    print(f"  íŒŒì‹±ëœ ë°ì´í„° ê¸°ì¤€ (DB ì¡°íšŒ ì‹¤íŒ¨):")
                    for row_count, product_count in product_stats.items():
                        print(f"  - {row_count}ê°œ row ìƒì„±: {product_count}ê°œ ì œí’ˆ")
                    print(f"  - ì „ì²´ ì œí’ˆ ìˆ˜: {df_parsed[['mdl_code', 'goods_nm']].drop_duplicates().shape[0]}ê°œ")

                    mdl_code_row_counts = product_row_counts

            except Exception as e:
                print(f"  âš ï¸ DB í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                # íŒŒì‹±ëœ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ fallback
                df_parsed['product_key'] = df_parsed['mdl_code'] + '_' + df_parsed['goods_nm']
                product_row_counts = df_parsed.groupby('product_key').size()
                product_stats = product_row_counts.value_counts().sort_index()

                print(f"  íŒŒì‹±ëœ ë°ì´í„° ê¸°ì¤€:")
                for row_count, product_count in product_stats.items():
                    print(f"  - {row_count}ê°œ row ìƒì„±: {product_count}ê°œ ì œí’ˆ")
                print(f"  - ì „ì²´ ì œí’ˆ ìˆ˜: {df_parsed[['mdl_code', 'goods_nm']].drop_duplicates().shape[0]}ê°œ")

                mdl_code_row_counts = product_row_counts

            # 3ê°œê°€ ì•„ë‹Œ ì œí’ˆë“¤ ì¶œë ¥ (DB ê¸°ì¤€ ë˜ëŠ” íŒŒì‹± ë°ì´í„° ê¸°ì¤€)
            if len(df_parsed) > 0:
                # mdl_code_row_countsê°€ ìœ„ì—ì„œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ì´ì œ product_key ê¸°ì¤€)
                if 'mdl_code_row_counts' not in locals():
                    df_parsed['product_key'] = df_parsed['mdl_code'] + '_' + df_parsed['goods_nm']
                    mdl_code_row_counts = df_parsed.groupby('product_key').size()

                # íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ë¯¸ë¦¬ ìƒì„±
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

                non_three_products = mdl_code_row_counts[mdl_code_row_counts != 3]
                if len(non_three_products) > 0:
                    print(f"\nâš ï¸ 3ê°œê°€ ì•„ë‹Œ rowë¥¼ ê°€ì§„ ì œí’ˆ ëª©ë¡ ({len(non_three_products)}ê°œ):")

                    # row ìˆ˜ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì¶œë ¥
                    for row_count in sorted(non_three_products.unique()):
                        products_with_count = non_three_products[non_three_products == row_count].index.tolist()
                        print(f"\n  [{row_count}ê°œ row ì €ì¥] - {len(products_with_count)}ê°œ ì œí’ˆ:")

                        # ì œí’ˆë³„ ë°ì´í„° ì •ë³´ ì¶œë ¥
                        for product_key in products_with_count[:10]:  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
                            # product_keyì—ì„œ mdl_code ì¶”ì¶œ
                            if 'product_key' not in df_parsed.columns:
                                df_parsed['product_key'] = df_parsed['mdl_code'] + '_' + df_parsed['goods_nm']

                            # í•´ë‹¹ ì œí’ˆì˜ ë°ì´í„° ìƒ˜í”Œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                            product_data = df_parsed[df_parsed['product_key'] == product_key]
                            if len(product_data) > 0:
                                sample_data = product_data.iloc[0]
                                mdl_code = sample_data.get('mdl_code', 'N/A')
                                goods_nm = sample_data.get('goods_nm', 'N/A')
                                disp_nm2 = sample_data.get('disp_nm2', 'N/A')
                                value = sample_data.get('value', 'N/A')

                                # dimension_types ë¦¬ìŠ¤íŠ¸
                                dimension_types = product_data['dimension_type'].tolist()

                                print(f"    â€¢ {mdl_code}: {goods_nm[:30]}... | {disp_nm2[:20]}...")
                                print(f"      ê°’: {value[:50]}..." if len(str(value)) > 50 else f"      ê°’: {value}")
                                print(f"      íŒŒì‹±ëœ íƒ€ì…: {dimension_types}")

                        if len(products_with_count) > 10:
                            print(f"    ... ì™¸ {len(products_with_count) - 10}ê°œ ë” ìˆìŒ")

                    # 3ê°œê°€ ì•„ë‹Œ rowë¥¼ ê°€ì§„ ì œí’ˆ ëª©ë¡ì„ íŒŒì¼ë¡œ ì €ì¥
                    non_standard_file = f"non_standard_products_{timestamp}.csv"

                    # ë°ì´í„° ì¤€ë¹„ - DB ì‹¤ì œ ë°ì´í„°ì™€ íŒŒì‹± ë°ì´í„° ë³‘í•©
                    non_standard_data = []

                    if 'product_key' not in df_parsed.columns:
                        df_parsed['product_key'] = df_parsed['mdl_code'] + '_' + df_parsed['goods_nm']

                    for product_key in non_three_products.index:
                        # íŒŒì‹± ë°ì´í„°ì—ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                        product_data = df_parsed[df_parsed['product_key'] == product_key]
                        # DB ê¸°ì¤€ row count ì‚¬ìš©
                        row_count = non_three_products[product_key]

                        if len(product_data) > 0:
                            sample_row = product_data.iloc[0]
                            # None ê°’ì„ í•„í„°ë§í•˜ê³  ë¬¸ìì—´ë¡œ ë³€í™˜
                            dimension_types = sorted([str(dt) for dt in product_data['dimension_type'].tolist() if dt is not None])

                            non_standard_data.append({
                                'mdl_code': sample_row.get('mdl_code', ''),
                                'goods_nm': sample_row.get('goods_nm', ''),
                                'disp_nm1': sample_row.get('disp_nm1', ''),
                                'disp_nm2': sample_row.get('disp_nm2', ''),
                                'value': sample_row.get('value', ''),
                                'target_disp_nm2': sample_row.get('target_disp_nm2', ''),
                                'row_count': row_count,
                                'dimension_types': ', '.join(dimension_types) if dimension_types else 'none',
                                'category_lv1': sample_row.get('category_lv1', ''),
                                'category_lv2': sample_row.get('category_lv2', '')
                            })

                    # CSVë¡œ ì €ì¥
                    if len(non_standard_data) > 0:
                        df_non_standard = pd.DataFrame(non_standard_data)
                        df_non_standard = df_non_standard.sort_values(['row_count', 'mdl_code', 'goods_nm'])
                        df_non_standard.to_csv(non_standard_file, index=False, encoding='utf-8-sig')

                        print(f"\nğŸ’¾ 3ê°œê°€ ì•„ë‹Œ rowë¥¼ ê°€ì§„ ì œí’ˆ ëª©ë¡ì„ '{non_standard_file}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
                        print(f"   ì´ {len(non_three_products)}ê°œ ì œí’ˆ, íŒŒì¼ì—ëŠ” ìƒì„¸ ì •ë³´ í¬í•¨")

                # ëª¨ë“  ì œí’ˆë³„ í†µê³„ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (3ê°œ row í¬í•¨)
                all_product_stats_file = f"all_product_stats_{timestamp}.csv"
                all_product_data = []

                # product_key ì»¬ëŸ¼ í™•ì¸ ë° ìƒì„±
                if 'product_key' not in df_parsed.columns:
                    df_parsed['product_key'] = df_parsed['mdl_code'] + '_' + df_parsed['goods_nm']

                for product_key in mdl_code_row_counts.index:
                    product_data = df_parsed[df_parsed['product_key'] == product_key]
                    row_count = mdl_code_row_counts[product_key]

                    if len(product_data) > 0:
                        sample_row = product_data.iloc[0]
                        # None ê°’ì„ í•„í„°ë§í•˜ê³  ë¬¸ìì—´ë¡œ ë³€í™˜
                        dimension_types = sorted([str(dt) for dt in product_data['dimension_type'].tolist() if dt is not None])

                        all_product_data.append({
                            'mdl_code': sample_row.get('mdl_code', ''),
                            'goods_nm': sample_row.get('goods_nm', ''),
                            'category_lv1': sample_row.get('category_lv1', ''),
                            'category_lv2': sample_row.get('category_lv2', ''),
                            'disp_nm1': sample_row.get('disp_nm1', ''),
                            'disp_nm2': sample_row.get('disp_nm2', ''),
                            'value': sample_row.get('value', ''),
                            'target_disp_nm2': sample_row.get('target_disp_nm2', ''),
                            'row_count': row_count,
                            'is_standard': 'O' if row_count == 3 else 'X',
                            'dimension_types': ', '.join(dimension_types) if dimension_types else 'none'
                        })

                # DataFrame ìƒì„± ë° ì €ì¥
                if len(all_product_data) > 0:
                    df_all_stats = pd.DataFrame(all_product_data)
                    df_all_stats = df_all_stats.sort_values(['is_standard', 'row_count', 'mdl_code', 'goods_nm'])
                    df_all_stats.to_csv(all_product_stats_file, index=False, encoding='utf-8-sig')

                    print(f"ğŸ’¾ ì „ì²´ ì œí’ˆë³„ í†µê³„ë¥¼ '{all_product_stats_file}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
                    print(f"   ì´ {len(mdl_code_row_counts)}ê°œ ì œí’ˆ(mdl_code + goods_nm)ì˜ ìƒì„¸ ì •ë³´ í¬í•¨")

            print(f"\n  - Staging í…Œì´ë¸” ì—…ë°ì´íŠ¸: ì™„ë£Œ")
            print(f"  - Mod í…Œì´ë¸” ì €ì¥: ì™„ë£Œ")
            if duplicate_count > 0:
                print(f"  - ì¤‘ë³µìœ¼ë¡œ ê±´ë„ˆë›´ ë°ì´í„°: {duplicate_count}ê°œ")
            print(f"  - ì „ì²´ ìˆ˜í–‰ ì‹œê°„: {elapsed_minutes}ë¶„ {elapsed_seconds:.2f}ì´ˆ")
            return True
        else:
            print("\nâŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨")
            print(f"â±ï¸  ì „ì²´ ìˆ˜í–‰ ì‹œê°„: {elapsed_minutes}ë¶„ {elapsed_seconds:.2f}ì´ˆ")
            return False

    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ìˆ˜í–‰ ì‹œê°„ ì¶œë ¥
        end_time = time.time()
        elapsed_time = end_time - start_time
        elapsed_minutes = int(elapsed_time // 60)
        elapsed_seconds = elapsed_time % 60

        print(f"\nâŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"â±ï¸  ìˆ˜í–‰ ì‹œê°„ (ì˜¤ë¥˜ ë°œìƒ ì‹œì ê¹Œì§€): {elapsed_minutes}ë¶„ {elapsed_seconds:.2f}ì´ˆ")
        import traceback
        traceback.print_exc()
        return False



def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='PostgreSQL ìŠ¤í™ ë°ì´í„° ë³€í™˜ íŒŒì´í”„ë¼ì¸ (ê²€ì¦ ê·œì¹™ ê¸°ë°˜)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f'''
ì˜ˆì œ:
  python transform_spec.py --goal í¬ê¸°ì‘ì—…                    # í¬ê¸° íŒŒì‹± ì‹¤í–‰ (ê¸°ì¡´ ë°ì´í„° ìœ ì§€)
  python transform_spec.py --goal í¬ê¸°ì‘ì—… --truncate         # mod í…Œì´ë¸” ë°ì´í„° ì‚­ì œ í›„ ì‹¤í–‰
  python transform_spec.py --goal í¬ê¸°ì‘ì—… --quiet            # ê°„ëµí•œ ì¶œë ¥
  python transform_spec.py --list-parsers                     # ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì„œ ëª©ë¡ ë³´ê¸°

ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì„œ (goal ê°’):
  {', '.join(list_available_parsers())}
        '''
    )

    parser.add_argument('--goal', type=str, help='íŒŒì‹± ëª©ì  (í•„ìˆ˜)')
    parser.add_argument('--truncate', action='store_true', help='mod í…Œì´ë¸” ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ê¸°ë³¸ê°’: ìœ ì§€)')
    parser.add_argument('--quiet', '-q', action='store_true', help='ê°„ëµí•œ ì¶œë ¥ë§Œ í‘œì‹œ')
    parser.add_argument('--list-parsers', action='store_true', help='ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì„œ ëª©ë¡ í‘œì‹œ')

    args = parser.parse_args()

    # íŒŒì„œ ëª©ë¡ í‘œì‹œ ìš”ì²­ ì²˜ë¦¬
    if args.list_parsers:
        print("\nì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì„œ ëª©ë¡:")
        print("=" * 40)
        for parser_goal in list_available_parsers():
            print(f"  - {parser_goal}")
        print("=" * 40)
        return

    # goal íŒŒë¼ë¯¸í„° í•„ìˆ˜ ì²´í¬
    if not args.goal:
        print("âŒ ì˜¤ë¥˜: --goal íŒŒë¼ë¯¸í„°ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")
        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ê°’: {', '.join(list_available_parsers())}")
        print("\nì‚¬ìš© ì˜ˆì‹œ:")
        print("  python transform_spec.py --goal í¬ê¸°ì‘ì—…")
        sys.exit(1)

    truncate_before_insert = args.truncate  # ê¸°ë³¸ê°’ì€ False (ë°ì´í„° ìœ ì§€)
    verbose = not args.quiet
    goal = args.goal

    print("\nğŸš€ PostgreSQL ìŠ¤í™ ë°ì´í„° ë³€í™˜ íŒŒì´í”„ë¼ì¸ (ê²€ì¦ ê·œì¹™ ê¸°ë°˜)")
    print("="*80)

    # ì„¤ì • í™•ì¸
    print("\n" + "="*80)
    print("ì‹¤í–‰ ì„¤ì • í™•ì¸")
    print("="*80)
    print(f"íŒŒì‹± ëª©ì  (goal): {goal}")
    print(f"Staging í…Œì´ë¸”: {STAGING_TABLE}")
    print(f"ì†ŒìŠ¤ í…Œì´ë¸”: {SOURCE_TABLE}")
    print(f"Result í…Œì´ë¸”: {MOD_TABLE}")
    print(f"ê¸°ì¡´ ë°ì´í„° ì‚­ì œ: {'ì˜ˆ' if truncate_before_insert else 'ì•„ë‹ˆì˜¤'}")
    print(f"ìƒì„¸ ì¶œë ¥: {'ì˜ˆ' if verbose else 'ì•„ë‹ˆì˜¤'}")

    confirm = input("\nê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
    if confirm != 'y':
        print("ì‘ì—…ì„ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
        return

    # ì—”ì§„ ìƒì„±
    engine = get_sqlalchemy_engine()
    if engine is None:
        print("âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
        sys.exit(1)

    try:
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        success = process_spec_data_with_validation(
            engine=engine,
            goal=goal,
            truncate_before_insert=truncate_before_insert,
            verbose=verbose
        )

        # ì¢…ë£Œ ì½”ë“œ ë°˜í™˜
        sys.exit(0 if success else 1)
    finally:
        engine.dispose()

if __name__ == "__main__":
    main()