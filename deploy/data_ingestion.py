# 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import os
import psycopg2
import pandas as pd
import numpy as np
from datetime import datetime
import json
from dotenv import load_dotenv
from openai import AzureOpenAI
from typing import Dict, List, Any
import warnings
warnings.filterwarnings('ignore')

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# 2. PostgreSQL ì—°ê²° ì„¤ì •
def get_db_connection():
    """PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
    try:
        conn = psycopg2.connect(
            host=os.getenv('PG_HOST'),
            port=os.getenv('PG_PORT'),
            database=os.getenv('PG_DATABASE'),
            user=os.getenv('PG_USER'),
            password=os.getenv('PG_PASSWORD')
        )
        print(f"âœ… PostgreSQL ì—°ê²° ì„±ê³µ: {os.getenv('PG_HOST')}")
        return conn
    except Exception as e:
        print(f"âŒ PostgreSQL ì—°ê²° ì‹¤íŒ¨: {e}")
        return None

# 3. Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
def get_openai_client():
    """Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    try:
        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        endpoint = os.getenv('ENDPOINT_URL')
        api_key = os.getenv('AZURE_OPENAI_API_KEY')
        api_version = os.getenv('AZURE_API_VERSION', '2024-02-01')  # ê¸°ë³¸ê°’ ì œê³µ
        
        if not endpoint:
            print("âŒ ENDPOINT_URL í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        if not api_key:
            print("âŒ AZURE_OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        # API ë²„ì „ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        if not api_version:
            api_version = '2024-02-01'
            print(f"âš ï¸ AZURE_API_VERSIONì´ ì„¤ì •ë˜ì§€ ì•Šì•„ ê¸°ë³¸ê°’ ì‚¬ìš©: {api_version}")
        
        print(f"ğŸ“‹ Azure OpenAI ì„¤ì •:")
        print(f"  - Endpoint: {endpoint[:50]}...")
        print(f"  - API Version: {api_version}")
        print(f"  - Deployment: {os.getenv('DEPLOYMENT_NAME')}")
        
        # Initialize without proxies parameter for compatibility
        client = AzureOpenAI(
            azure_endpoint=endpoint,
            api_key=api_key,
            api_version=api_version
        )
        print(f"âœ… Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì„±ê³µ")
        return client
    except Exception as e:
        print(f"âŒ Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

# 4. í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒ
def get_table_schema(table_name='test'):
    """í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒ (ì½”ë©˜íŠ¸ í¬í•¨)"""
    conn = get_db_connection()
    if not conn:
        return None
    
    try:
        # PostgreSQLì—ì„œ ì»¬ëŸ¼ ì •ë³´ì™€ ì½”ë©˜íŠ¸ë¥¼ í•¨ê»˜ ì¡°íšŒ
        query = """
        SELECT 
            c.column_name,
            c.data_type,
            c.character_maximum_length,
            c.numeric_precision,
            c.numeric_scale,
            c.is_nullable,
            c.column_default,
            pgd.description as column_comment
        FROM information_schema.columns c
        LEFT JOIN pg_catalog.pg_statio_all_tables as st
            ON c.table_schema = st.schemaname 
            AND c.table_name = st.relname
        LEFT JOIN pg_catalog.pg_description pgd 
            ON pgd.objoid = st.relid 
            AND pgd.objsubid = c.ordinal_position
        WHERE c.table_schema = 'public' 
        AND c.table_name = %s
        ORDER BY c.ordinal_position;
        """
        
        df_schema = pd.read_sql_query(query, conn, params=(table_name,))
        print(f"âœ… í…Œì´ë¸” '{table_name}' ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ì„±ê³µ")
        print(f"   - ì»¬ëŸ¼ ìˆ˜: {len(df_schema)}")
        
        # ì½”ë©˜íŠ¸ê°€ ìˆëŠ” ì»¬ëŸ¼ ìˆ˜ í™•ì¸
        comment_count = df_schema['column_comment'].notna().sum()
        print(f"   - ì½”ë©˜íŠ¸ê°€ ìˆëŠ” ì»¬ëŸ¼: {comment_count}ê°œ")
        
        return df_schema
    
    except Exception as e:
        print(f"âŒ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return None
    
    finally:
        conn.close()

# ìŠ¤í‚¤ë§ˆ ì¡°íšŒ
df_schema = get_table_schema(table_name="kt_merged_product_20251001")
if df_schema is not None:
    print("\ní…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´:")
    display(df_schema.head(10))

# 6. ì»¬ëŸ¼ë³„ ìƒì„¸ í†µê³„ ë¶„ì„
import json

def get_column_statistics(table_name='test', sample_size=10000):
    """ì»¬ëŸ¼ë³„ ìƒì„¸ í†µê³„ ì •ë³´ ìˆ˜ì§‘"""
    conn = get_db_connection()
    if not conn:
        return None

    try:
        # ë¨¼ì € í…Œì´ë¸”ì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        cursor = conn.cursor()
        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        total_rows = cursor.fetchone()[0]

        if total_rows == 0:
            print(f"âš ï¸ í…Œì´ë¸” '{table_name}'ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        # ìƒ˜í”Œ í¬ê¸° ì¡°ì • (ì „ì²´ í–‰ ìˆ˜ë³´ë‹¤ í¬ë©´ ì „ì²´ í–‰ ìˆ˜ë¡œ ì¡°ì •)
        actual_sample_size = min(sample_size, total_rows)

        # ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ
        query = f"SELECT * FROM {table_name} LIMIT {actual_sample_size}"
        df = pd.read_sql_query(query, conn)

        column_stats = []

        for col in df.columns:
            stats = {
                'column_name': col,
                'data_type': str(df[col].dtype),
                'non_null_count': int(df[col].notna().sum()),
                'null_count': int(df[col].isna().sum()),
                'null_ratio': f"{df[col].isna().mean() * 100:.2f}%",
                # ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë“  í†µê³„ ê°’ì„ Noneìœ¼ë¡œ ì´ˆê¸°í™”
                'min': None,
                'max': None,
                'mean': None,
                'median': None,
                'std': None,
                'values': None  # ìƒˆë¡œ ì¶”ê°€ëœ ì»¬ëŸ¼
            }
            
            # ìŠ¤í‚¤ë§ˆ ì •ë³´ì—ì„œ ì½”ë©˜íŠ¸ ì¶”ê°€
            if df_schema is not None and 'column_comment' in df_schema.columns:
                schema_row = df_schema[df_schema['column_name'] == col]
                if not schema_row.empty:
                    stats['column_comment'] = schema_row.iloc[0]['column_comment']
                else:
                    stats['column_comment'] = None
            else:
                stats['column_comment'] = None

            # unique_count ê³„ì‚° ì‹œ ì—ëŸ¬ ì²˜ë¦¬
            try:
                # JSON/JSONB ì»¬ëŸ¼ì´ë‚˜ ë³µì¡í•œ ê°ì²´ê°€ í¬í•¨ëœ ê²½ìš°ë¥¼ ì²˜ë¦¬
                if df[col].dtype == 'object':
                    # ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ê³ ìœ ê°’ ê³„ì‚°
                    unique_count = df[col].astype(str).nunique()
                else:
                    unique_count = df[col].nunique()

                stats['unique_count'] = int(unique_count)
                stats['unique_ratio'] = f"{unique_count / len(df) * 100:.2f}%"
            except Exception as e:
                # ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ì„¤ì •
                stats['unique_count'] = None
                stats['unique_ratio'] = None
                print(f"  âš ï¸ ì»¬ëŸ¼ '{col}' unique_count ê³„ì‚° ì‹¤íŒ¨: {e}")

            # product_specification ì»¬ëŸ¼ íŠ¹ë³„ ì²˜ë¦¬
            if col == 'product_specification':
                try:
                    # JSON key ì¶”ì¶œ
                    all_keys = set()
                    non_null_values = df[col].dropna()
                    
                    for value in non_null_values:
                        try:
                            # JSON ë¬¸ìì—´ì„ íŒŒì‹±
                            if isinstance(value, str):
                                json_data = json.loads(value)
                            else:
                                json_data = value
                            
                            # key ì¶”ì¶œ
                            if isinstance(json_data, dict):
                                all_keys.update(json_data.keys())
                            elif isinstance(json_data, list):
                                for item in json_data:
                                    if isinstance(item, dict):
                                        all_keys.update(item.keys())
                        except:
                            continue
                    
                    # key ë¦¬ìŠ¤íŠ¸ë¥¼ valuesì— ì €ì¥
                    stats['values'] = sorted(list(all_keys))
                    
                    # ê¸°ë³¸ í†µê³„ë„ ì¶”ê°€
                    if len(non_null_values) > 0:
                        str_values = non_null_values.astype(str)
                        str_lengths = str_values.str.len()
                        
                        stats.update({
                            'min_length': int(str_lengths.min()) if len(str_lengths) > 0 else None,
                            'max_length': int(str_lengths.max()) if len(str_lengths) > 0 else None,
                            'avg_length': float(str_lengths.mean()) if len(str_lengths) > 0 else None
                        })
                        
                        # most_commonì€ key ê°œìˆ˜ë¡œ ê³„ì‚°
                        stats['most_common'] = {"total_unique_keys": len(all_keys)}
                    else:
                        stats.update({
                            'min_length': None,
                            'max_length': None,
                            'avg_length': None,
                            'most_common': {}
                        })
                        
                except Exception as e:
                    print(f"  âš ï¸ ì»¬ëŸ¼ '{col}' JSON ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    stats['values'] = None
                    stats.update({
                        'min_length': None,
                        'max_length': None,
                        'avg_length': None,
                        'most_common': {}
                    })
                    
            # ìˆ˜ì¹˜í˜• ë°ì´í„° í†µê³„ (ìˆ«ìí˜• ì»¬ëŸ¼ì—ë§Œ ì ìš©)
            elif pd.api.types.is_numeric_dtype(df[col]):
                # nullì´ ì•„ë‹Œ ê°’ë§Œ ì¶”ì¶œ
                non_null_values = df[col].dropna()
                if len(non_null_values) > 0:
                    try:
                        stats['min'] = float(non_null_values.min())
                    except:
                        stats['min'] = None

                    try:
                        stats['max'] = float(non_null_values.max())
                    except:
                        stats['max'] = None

                    try:
                        stats['mean'] = float(non_null_values.mean())
                    except:
                        stats['mean'] = None

                    try:
                        stats['median'] = float(non_null_values.median())
                    except:
                        stats['median'] = None

                    try:
                        # stdëŠ” ìƒ˜í”Œì´ 2ê°œ ì´ìƒì¼ ë•Œë§Œ ê³„ì‚° ê°€ëŠ¥
                        if len(non_null_values) > 1:
                            stats['std'] = float(non_null_values.std())
                        else:
                            stats['std'] = None
                    except:
                        stats['std'] = None
                    
                    # ìˆ˜ì¹˜í˜• ë°ì´í„°: ëª¨ë“  distinct ê°’ ì¶”ì¶œ (ê³ ìœ ê°’ì´ ë§ìœ¼ë©´ ìƒìœ„ 15ê°œ)
                    try:
                        unique_values = non_null_values.unique()
                        if len(unique_values) <= 1000:  # distinct ê°’ì´ 1000ê°œ ì´í•˜ë©´ ëª¨ë‘ í¬í•¨
                            stats['values'] = sorted(unique_values.tolist())
                        else:  # 1000ê°œ ì´ˆê³¼ë©´ ìƒìœ„ 100ê°œë§Œ
                            top_100_values = non_null_values.nlargest(100).tolist()
                            stats['values'] = top_100_values
                    except:
                        stats['values'] = None
                else:
                    # non_null_valuesê°€ ì—†ìœ¼ë©´ ëª¨ë“  í†µê³„ ê°’ì€ ì´ë¯¸ Noneìœ¼ë¡œ ì„¤ì •ë¨
                    pass

            # ë¬¸ìì—´ ë° ê°ì²´ ë°ì´í„° í†µê³„ (nominal ì»¬ëŸ¼)
            elif df[col].dtype == 'object':
                non_null_values = df[col].dropna()
                if len(non_null_values) > 0:
                    try:
                        # ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ê¸¸ì´ ê³„ì‚°
                        str_values = non_null_values.astype(str)
                        str_lengths = str_values.str.len()

                        stats.update({
                            'min_length': int(str_lengths.min()) if len(str_lengths) > 0 else None,
                            'max_length': int(str_lengths.max()) if len(str_lengths) > 0 else None,
                            'avg_length': float(str_lengths.mean()) if len(str_lengths) > 0 else None
                        })

                        # most_common ê³„ì‚° ì‹œ ì—ëŸ¬ ì²˜ë¦¬ (3ê°œì—ì„œ 100ê°œë¡œ ì¦ê°€)
                        try:
                            # ë³µì¡í•œ ê°ì²´ëŠ” ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì¹´ìš´íŠ¸
                            value_counts = df[col].astype(str).value_counts().head(100)
                            stats['most_common'] = value_counts.to_dict()
                        except:
                            stats['most_common'] = {}
                        
                        # nominal ë°ì´í„°: ëª¨ë“  distinct ê°’ ì¶”ì¶œ
                        try:
                            unique_values = df[col].unique()
                            # NULL ê°’ ì œì™¸
                            unique_values = [v for v in unique_values if pd.notna(v)]
                            
                            # ê³ ìœ ê°’ì´ ë„ˆë¬´ ë§ì§€ ì•Šìœ¼ë©´ ëª¨ë‘ í¬í•¨
                            if len(unique_values) <= 3000:  # distinct ê°’ì´ 3000ê°œ ì´í•˜ë©´ ëª¨ë‘ í¬í•¨
                                stats['values'] = sorted(unique_values, key=str)
                            else:  # 3000ê°œ ì´ˆê³¼ë©´ ê°€ì¥ ë¹ˆë²ˆí•œ 300ê°œë§Œ
                                top_values = df[col].value_counts().head(300).index.tolist()
                                stats['values'] = top_values
                        except:
                            stats['values'] = None

                    except Exception as e:
                        print(f"  âš ï¸ ì»¬ëŸ¼ '{col}' ë¬¸ìì—´ í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                        stats.update({
                            'min_length': None,
                            'max_length': None,
                            'avg_length': None,
                            'most_common': {}
                        })
                else:
                    stats.update({
                        'min_length': None,
                        'max_length': None,
                        'avg_length': None,
                        'most_common': {}
                    })

            # ë‚ ì§œí˜• ë°ì´í„° í†µê³„
            elif pd.api.types.is_datetime64_any_dtype(df[col]):
                non_null_values = df[col].dropna()
                if len(non_null_values) > 0:
                    stats.update({
                        'min_date': str(non_null_values.min()),
                        'max_date': str(non_null_values.max()),
                        'date_range': str(non_null_values.max() - non_null_values.min())
                    })
                    
                    # ë‚ ì§œí˜• ë°ì´í„°: ëª¨ë“  distinct ë‚ ì§œ ì¶”ì¶œ (ê³ ìœ ê°’ì´ ë§ìœ¼ë©´ ìµœê·¼ 100ê°œ)
                    try:
                        unique_dates = non_null_values.unique()
                        if len(unique_dates) <= 300:  # distinct ë‚ ì§œê°€ 300ê°œ ì´í•˜ë©´ ëª¨ë‘ í¬í•¨
                            stats['values'] = pd.Series(unique_dates).sort_values().dt.strftime('%Y-%m-%d').tolist()
                        else:  # 100ê°œ ì´ˆê³¼ë©´ ìµœê·¼ 100ê°œë§Œ
                            recent_100_dates = non_null_values.nlargest(100).dt.strftime('%Y-%m-%d').tolist()
                            stats['values'] = recent_100_dates
                    except:
                        stats['values'] = None
                else:
                    stats.update({
                        'min_date': None,
                        'max_date': None,
                        'date_range': None
                    })

            column_stats.append(stats)

        print(f"âœ… ì»¬ëŸ¼ë³„ í†µê³„ ë¶„ì„ ì™„ë£Œ (ìƒ˜í”Œ í¬ê¸°: {len(df)}í–‰)")
        return pd.DataFrame(column_stats)

    except Exception as e:
        print(f"âŒ ì»¬ëŸ¼ë³„ í†µê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None

    finally:
        if 'cursor' in locals():
            cursor.close()
        conn.close()

# ì»¬ëŸ¼ í†µê³„ ìˆ˜ì§‘
df_column_stats = get_column_statistics(table_name="kt_merged_product_20251001")
if df_column_stats is not None:
    display(df_column_stats)


# 7. Azure OpenAIë¥¼ í™œìš©í•œ ì»¬ëŸ¼ ì„¤ëª… ìƒì„±
def generate_column_description(column_info, table_context='test'):
    """Azure OpenAIë¥¼ ì‚¬ìš©í•˜ì—¬ ì»¬ëŸ¼ ì„¤ëª… ìƒì„±"""
    
    if not openai_client:
        print("âš ï¸ OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    
    # ë°ì´í„° íƒ€ì… í™•ì¸
    data_type = column_info.get('data_type', '')
    is_numeric = any(dtype in data_type.lower() for dtype in ['int', 'float', 'numeric', 'decimal', 'double'])
    is_string = 'object' in data_type.lower() or 'varchar' in data_type.lower() or 'text' in data_type.lower()
    is_date = 'date' in data_type.lower() or 'time' in data_type.lower()
    
    # ê¸°ë³¸ ì •ë³´
    base_info = f"""
    í…Œì´ë¸”ëª…: {table_context}
    ì»¬ëŸ¼ ì •ë³´:
    - ì»¬ëŸ¼ëª…: {column_info.get('column_name')}
    - ë°ì´í„° íƒ€ì…: {column_info.get('data_type')}
    - NULL ë¹„ìœ¨: {column_info.get('null_ratio', 'N/A')}
    - ê³ ìœ ê°’ ê°œìˆ˜: {column_info.get('unique_count', 'N/A')}"""
    
    # ê¸°ì¡´ ì½”ë©˜íŠ¸ê°€ ìˆìœ¼ë©´ ì¶”ê°€
    column_comment = column_info.get('column_comment')
    if column_comment and column_comment != 'None' and pd.notna(column_comment):
        base_info += f"\n    - ê¸°ì¡´ ì„¤ëª…: {column_comment}"
    
    # ë°ì´í„° íƒ€ì…ë³„ ì¶”ê°€ ì •ë³´
    type_specific_info = ""
    
    if is_numeric:
        # ìˆ«ìí˜• ë°ì´í„°ì¸ ê²½ìš°
        min_val = column_info.get('min')
        max_val = column_info.get('max')
        mean_val = column_info.get('mean')
        median_val = column_info.get('median')
        std_val = column_info.get('std')
        
        if min_val is not None or max_val is not None or mean_val is not None:
            type_specific_info += "\n    - í†µê³„ ì •ë³´:"
            if min_val is not None:
                type_specific_info += f"\n      - ìµœì†Œê°’: {min_val}"
            if max_val is not None:
                type_specific_info += f"\n      - ìµœëŒ€ê°’: {max_val}"
            if mean_val is not None:
                type_specific_info += f"\n      - í‰ê· : {mean_val:.2f}"
            if median_val is not None:
                type_specific_info += f"\n      - ì¤‘ì•™ê°’: {median_val:.2f}"
            if std_val is not None:
                type_specific_info += f"\n      - í‘œì¤€í¸ì°¨: {std_val:.2f}"
    
    elif is_string:
        # ë¬¸ìì—´ ë°ì´í„°ì¸ ê²½ìš°
        min_length = column_info.get('min_length')
        max_length = column_info.get('max_length')
        avg_length = column_info.get('avg_length')
        most_common = column_info.get('most_common', {})
        
        if min_length is not None or max_length is not None or avg_length is not None:
            type_specific_info += "\n    - ë¬¸ìì—´ ê¸¸ì´ ì •ë³´:"
            if min_length is not None:
                type_specific_info += f"\n      - ìµœì†Œ ê¸¸ì´: {min_length}"
            if max_length is not None:
                type_specific_info += f"\n      - ìµœëŒ€ ê¸¸ì´: {max_length}"
            if avg_length is not None:
                type_specific_info += f"\n      - í‰ê·  ê¸¸ì´: {avg_length:.1f}"
        
        if most_common and len(most_common) > 0:
            type_specific_info += "\n    - ê°€ì¥ ë¹ˆë²ˆí•œ ê°’:"
            for value, count in list(most_common.items())[:]:
                # ê¸´ ë¬¸ìì—´ì€ ì˜ë¼ì„œ í‘œì‹œ
                display_value = value if len(str(value)) <= 100 else str(value)[:100] + "..."
                type_specific_info += f"\n      - '{display_value}': {count}ê°œ"
    
    elif is_date:
        # ë‚ ì§œí˜• ë°ì´í„°ì¸ ê²½ìš°
        min_date = column_info.get('min_date')
        max_date = column_info.get('max_date')
        date_range = column_info.get('date_range')
        
        if min_date or max_date or date_range:
            type_specific_info += "\n    - ë‚ ì§œ ë²”ìœ„:"
            if min_date:
                type_specific_info += f"\n      - ìµœì†Œ ë‚ ì§œ: {min_date}"
            if max_date:
                type_specific_info += f"\n      - ìµœëŒ€ ë‚ ì§œ: {max_date}"
            if date_range:
                type_specific_info += f"\n      - ë‚ ì§œ ë²”ìœ„: {date_range}"
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„± - ê¸°ì¡´ ì½”ë©˜íŠ¸ê°€ ìˆëŠ” ê²½ìš°ì™€ ì—†ëŠ” ê²½ìš°ë¥¼ êµ¬ë¶„
    if column_comment and column_comment != 'None' and pd.notna(column_comment):
        prompt = f"""{base_info}{type_specific_info}
    
    ê¸°ì¡´ ì„¤ëª…ì„ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ê°œì„ ëœ ì„¤ëª…ì„ ìƒì„±í•´ì£¼ì„¸ìš”:
    1. ì§§ì€ ì„¤ëª… (í•œ ì¤„, 20ì ì´ë‚´) - ê¸°ì¡´ ì„¤ëª…ì„ ì°¸ê³ í•˜ì—¬ ë” ëª…í™•í•˜ê²Œ
    2. ìƒì„¸ ì„¤ëª… (2-3ì¤„, ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ë¯¸ í¬í•¨) - ê¸°ì¡´ ì„¤ëª…ì„ í™•ì¥í•˜ì—¬
    3. ë°ì´í„° íŠ¹ì„± (NULL í—ˆìš© ì—¬ë¶€, ê°’ ë²”ìœ„ ë“±)
    
    ê¸°ì¡´ ì„¤ëª…ì´ ì¶©ë¶„íˆ ëª…í™•í•˜ë‹¤ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜, í†µê³„ ì •ë³´ë¥¼ ë°˜ì˜í•˜ì—¬ ë³´ì™„í•´ì£¼ì„¸ìš”.
    """
    else:
        prompt = f"""{base_info}{type_specific_info}
    
    ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
    1. ì§§ì€ ì„¤ëª… (í•œ ì¤„, 20ì ì´ë‚´)
    2. ìƒì„¸ ì„¤ëª… (2-3ì¤„, ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ë¯¸ í¬í•¨)
    3. ë°ì´í„° íŠ¹ì„± (NULL í—ˆìš© ì—¬ë¶€, ê°’ ë²”ìœ„ ë“±)
    """
    
    try:
        print(f"\nğŸ“ API ìš”ì²­ ì •ë³´:")
        print(f"  - ëª¨ë¸: {os.getenv('DEPLOYMENT_NAME')}")
        print(f"  - ì—”ë“œí¬ì¸íŠ¸: {os.getenv('ENDPOINT_URL')[:50]}...")
        
        # í”„ë¡¬í”„íŠ¸ ì¼ë¶€ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
        print(f"  - í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì")
        print(f"  - í”„ë¡¬í”„íŠ¸ ì²˜ìŒ 200ì:\n{prompt[:200]}...")
        
        response = openai_client.chat.completions.create(
            model=os.getenv('DEPLOYMENT_NAME'),
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë°ì´í„°ë² ì´ìŠ¤ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì»¬ëŸ¼ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ë¯¸ë¥¼ ëª…í™•í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”. ê¸°ì¡´ ì½”ë©˜íŠ¸ê°€ ìˆë‹¤ë©´ ì´ë¥¼ ì°¸ê³ í•˜ì—¬ ê°œì„ ëœ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=3000,
            temperature=1,
            # temperature ì œê±° (gpt-5-01 ëª¨ë¸ì—ì„œ ì§€ì› ì•ˆ í•  ìˆ˜ ìˆìŒ)
        )
        
        # ì „ì²´ ì‘ë‹µ ê°ì²´ í™•ì¸
        print(f"\nğŸ“‹ ì‘ë‹µ ê°ì²´ íƒ€ì…: {type(response)}")
        print(f"  - choices ê°œìˆ˜: {len(response.choices) if response.choices else 0}")
        
        # ì‘ë‹µ í™•ì¸
        if response and response.choices and len(response.choices) > 0:
            choice = response.choices[0]
            print(f"  - choice ê°ì²´: {choice}")
            print(f"  - finish_reason: {choice.finish_reason}")
            print(f"  - message íƒ€ì…: {type(choice.message)}")
            
            # content ì†ì„± í™•ì¸
            if hasattr(choice.message, 'content'):
                content = choice.message.content
                print(f"  - content íƒ€ì…: {type(content)}")
                print(f"  - content ê°’: '{content}'")
                
                if content:
                    print(f"âœ… API ì‘ë‹µ ìˆ˜ì‹  (ê¸¸ì´: {len(content)}ì)")
                    return content
                else:
                    print("âš ï¸ API ì‘ë‹µ contentê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    # ë¹ˆ ì‘ë‹µì¼ ê²½ìš° ê¸°ë³¸ ê°’ ë°˜í™˜
                    if column_comment:
                        return f"1. {column_comment}\n2. {column_comment} ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” ì»¬ëŸ¼ì…ë‹ˆë‹¤.\n3. NULL í—ˆìš©, ë¬¸ìì—´ íƒ€ì…"
                    else:
                        return f"1. {column_info.get('column_name')} ì •ë³´\n2. {column_info.get('column_name')} ê´€ë ¨ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\n3. {column_info.get('null_ratio')} NULL ë¹„ìœ¨"
            else:
                print("âš ï¸ messageì— content ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤.")
                print(f"  - message ì†ì„±ë“¤: {dir(choice.message)}")
                return None
        else:
            print("âš ï¸ API ì‘ë‹µ í˜•ì‹ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤.")
            print(f"   ì‘ë‹µ ê°ì²´: {response}")
            return None
    
    except Exception as e:
        print(f"âŒ OpenAI ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {e}")
        print(f"   ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
        if hasattr(e, 'response'):
            print(f"   ì‘ë‹µ ìƒíƒœ: {getattr(e.response, 'status_code', 'N/A')}")
            print(f"   ì‘ë‹µ ë‚´ìš©: {getattr(e.response, 'text', 'N/A')}")
        
        # ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ë³¸ ì„¤ëª… ë°˜í™˜
        if column_comment:
            return f"1. {column_comment}\n2. {column_comment} ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” ì»¬ëŸ¼ì…ë‹ˆë‹¤.\n3. NULL í—ˆìš© ì—¬ë¶€ í™•ì¸ í•„ìš”"
        else:
            return f"1. {column_info.get('column_name')} ì»¬ëŸ¼\n2. ìƒì„¸ ì„¤ëª… ìƒì„± ì‹¤íŒ¨\n3. ë°ì´í„° íƒ€ì…: {column_info.get('data_type')}"

# í…ŒìŠ¤íŠ¸: ì²« ë²ˆì§¸ ì»¬ëŸ¼ì— ëŒ€í•œ ì„¤ëª… ìƒì„±
if df_column_stats is not None and len(df_column_stats) > 0:
    print("=" * 60)
    print("í…ŒìŠ¤íŠ¸: ì²« ë²ˆì§¸ ì»¬ëŸ¼ ì„¤ëª… ìƒì„±")
    print("=" * 60)
    
    test_column = df_column_stats.iloc[0].to_dict()
    print(f"\ní…ŒìŠ¤íŠ¸ ì»¬ëŸ¼ ì •ë³´:")
    print(f"  - ì»¬ëŸ¼ëª…: {test_column.get('column_name')}")
    print(f"  - ë°ì´í„° íƒ€ì…: {test_column.get('data_type')}")
    print(f"  - ê¸°ì¡´ ì½”ë©˜íŠ¸: {test_column.get('column_comment')}")
    
    description = generate_column_description(test_column, table_context="kt_merged_product_20251001")
    
    print("\n" + "=" * 60)
    if description:
        print(f"ìƒì„±ëœ ì„¤ëª…:")
        print("-" * 50)
        print(description)
    else:
        print("âš ï¸ ì„¤ëª… ìƒì„± ì‹¤íŒ¨ (None ë°˜í™˜)")
        
    # ì¶”ê°€ ë””ë²„ê¹…: ì§ì ‘ API í˜¸ì¶œ í…ŒìŠ¤íŠ¸
    print("\n" + "=" * 60)
    print("ì§ì ‘ API í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    if openai_client:
        try:
            print("ê°„ë‹¨í•œ ë©”ì‹œì§€ë¡œ í…ŒìŠ¤íŠ¸ ì¤‘...")
            test_response = openai_client.chat.completions.create(
                model=os.getenv('DEPLOYMENT_NAME'),
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": "Say 'Hello World' in Korean."}
                ],
                max_completion_tokens=50
            )
            
            print(f"í…ŒìŠ¤íŠ¸ ì‘ë‹µ íƒ€ì…: {type(test_response)}")
            if test_response.choices:
                print(f"Choices ê°œìˆ˜: {len(test_response.choices)}")
                print(f"ì²« ë²ˆì§¸ choice: {test_response.choices[0]}")
                print(f"Message: {test_response.choices[0].message}")
                print(f"Content: '{test_response.choices[0].message.content}'")
            else:
                print("No choices in response")
                
        except Exception as test_e:
            print(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {test_e}")

# 8. ëª¨ë“  ì»¬ëŸ¼ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„° ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬)
def generate_all_column_metadata(df_stats, batch_size=5):
    """ëª¨ë“  ì»¬ëŸ¼ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„° ìƒì„±"""
    
    if df_stats is None or len(df_stats) == 0:
        print("âŒ ì»¬ëŸ¼ í†µê³„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    metadata_list = []
    total_columns = len(df_stats)
    
    print(f"ì´ {total_columns}ê°œ ì»¬ëŸ¼ì— ëŒ€í•œ ë©”íƒ€ë°ì´í„° ìƒì„± ì‹œì‘...")
    
    for i in range(0, total_columns, batch_size):
        batch_end = min(i + batch_size, total_columns)
        print(f"\në°°ì¹˜ ì²˜ë¦¬: {i+1}-{batch_end}/{total_columns}")
        
        for idx in range(i, batch_end):
            column_info = df_stats.iloc[idx].to_dict()
            column_name = column_info['column_name']
            
            print(f"  - {column_name} ì²˜ë¦¬ ì¤‘...")
            
            # OpenAI ì„¤ëª… ìƒì„±
            description = generate_column_description(column_info, table_context="kt_merged_product_20251001")
            
            # ë©”íƒ€ë°ì´í„° êµ¬ì„±
            metadata = {
                'column_name': column_name,
                'data_type': column_info.get('data_type'),
                'null_ratio': column_info.get('null_ratio'),
                'unique_count': column_info.get('unique_count'),
                'description': description,
                'generated_at': datetime.now().isoformat()
            }
            
            # ìˆ˜ì¹˜í˜• ë°ì´í„° ì¶”ê°€ ì •ë³´
            if 'min' in column_info:
                metadata.update({
                    'min': column_info.get('min'),
                    'max': column_info.get('max'),
                    'mean': column_info.get('mean'),
                    'median': column_info.get('median')
                })
            
            metadata_list.append(metadata)
            
            # API í˜¸ì¶œ ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
            import time
            time.sleep(0.5)
    
    print(f"\nâœ… ì´ {len(metadata_list)}ê°œ ì»¬ëŸ¼ ë©”íƒ€ë°ì´í„° ìƒì„± ì™„ë£Œ")
    return pd.DataFrame(metadata_list)

# 9. í…Œì´ë¸” ì „ì²´ ì„¤ëª… ìƒì„±
def generate_table_description(table_name='test', table_stats=None, column_stats=None):
    """í…Œì´ë¸” ì „ì²´ì— ëŒ€í•œ ì¢…í•©ì ì¸ ì„¤ëª… ìƒì„±"""
    
    if not openai_client:
        return None
    
    # ì£¼ìš” ì»¬ëŸ¼ ì •ë³´ ì¶”ì¶œ
    key_columns = []
    if column_stats is not None and len(column_stats) > 0:
        key_columns = column_stats.head(10)['column_name'].tolist()
    
    prompt = f"""
    ë‹¤ìŒ ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸”ì— ëŒ€í•œ ì¢…í•©ì ì¸ ì„¤ëª…ì„ ì˜ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. 
    í…Œì´ë¸”ì€ ì‚¼ì„±ë‹·ì»´ì˜ ì œí’ˆ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤. 
    
    í…Œì´ë¸”ëª…: {table_name}
    
    í…Œì´ë¸” í†µê³„:
    - ì „ì²´ í–‰ ìˆ˜: {table_stats.get('total_rows', 'N/A') if table_stats else 'N/A'}
    - í…Œì´ë¸” í¬ê¸°: {table_stats.get('table_size', 'N/A') if table_stats else 'N/A'}
    - ì „ì²´ ì»¬ëŸ¼ ìˆ˜: {len(column_stats) if column_stats is not None else 'N/A'}
    
    ì£¼ìš” ì»¬ëŸ¼: {', '.join(key_columns)}
    
    ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ì„¤ëª…í•´ì£¼ì„¸ìš”:
    1. í…Œì´ë¸”ì˜ ì£¼ìš” ëª©ì ê³¼ ì—­í•  (2-3ì¤„)
    2. ì €ì¥ë˜ëŠ” ë°ì´í„°ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ë¯¸
    3. ë‹¤ë¥¸ í…Œì´ë¸”ê³¼ì˜ ì ì¬ì  ì—°ê´€ ê´€ê³„
    4. ë°ì´í„° í™œìš© ì‚¬ë¡€ (ì˜ˆ: ë¦¬í¬íŠ¸, ë¶„ì„, API ë“±)
    """
    
    try:
        print("--------")
        response = openai_client.chat.completions.create(
            model=os.getenv('DEPLOYMENT_NAME'),
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë°ì´í„° ì•„í‚¤í…íŠ¸ì…ë‹ˆë‹¤. í…Œì´ë¸”ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ëª©ì ê³¼ í™œìš© ë°©ì•ˆì„ ëª…í™•í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            # temperature íŒŒë¼ë¯¸í„° ì œê±° (ê¸°ë³¸ê°’ 1 ì‚¬ìš©)
            max_completion_tokens=500
        )
        
        return response.choices[0].message.content
    
    except Exception as e:
        print(f"âŒ í…Œì´ë¸” ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {e}")
        return None

# 10-3. ë©”íƒ€ë°ì´í„° ì €ì¥ (MongoDB - ì„ íƒëœ ì»¬ëŸ¼ë§Œ)
def save_metadata_to_mongodb(df_column_stats, df_metadata, table_name, selected_columns, collection_name, output_dir='./metadata'):
    """ì„ íƒëœ ì»¬ëŸ¼ë“¤ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ MongoDBì— ì €ì¥"""
    
    import os
    import pymongo
    from datetime import datetime
    from dotenv import load_dotenv
    
    load_dotenv('.env')
    
    # MongoDB ì—°ê²° ë¬¸ìì—´ ê°€ì ¸ì˜¤ê¸°
    CONNECTION_STRING = os.getenv('COSMOS_CONNECTION_STRING') or os.getenv('MONGODB_CONNECTION_STRING')
    
    if not CONNECTION_STRING:
        print("âŒ MongoDB ì—°ê²° ë¬¸ìì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì— COSMOS_CONNECTION_STRING ë˜ëŠ” MONGODB_CONNECTION_STRINGì„ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return False
    
    try:
        # MongoDB í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        print(f"\nğŸ“¡ MongoDB ì—°ê²° ì‹œë„...")
        client = pymongo.MongoClient(CONNECTION_STRING)
        
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        client.admin.command('ping')
        print(f"âœ… MongoDB ì—°ê²° ì„±ê³µ")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ (ì—°ê²° ë¬¸ìì—´ì—ì„œ ì¶”ì¶œí•˜ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©)
        db_name = "rubicon"  # ê¸°ë³¸ ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„
        
        # ì—°ê²° ë¬¸ìì—´ì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„ ì¶”ì¶œ ì‹œë„
        if '/' in CONNECTION_STRING and '?' in CONNECTION_STRING:
            try:
                db_name_from_conn = CONNECTION_STRING.split('/')[-1].split('?')[0]
                if db_name_from_conn:
                    db_name = db_name_from_conn
            except:
                pass
        
        db = client[db_name]
        collection = db[collection_name]
        
        print(f"ğŸ“‚ ë°ì´í„°ë² ì´ìŠ¤: {db_name}")
        print(f"ğŸ“ ì»¬ë ‰ì…˜: {collection_name}")
        print(f"ğŸ¯ ì„ íƒëœ ì»¬ëŸ¼ ìˆ˜: {len(selected_columns)}")
        
        # ì„ íƒëœ ì»¬ëŸ¼ë“¤ì˜ ë©”íƒ€ë°ì´í„° ì¤€ë¹„
        documents = []
        inserted_count = 0
        
        for column_name in selected_columns:
            # df_column_statsì—ì„œ í•´ë‹¹ ì»¬ëŸ¼ ì •ë³´ ì°¾ê¸°
            column_stats = df_column_stats[df_column_stats['column_name'] == column_name]
            
            if column_stats.empty:
                print(f"  âš ï¸ ì»¬ëŸ¼ '{column_name}'ì„(ë¥¼) í†µê³„ ë°ì´í„°ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            row = column_stats.iloc[0]
            
            # df_metadataì—ì„œ ìƒì„±ëœ ì„¤ëª… ê°€ì ¸ì˜¤ê¸°
            metadata_row = None
            if df_metadata is not None:
                metadata_matches = df_metadata[df_metadata['column_name'] == column_name]
                if not metadata_matches.empty:
                    metadata_row = metadata_matches.iloc[0]
            
            # ì„¤ëª… íŒŒì‹± (ì¸ë±ìŠ¤ ê¸°ë°˜ íŒŒì‹±ìœ¼ë¡œ ìˆ˜ì •)
            short_desc = ""
            long_desc = ""
            data_desc = ""
            
            if metadata_row is not None and pd.notna(metadata_row.get('description')):
                description_text = metadata_row['description']
                lines = description_text.split('\n')
                
                # ì¸ë±ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ íŒŒì‹± (ë¹ˆ ì¤„ í¬í•¨í•˜ì—¬ ì •í™•í•œ ìœ„ì¹˜)
                try:
                    # short_description: ì¸ë±ìŠ¤ 1 (0ë¶€í„° ì‹œì‘, ë‘ ë²ˆì§¸ ì¤„)
                    if len(lines) > 1:
                        short_desc = lines[1].strip()
                        # "1. " ì œê±°
                        if short_desc.startswith('1.'):
                            short_desc = short_desc[2:].strip()
                    
                    # long_description: ì¸ë±ìŠ¤ 4 (ë‹¤ì„¯ ë²ˆì§¸ ì¤„)
                    if len(lines) > 4:
                        long_desc = lines[4].strip()
                        # "2. " ì œê±°
                        if long_desc.startswith('2.'):
                            long_desc = long_desc[2:].strip()
                    
                    # data_description: ì¸ë±ìŠ¤ 7 (ì—¬ëŸ ë²ˆì§¸ ì¤„)
                    if len(lines) > 7:
                        data_desc = lines[7].strip()
                        # "3. " ì œê±°
                        if data_desc.startswith('3.'):
                            data_desc = data_desc[2:].strip()
                        
                except Exception as e:
                    print(f"  âš ï¸ ì„¤ëª… íŒŒì‹± ì‹¤íŒ¨ ({column_name}): {e}")
                    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì „ì²´ ì¤„ì„ ìˆœíšŒí•˜ë©° ë²ˆí˜¸ë¡œ ì°¾ê¸° (ë°±ì—… ë°©ë²•)
                    for line in lines:
                        line_strip = line.strip()
                        if line_strip.startswith('1.') and not short_desc:
                            short_desc = line_strip[2:].strip()
                        elif line_strip.startswith('2.') and not long_desc:
                            long_desc = line_strip[2:].strip()
                        elif line_strip.startswith('3.') and not data_desc:
                            data_desc = line_strip[2:].strip()
            
            # ê¸°ë³¸ê°’ ì„¤ì • (ì„¤ëª…ì´ ì—†ëŠ” ê²½ìš°)
            if not short_desc:
                short_desc = f"{column_name} ì •ë³´"
            if not long_desc:
                long_desc = f"{column_name} ì»¬ëŸ¼ì— ì €ì¥ë˜ëŠ” ë°ì´í„°ì…ë‹ˆë‹¤."
            if not data_desc:
                data_desc = f"ë°ì´í„° íƒ€ì…: {row.get('data_type', 'unknown')}, NULL ë¹„ìœ¨: {row.get('null_ratio', 'N/A')}"
            
            # values í•„ë“œ ì²˜ë¦¬
            values_list = []
            values_field = row.get('values')
            
            if values_field is not None:
                if isinstance(values_field, list):
                    values_list = values_field[:]
                    values_list = [str(v) if not isinstance(v, (str, int, float)) else v for v in values_list]
                elif isinstance(values_field, (np.ndarray, pd.Series)):
                    values_list = values_field.tolist()[:] if len(values_field) > 0 else []
                    values_list = [str(v) if not isinstance(v, (str, int, float)) else v for v in values_list]
            
            # most_common ëŒ€ì²´
            if not values_list:
                most_common = row.get('most_common')
                if most_common is not None and isinstance(most_common, dict) and len(most_common) > 0:
                    values_list = list(most_common.keys())[:]
            
            # column_type ê²°ì •
            column_type = row.get('data_type', 'unknown')
            if df_schema is not None:
                schema_match = df_schema[df_schema['column_name'] == column_name]
                if not schema_match.empty:
                    schema_info = schema_match.iloc[0]
                    data_type = schema_info.get('data_type', '')
                    max_length = schema_info.get('character_maximum_length')
                    numeric_precision = schema_info.get('numeric_precision')
                    numeric_scale = schema_info.get('numeric_scale')
                    
                    if pd.notna(max_length):
                        column_type = f"{data_type}({int(max_length)})"
                    elif pd.notna(numeric_precision) and pd.notna(numeric_scale):
                        column_type = f"{data_type}({int(numeric_precision)},{int(numeric_scale)})"
                    elif pd.notna(numeric_precision):
                        column_type = f"{data_type}({int(numeric_precision)})"
                    else:
                        column_type = data_type
            
            # MongoDB ë¬¸ì„œ ìƒì„±
            document = {
                "_id": f"{table_name}_{column_name}",  # ê³ ìœ  ID
                "table": table_name.replace("kt_merged_", "").replace("_20251001", ""),
                "column": column_name,
                "column_type": column_type,
                "comment": row.get('column_comment', '') if pd.notna(row.get('column_comment')) else "",
                "short_description": short_desc,
                "long_description": long_desc,
                "data_description": data_desc,
                "values": values_list,
                "sql_use": "Y",
                "synonyms": [],
                "statistics": {
                    "null_ratio": row.get('null_ratio', 'N/A'),
                    "unique_count": int(row.get('unique_count')) if pd.notna(row.get('unique_count')) else None,
                    "unique_ratio": row.get('unique_ratio', 'N/A')
                },
                "created_at": datetime.now(),
                "updated_at": datetime.now()
            }
            
            # ì¶”ê°€ í†µê³„ ì •ë³´
            if pd.notna(row.get('min')):
                document["statistics"].update({
                    "min": float(row.get('min')) if pd.notna(row.get('min')) else None,
                    "max": float(row.get('max')) if pd.notna(row.get('max')) else None,
                    "mean": float(row.get('mean')) if pd.notna(row.get('mean')) else None,
                    "median": float(row.get('median')) if pd.notna(row.get('median')) else None,
                    "std": float(row.get('std')) if pd.notna(row.get('std')) else None
                })
            
            if pd.notna(row.get('min_length')):
                document["statistics"].update({
                    "min_length": int(row.get('min_length')) if pd.notna(row.get('min_length')) else None,
                    "max_length": int(row.get('max_length')) if pd.notna(row.get('max_length')) else None,
                    "avg_length": float(row.get('avg_length')) if pd.notna(row.get('avg_length')) else None
                })
            
            documents.append(document)
        
        # MongoDBì— ì‚½ì… (upsert: ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì‚½ì…)
        if documents:
            print(f"\nğŸ“ {len(documents)}ê°œ ë¬¸ì„œë¥¼ MongoDBì— ì €ì¥ ì¤‘...")
            
            for doc in documents:
                try:
                    # upsert ë°©ì‹ìœ¼ë¡œ ì €ì¥
                    result = collection.replace_one(
                        {"_id": doc["_id"]},
                        doc,
                        upsert=True
                    )
                    
                    if result.upserted_id:
                        print(f"  âœ… ì‚½ì…: {doc['column']}")
                    else:
                        print(f"  ğŸ”„ ì—…ë°ì´íŠ¸: {doc['column']}")
                    
                    inserted_count += 1
                    
                except Exception as e:
                    print(f"  âŒ ì‹¤íŒ¨: {doc['column']} - {str(e)}")
            
            print(f"\nâœ… ì´ {inserted_count}ê°œ ë¬¸ì„œ ì €ì¥ ì™„ë£Œ")
            
            # ì €ì¥ëœ ë°ì´í„° í™•ì¸
            total_count = collection.count_documents({})
            print(f"ğŸ“Š ì»¬ë ‰ì…˜ '{collection_name}'ì˜ ì „ì²´ ë¬¸ì„œ ìˆ˜: {total_count}")
            
            # ìƒ˜í”Œ ë¬¸ì„œ ì¶œë ¥
            sample_doc = collection.find_one({"table": table_name.replace("kt_merged_", "").replace("_20251001", "")})
            if sample_doc:
                print(f"\nğŸ“‹ ìƒ˜í”Œ ë¬¸ì„œ:")
                # _idì™€ ë‚ ì§œ í•„ë“œëŠ” ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥
                sample_doc['_id'] = str(sample_doc['_id'])
                sample_doc['created_at'] = str(sample_doc.get('created_at', ''))
                sample_doc['updated_at'] = str(sample_doc.get('updated_at', ''))
                print(json.dumps(sample_doc, ensure_ascii=False, indent=2))
        else:
            print("âš ï¸ ì €ì¥í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ì„ íƒì : JSON ë°±ì—… íŒŒì¼ë„ ìƒì„±
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_path = os.path.join(output_dir, f'{collection_name}_mongodb_backup_{timestamp}.json')
            
            # datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            backup_docs = []
            for doc in documents:
                backup_doc = doc.copy()
                backup_doc['created_at'] = str(doc['created_at'])
                backup_doc['updated_at'] = str(doc['updated_at'])
                backup_docs.append(backup_doc)
            
            with open(backup_path, 'w', encoding='utf-8') as f:
                json.dump(backup_docs, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"ğŸ’¾ ë°±ì—… íŒŒì¼ ì €ì¥: {backup_path}")
        
        return True
        
    except pymongo.errors.ServerSelectionTimeoutError:
        print("âŒ MongoDB ì—°ê²° ì‹œê°„ ì´ˆê³¼")
        print("   ì—°ê²° ë¬¸ìì—´ê³¼ ë„¤íŠ¸ì›Œí¬ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return False
        
    except pymongo.errors.OperationFailure as e:
        print(f"âŒ MongoDB ì‘ì—… ì‹¤íŒ¨: {str(e)}")
        print("   ê¶Œí•œ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return False
        
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return False
        
    finally:
        if 'client' in locals():
            client.close()
            print("ğŸ”Œ MongoDB ì—°ê²° ì¢…ë£Œ")

# ì„ íƒëœ ì»¬ëŸ¼ ëª©ë¡ ì •ì˜
selected_columns = [
    'display_category_major',
    'display_category_middle',
    'display_category_minor',
    'product_category_major',
    'product_category_middle',
    'product_category_minor',
    'model_name',
    'model_code',
    'product_id',
    'product_name',
    'product_color',
    'release_date',
    'is_ai_subscription_eligible',
    'is_smart_subscription_eligible',
    'is_galaxy_club_eligible',
    'is_installment_payment_available',
    'product_detail_url',
    'unique_selling_point',
    'review_count',
    'review_rating_score',
    'standard_price',
    'member_price',
    'benefit_price',
    'review_text_collection',
    # 'display_classification_name',
    'product_specification',
    'web_coupon_discount_amount',
    'stock_quantity',
    'bundle_component_model_code',
    'site_code',
    'final_price',
    'is_bispokle_goods',
    'is_bundle_product',
    'category_rank_recommend',
    'category_rank_quantity',
    'category_rank_rating',
    'total_sale_amount',
    'total_sale_quantity',
    'event_info',
    'coupon_info',
    'promotion_info'
]

# MongoDBì— ë©”íƒ€ë°ì´í„° ì €ì¥
if df_column_stats is not None:
    print("=" * 60)
    print("MongoDBì— ì„ íƒëœ ì»¬ëŸ¼ ë©”íƒ€ë°ì´í„° ì €ì¥")
    print("=" * 60)
    
    success = save_metadata_to_mongodb(
        df_column_stats=df_column_stats,
        df_metadata=df_metadata,
        table_name='kt_merged_product_20251001',
        selected_columns=selected_columns,
        collection_name='synonyms_20251014'  # ì»¬ë ‰ì…˜ ì´ë¦„ì„ ì—¬ê¸°ì„œ ì§€ì •
    )
    
    if success:
        print("\nâœ… MongoDB ì €ì¥ ì‘ì—… ì™„ë£Œ")
    else:
        print("\nâŒ MongoDB ì €ì¥ ì‘ì—… ì‹¤íŒ¨")