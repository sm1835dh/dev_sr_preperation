import os
import json
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional
from datetime import datetime
from db_connector import DatabaseConnector

class MongoDBSaver:
    """MongoDB ë°ì´í„° ì €ì¥ ê´€ë¦¬"""

    def __init__(self, db_name: str = "rubicon"):
        self.db_name = db_name
        self.db_connector = DatabaseConnector()

    def save_metadata_to_collection(
        self,
        df_column_stats: pd.DataFrame,
        df_metadata: Optional[pd.DataFrame],
        df_schema: Optional[pd.DataFrame],
        table_name: str,
        selected_columns: List[str],
        collection_name: str,
        output_dir: str = './metadata'
    ) -> bool:
        """ì„ íƒëœ ì»¬ëŸ¼ë“¤ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ MongoDBì— ì €ì¥"""

        client = self.db_connector.get_mongodb_client()
        if not client:
            return False

        try:
            db = self.db_connector.get_mongodb_database(client, self.db_name)
            collection = db[collection_name]

            print(f"ğŸ“‚ ë°ì´í„°ë² ì´ìŠ¤: {self.db_name}")
            print(f"ğŸ“ ì»¬ë ‰ì…˜: {collection_name}")
            print(f"ğŸ¯ ì„ íƒëœ ì»¬ëŸ¼ ìˆ˜: {len(selected_columns)}")

            documents = []
            inserted_count = 0

            for column_name in selected_columns:
                document = self._create_document(
                    column_name,
                    df_column_stats,
                    df_metadata,
                    df_schema,
                    table_name
                )

                if document:
                    documents.append(document)

            # MongoDBì— ì‚½ì…
            if documents:
                print(f"\nğŸ“ {len(documents)}ê°œ ë¬¸ì„œë¥¼ MongoDBì— ì €ì¥ ì¤‘...")

                for doc in documents:
                    try:
                        result = collection.replace_one(
                            {"_id": doc["_id"]},
                            doc,
                            upsert=True
                        )

                        if result.upserted_id:
                            print(f"  âœ… ì‚½ì…: {doc['column']}")
                        else:
                            print(f"  ğŸ”„ ì—…ë°ì´íŠ¸: {doc['column']}")

                        inserted_count += 1

                    except Exception as e:
                        print(f"  âŒ ì‹¤íŒ¨: {doc['column']} - {str(e)}")

                print(f"\nâœ… ì´ {inserted_count}ê°œ ë¬¸ì„œ ì €ì¥ ì™„ë£Œ")

                # ì €ì¥ëœ ë°ì´í„° í™•ì¸
                total_count = collection.count_documents({})
                print(f"ğŸ“Š ì»¬ë ‰ì…˜ '{collection_name}'ì˜ ì „ì²´ ë¬¸ì„œ ìˆ˜: {total_count}")

                # ë°±ì—… íŒŒì¼ ìƒì„±
                if output_dir:
                    self._create_backup(documents, collection_name, output_dir)

            else:
                print("âš ï¸ ì €ì¥í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False

            return True

        except Exception as e:
            print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

        finally:
            if client:
                client.close()
                print("ğŸ”Œ MongoDB ì—°ê²° ì¢…ë£Œ")

    def _create_document(
        self,
        column_name: str,
        df_column_stats: pd.DataFrame,
        df_metadata: Optional[pd.DataFrame],
        df_schema: Optional[pd.DataFrame],
        table_name: str
    ) -> Optional[Dict[str, Any]]:
        """MongoDB ë¬¸ì„œ ìƒì„±"""

        # df_column_statsì—ì„œ í•´ë‹¹ ì»¬ëŸ¼ ì •ë³´ ì°¾ê¸°
        column_stats = df_column_stats[df_column_stats['column_name'] == column_name]

        if column_stats.empty:
            print(f"  âš ï¸ ì»¬ëŸ¼ '{column_name}'ì„(ë¥¼) í†µê³„ ë°ì´í„°ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        row = column_stats.iloc[0]

        # ì„¤ëª… íŒŒì‹±
        short_desc, long_desc, data_desc = self._parse_descriptions(
            column_name, df_metadata, row
        )

        # values í•„ë“œ ì²˜ë¦¬
        values_list = self._process_values(row)

        # column_type ê²°ì •
        column_type = self._determine_column_type(column_name, row, df_schema)

        # Check if this is a JSONB field
        is_jsonb_field = row.get('is_jsonb_field', False)

        # Generate comment with SQL examples
        comment = self._generate_comment(row, column_name, table_name, is_jsonb_field)

        # MongoDB ë¬¸ì„œ ìƒì„±
        document = {
            "_id": f"{table_name}_{column_name}",
            "table": table_name.replace("kt_merged_", "").replace("_20251001", ""),
            "column": column_name,
            "column_type": column_type,
            "comment": comment,
            "short_description": short_desc,
            "long_description": long_desc,
            "data_description": data_desc,
            "values": values_list,
            "sql_use": "Y",
            "synonyms": [],
            "statistics": {
                "null_ratio": row.get('null_ratio', 'N/A'),
                "unique_count": int(row.get('unique_count')) if pd.notna(row.get('unique_count')) else None,
                "unique_ratio": row.get('unique_ratio', 'N/A')
            },
            "created_at": datetime.now(),
            "updated_at": datetime.now()
        }

        # Add JSONB-specific metadata
        if is_jsonb_field:
            document["is_jsonb_field"] = True
            document["jsonb_column"] = row.get('jsonb_column', '')
            document["field_name"] = row.get('field_name', '')
            document["original_field_path"] = row.get('original_field_path', '')

        # ì¶”ê°€ í†µê³„ ì •ë³´
        self._add_statistics(document, row)

        return document

    def _parse_descriptions(
        self,
        column_name: str,
        df_metadata: Optional[pd.DataFrame],
        row: pd.Series
    ) -> tuple:
        """ì„¤ëª… íŒŒì‹±"""
        short_desc = ""
        long_desc = ""
        data_desc = ""

        if df_metadata is not None:
            metadata_matches = df_metadata[df_metadata['column_name'] == column_name]
            if not metadata_matches.empty:
                metadata_row = metadata_matches.iloc[0]

                if pd.notna(metadata_row.get('description')):
                    description_text = metadata_row['description']
                    lines = description_text.split('\n')

                    try:
                        # ì¸ë±ìŠ¤ ê¸°ë°˜ íŒŒì‹±
                        if len(lines) > 1:
                            short_desc = lines[1].strip()
                            if short_desc.startswith('1.'):
                                short_desc = short_desc[2:].strip()

                        if len(lines) > 4:
                            long_desc = lines[4].strip()
                            if long_desc.startswith('2.'):
                                long_desc = long_desc[2:].strip()

                        if len(lines) > 7:
                            data_desc = lines[7].strip()
                            if data_desc.startswith('3.'):
                                data_desc = data_desc[2:].strip()

                    except Exception as e:
                        print(f"  âš ï¸ ì„¤ëª… íŒŒì‹± ì‹¤íŒ¨ ({column_name}): {e}")
                        # ë°±ì—… ë°©ë²•
                        for line in lines:
                            line_strip = line.strip()
                            if line_strip.startswith('1.') and not short_desc:
                                short_desc = line_strip[2:].strip()
                            elif line_strip.startswith('2.') and not long_desc:
                                long_desc = line_strip[2:].strip()
                            elif line_strip.startswith('3.') and not data_desc:
                                data_desc = line_strip[2:].strip()

        # ê¸°ë³¸ê°’ ì„¤ì •
        if not short_desc:
            short_desc = f"{column_name} ì •ë³´"
        if not long_desc:
            long_desc = f"{column_name} ì»¬ëŸ¼ì— ì €ì¥ë˜ëŠ” ë°ì´í„°ì…ë‹ˆë‹¤."
        if not data_desc:
            data_desc = f"ë°ì´í„° íƒ€ì…: {row.get('data_type', 'unknown')}, NULL ë¹„ìœ¨: {row.get('null_ratio', 'N/A')}"

        return short_desc, long_desc, data_desc

    def _process_values(self, row: pd.Series) -> List:
        """values í•„ë“œ ì²˜ë¦¬"""
        values_list = []
        values_field = row.get('values')

        if values_field is not None:
            if isinstance(values_field, list):
                values_list = values_field[:]
                values_list = [str(v) if not isinstance(v, (str, int, float)) else v for v in values_list]
            elif isinstance(values_field, (np.ndarray, pd.Series)):
                values_list = values_field.tolist()[:] if len(values_field) > 0 else []
                values_list = [str(v) if not isinstance(v, (str, int, float)) else v for v in values_list]

        # most_common ëŒ€ì²´
        if not values_list:
            most_common = row.get('most_common')
            if most_common is not None and isinstance(most_common, dict) and len(most_common) > 0:
                values_list = list(most_common.keys())[:]

        return values_list

    def _determine_column_type(
        self,
        column_name: str,
        row: pd.Series,
        df_schema: Optional[pd.DataFrame]
    ) -> str:
        """ì»¬ëŸ¼ íƒ€ì… ê²°ì •"""
        column_type = row.get('data_type', 'unknown')

        if df_schema is not None:
            schema_match = df_schema[df_schema['column_name'] == column_name]
            if not schema_match.empty:
                schema_info = schema_match.iloc[0]
                data_type = schema_info.get('data_type', '')
                max_length = schema_info.get('character_maximum_length')
                numeric_precision = schema_info.get('numeric_precision')
                numeric_scale = schema_info.get('numeric_scale')

                if pd.notna(max_length):
                    column_type = f"{data_type}({int(max_length)})"
                elif pd.notna(numeric_precision) and pd.notna(numeric_scale):
                    column_type = f"{data_type}({int(numeric_precision)},{int(numeric_scale)})"
                elif pd.notna(numeric_precision):
                    column_type = f"{data_type}({int(numeric_precision)})"
                else:
                    column_type = data_type

        return column_type

    def _add_statistics(self, document: Dict, row: pd.Series):
        """ì¶”ê°€ í†µê³„ ì •ë³´ ì¶”ê°€"""
        if pd.notna(row.get('min')):
            document["statistics"].update({
                "min": float(row.get('min')) if pd.notna(row.get('min')) else None,
                "max": float(row.get('max')) if pd.notna(row.get('max')) else None,
                "mean": float(row.get('mean')) if pd.notna(row.get('mean')) else None,
                "median": float(row.get('median')) if pd.notna(row.get('median')) else None,
                "std": float(row.get('std')) if pd.notna(row.get('std')) else None
            })

        if pd.notna(row.get('min_length')):
            document["statistics"].update({
                "min_length": int(row.get('min_length')) if pd.notna(row.get('min_length')) else None,
                "max_length": int(row.get('max_length')) if pd.notna(row.get('max_length')) else None,
                "avg_length": float(row.get('avg_length')) if pd.notna(row.get('avg_length')) else None
            })

    def _generate_comment(self, row: pd.Series, column_name: str,
                         table_name: str, is_jsonb_field: bool) -> str:
        """
        Generate comment with SQL query examples for Text2SQL support
        """
        # Get existing comment from PostgreSQL
        existing_comment = row.get('column_comment', '')
        if pd.notna(existing_comment) and existing_comment:
            base_comment = existing_comment
        else:
            base_comment = ""

        # Add SQL examples for JSONB fields
        if is_jsonb_field:
            sql_path = column_name  # This is already the SQL path like "product_specification->'KT_SPEC'->>'ë¬´ê²Œ (g)'"
            field_name = row.get('field_name', '')
            data_type = row.get('data_type', 'object')

            # Generate sample values text
            sample_values = row.get('values', [])
            sample_text = ""
            if sample_values:
                sample_preview = sample_values[:3]
                sample_text = f" (ì˜ˆì‹œ ê°’: {', '.join(str(v) for v in sample_preview)})"

            # Check if this is a numeric field
            is_numeric = data_type == 'numeric'

            # Build SQL example comment based on data type
            if is_numeric:
                # For numeric fields, provide basic and range query examples
                sql_examples = f"""
ì¡°íšŒ ë°©ë²•:
  - ê¸°ë³¸ ì¡°íšŒ: SELECT {sql_path} FROM {table_name}
  - ë²”ìœ„ ê²€ìƒ‰ (ì‚¬ì´): SELECT * FROM {table_name} WHERE ({sql_path})::numeric BETWEEN ìµœì†Œê°’ AND ìµœëŒ€ê°’
{sample_text}
"""
            else:
                # For text fields, provide basic and exact match examples
                sql_examples = f"""
ì¡°íšŒ ë°©ë²•:
  - ê¸°ë³¸ ì¡°íšŒ: SELECT {sql_path} FROM {table_name}
  - ì •í™•í•œ ê°’ ê²€ìƒ‰: SELECT * FROM {table_name} WHERE {sql_path} = 'íŠ¹ì •ê°’'
{sample_text}
"""
            # Combine with existing comment
            if base_comment:
                return f"{base_comment}\n\n{sql_examples.strip()}"
            else:
                return sql_examples.strip()

        # For regular columns, return existing comment or empty
        return base_comment if base_comment else ""

    def _create_backup(self, documents: List[Dict], collection_name: str, output_dir: str):
        """ë°±ì—… íŒŒì¼ ìƒì„±"""
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_path = os.path.join(output_dir, f'{collection_name}_mongodb_backup_{timestamp}.json')

        # datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        backup_docs = []
        for doc in documents:
            backup_doc = doc.copy()
            backup_doc['created_at'] = str(doc['created_at'])
            backup_doc['updated_at'] = str(doc['updated_at'])
            backup_docs.append(backup_doc)

        with open(backup_path, 'w', encoding='utf-8') as f:
            json.dump(backup_docs, f, ensure_ascii=False, indent=2, default=str)

        print(f"ğŸ’¾ ë°±ì—… íŒŒì¼ ì €ì¥: {backup_path}")